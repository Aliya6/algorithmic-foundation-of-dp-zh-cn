{"./":{"url":"./","title":"介绍","keywords":"","body":"差分隐私算法基础 The Algorithmic Foundations of Differential Pivacy by Cynthia Dwork Chinese Translation 关于本书 本书起始于2019年08月，记录了本人从零开始学习 The Algorithmic Foundations of Differential Pivacy 的过程。由于差分隐私目前主要研究领域着重于学术界，对于 Cynthia Dwork 女士的《差分隐私算法基础》目前没有一本完整的中文翻译书籍。通过在学习过程中翻译该书籍有助于本人理解差分隐私概念与算法基础。因此希望将该书翻译成中文，能够帮助大家理解差分隐私概念，少走弯路，并且在此过程中督促本人学习差分隐私。 由于本书英文较为晦涩难懂，翻译过程中难免会添加一些个人理解、语义修正和删减，其中不乏错误之处，欢迎指正讨论。 原书作者 感谢英文原著作者 @Cynthia Dwork 和 @Aaron Roth《The Algorithmic Foundations of Differential Pivacy》。有了 Cynthia Dwork 女士才有了差分隐私的一切。 译者 guoJohnny @guoJohnny 项目源码 项目源码存放于 Github 上，https://github.com/guoJohnny/algorithmic-foundation-of-dp-zh-cn。 欢迎建议指正或直接贡献翻译 https://github.com/guoJohnny/algorithmic-foundation-of-dp-zh-cn/issues LICENSE 署名-非商业性使用-相同方式共享 4.0 (CC BY-NC-SA 4.0)。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-11-19 21:48:45 "},"Preface.html":{"url":"Preface.html","title":"前言","keywords":"","body":"前言 隐私保护数据分析的问题由来已久，涉及多个学科。随着有关个人的电子数据变得越来越详细，并且随着技术能够更强大地收集和管理这些数据，对隐私的鲁棒性、隐私的意义和隐私在数学上严格的定义需求不断增长，对满足隐私定义的算法需求也在不断增长。差分隐私就是这样的定义。 在讨论了差分隐私的含义之后，本书主要介绍了实现差分隐私的基本技术，并将这些技术应用于创造性的结合中（第3-7节），其中使用了'查询发布问题'作为示例。其中最重要的是：对比单纯用差分隐私计算替换非隐私的每个计算步骤这种实现方法，通过重新思考计算目标能有更好的结果。 尽管有一些惊人的强大的计算结果，但仍然存在根本的局限性——不仅局限于使用差分隐私可以实现什么目标，而且还局限于什么方法可以防止隐私被完全破坏（泄露）（第8节）。 实际上，本书中讨论的所有算法都针对不同计算能力的对手保持着不同的隐私。某些算法是计算密集型的，其他算法则为高效率的。攻击者和算法的计算复杂度均在第9节中讨论。 在第10节和第11节中，我们从基础知识转向查询发布以外的应用程序，讨论了用于机制设计和机器学习的差分私有方法。关于差分私有算法的绝大多数文献都考虑了要进行大量分析的单个静态数据库。在第12节中讨论了其他模型中的差分隐私，包括分布式数据库和数据流计算。 最后，这本书是对差分隐私问题和技术的全面介绍，但并不是要进行详尽的调查，因为到目前为止，在差分隐私方面有大量研究，我们可以只覆盖一小部分。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-11-02 19:13:07 "},"1-The-Promise-of-Differential-Privacy/Overview.html":{"url":"1-The-Promise-of-Differential-Privacy/Overview.html","title":"一、差分隐私的承诺","keywords":"","body":"一、差分隐私的承诺 差分隐私 描述了数据持有者对数据主体的承诺：“无论您将数据用于任何研究或分析，都不会受到不利影响或其他影响。” 差分数据库机制可以使机密数据广泛用于准确的数据分析，而无需诉诸数据清洗，数据使用协议，数据保护计划，或其他受限方面。但是，保证隐私性的同时，将消耗数据实用性：《信息恢复基本法》指出，对太多问题的过于准确的回答将以一种惊人的方式破坏隐私。关于差分隐私的算法研究的目标是将这种不可避免性推迟尽可能长的时间。 差分隐私解决了一个问题，即在学习公众数据信息的同时，对个人一无所知。 医学数据库可能会告诉我们，吸烟会导致癌症，影响保险公司对吸烟者长期医疗费用的看法。吸烟者受到分析的伤害了吗？如果保险公司知道他吸烟，他的保险费可能会上涨。他可能也会得到帮助。但保险公司学习他的健康风险，使他进入戒烟计划。吸烟者的隐私被侵犯了吗？当然，研究结束后对他的了解比以前更多，但他的信息是不是“泄露”了？差分隐私将认为它不是，理由是对吸烟者的影响是相同的独立于他是否在研究中。是这项研究得出的结论影响了吸烟者，而不是他在数据集中的存在与否影响了实验得出的结论。 差分隐私保证了相同的结论，例如，吸烟会导致癌症，这与是否有人选择进入或退出数据集无关。具体地说，它确保任何输出序列（对查询的响应）在“本质上”发生的可能性相同，与任何个体的存在或不存在无关。这里，概率被隐私机制（由数据持有者控制）所做的随机选择所取代，术语“本质上”被抽象为参数 ε。较小的 ε 将产生更好的隐私（和更不准确的响应）。 差分隐私是一个定义，而不是一个算法。对于给定的计算任务 T 和给定的 ε 值，将有许多不同的私有算法以 ε-差分隐私 方式实现 T。有些算法会比其他算法更准确。当 ε 很小时，很难为任务 T 找到一个高精度的ε-差分隐私算法，就像为一个特定的计算任务找到一个数值稳定的算法一样。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-11-09 21:29:23 "},"1-The-Promise-of-Differential-Privacy/Privacy-preserving-data-analysis.html":{"url":"1-The-Promise-of-Differential-Privacy/Privacy-preserving-data-analysis.html","title":"隐私保护的数据分析","keywords":"","body":"1.1 隐私保护的数据分析 差分隐私是针对隐私保护数据分析问题而提出的一种隐私定义。我们简要地讨论了解决隐私保护的其他方式的一些问题（个人认为：此处的其他方式应该是：属性隐藏、匿名、少量数据等隐私保护方式）。 数据不能完全匿名并且仍然有用 一般来说，数据越丰富，就越有趣和有用。这就产生了“匿名化”和“删除可识别个人信息”的概念，这些概念希望部分数据记录可以被掩盖，其余部分可以发布并用于分析。 然而，由于数据的丰富性使得“个人”数据属性可能与其他领域的数据属性相重合，比如邮政编码、出生日期和性别的组合，甚至三个电影的名字和一个独立的人观看这些电影的大致日期。这种“命名”功能可用于联动攻击，以将不同数据集中的“匿名”记录与非匿名记录进行匹配。有如下两个事例： 1.通过将匿名医疗遭遇数据与（公开提供的）选民登记记录相匹配，确定了马萨丘塞特政府的医疗记录。 2.通过与互联网电影数据库（IMDB）的链接，确定了 Netflix 用户，其观看历史记录包含在 Netflix 发布的匿名电影记录集合中，作为推荐竞赛的训练数据。 差分隐私能中和联动攻击：因为差分隐私是数据访问机制的一个属性，并且与对手可用的辅助信息（背景知识）的存在或不存在无关，访问 IMDb 用户数据将不能对存在 Netflix 训练集中的用户数据进行联动攻击,换言之，攻击在数据集中的用户成功的可能性不会超过不在数据集中的用户。 重标识“匿名”记录并非唯一风险 “匿名”数据记录的重新标识显然是不可取的，这不仅是因为重新标识本身（这肯定揭示了数据集中的成员身份），而且还因为记录可能包含损害信息，如果它与个人相关联，则可能会造成损害。在给定日期从特定紧急护理中心收集的医疗遭遇记录可能只列出少量不同的投诉或诊断。邻居在相关日期访问设施的附加信息给出了邻居病情的一系列可能诊断结果。可能无法将特定记录与邻居匹配这一事实为邻居提供了最低限度的隐私保护。 个人理解：此处的重标识“匿名”记录应该指的是上一小节中通过其他数据集共有属性对匿名数据进行标识。个人认为此处的邻居诊断例子是指，通过关联特定信息，虽然无法确切知道这个人患了什么病，但却缩小了其患病的种类，排除了多余信息，这样是否是种变相的隐私泄露？因为这样只能提供很小的隐私保护。 不具有保护性的大数据集查询 对于特定个体的查询无法准确地得到安全的回答，事实上，人们可能希望直接拒绝他们（如果在计算上无法识别他们）。但如下面的差分攻击所示，强迫查询超过大型集并不是万能的。假设攻击者已知X先生在某个医学数据库中。综上所述，这两个大问题的答案是 ： 1.“数据库中有多少人具有镰状细胞特征？” 2.“数据库中除了X外，还有多少人有镰状细胞的特征？” 通过这两个数据查询得出数据，交出X先生是否有镰状细胞特征。 个人理解：如果某种查询是不允许针对特定个人（这里指的是X）进行查询，只能针对大规模数据统计类查询，这种数据发布的方式也是不具有保护性的，能通过差分攻击攻击得到隐私数据，如标题所示，对大数据集的查询不具有保护性。 查询审查存在的问题 查询审核有问题。如果根据历史记录，回答当前的查询会损害隐私，那么人们可能会倾向于审查查询和响应的序列，以阻止任何响应。例如，审核员可能在寻找可能构成差分攻击的成对查询。这种方法有两个困难。首先，拒绝回答一个问题本身就有可能被披露。第二，查询审计在计算上是不可行的；事实上，如果查询语言足够丰富，则甚至不存在算法过程来判断一对查询是否构成差分攻击。（审核、防止差分攻击语句是不现实的） “不安全”的摘要统计 在某种意义上，将摘要统计作为隐私解决方案是失败的，是直接来自上述差分攻击。摘要统计的其他问题包括针对数据库的各种重建攻击，数据库中每个人都有一个要保护的“秘密位”。有用性目标可以是允许，例如，形式的问题“满足属性 p 的多少人具有秘密比特值 1 ？”。另一方面，对手的目标是增加猜测个人秘密的机会。第 8.1 节中描述的重建攻击显示了即使是这种类型的线性查询数也难以保护：除非引入足够的不精确性，否则几乎所有的秘密比特都可以重建。 公布汇总统计数据的风险的一个显著例证是，应用统计技术，最初是为了确认或驳斥个人 DNA 在法医学混合物中的存在，以裁定个人是否参与全基因组关联研究。根据人类基因组计划的一个网站，“单核苷酸多态性”（SNPs，发音为“snips”）是当基因组序列中的单核苷酸（A、T、C或G）改变时发生的 DNA 序列变异。例如，一个 SNP 可能会改变 AAGGCTAA 到 ATGGCTAA 的 DNA 序列。“在这种情况下，我们说有两个等位基因：A 和 T。对于这样一个 SNP，我们可以问，给定一个特定的参考群体，这两个可能等位基因的频率是多少？考虑到参考群体中 SNP 的等位基因频率，我们可以研究这些频率对于有特定疾病的亚群（即“病例”组）可能有什么不同，寻找与疾病相关的等位基因。因此，全基因组关联研究可能包含大量snp病例组的等位基因频率。根据定义，这些等位基因频率只是聚合的统计数据，而（错误的）假设是，通过这种聚合，它们保留了隐私。然而，考虑到个体的基因组数据，理论上有可能确定个体是否属于病例组（并且，因此，有疾病）。作为回应，国家卫生研究院和 Wellcome信托基金终止了公众从他们资助的研究中获取总频率数据的途径。 （受制于相关知识缺失，未能理解此段重建攻击和 DNA 事例，需要对第 8.1 节进行了解） 这是一个具有挑战性的问题，即使是对于差分隐私，因为涉及到大量的——数十万甚至一百万——测量，这些测量包含和关联了大群体中的小数量的个体。 长期的事实并不“好” 如果一个数据主体随着时间的推移而被跟踪，那么揭露数据个体长期的行为（例如购买面包）可能会有问题。举个例子，假设某人，他年复一年地定期买面包，直到突然转向很少买面包。一位分析师可能会得出结论，某人很可能被诊断为2型糖尿病。分析员可能是正确的，也可能是不正确的；不管怎样，某人的隐私都会受到伤害。 （此处原文为Ordinary Fact，根据下文内容来看，更应该表示为一种长期的普遍性结果，故将其翻译为“长期”） “少数人”原则 在某些情况下，一种特定的技术实际上可以为数据集的“典型”成员提供隐私保护，或者更普遍地说，为“大多数”成员提供隐私保护。在这种情况下，人们经常听到这样的说法，即这种技术是足够的，因为它损害了“少数”参与者的隐私。撇开那些对隐私最重要的人来说可能是离群者这一担忧不谈，“少数人”原则在本质上并不是没有价值的：需要做出社会判断，权衡成本和收益。一个清晰的隐私定义与“少数人”的理念相一致，但还没有发展出来；但是，对于单个数据集，“只有少数”的隐私可以通过随机选择行的子集并将其全部释放来实现（引理4.3，第4节）。抽样界限描述了统计分析的质量，可以在随机子样本上执行，它控制要释放的行数。当“少数人”的原则被拒绝时，差分隐私提供了另一种选择。 （个人理解为离群点更容易遭受差分攻击，需要在之后深入理解） Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-11-10 07:55:04 "},"1-The-Promise-of-Differential-Privacy/Bibliographic-notes.html":{"url":"1-The-Promise-of-Differential-Privacy/Bibliographic-notes.html","title":"参考文献","keywords":"","body":"参考文献 Sweeney [81] linked voter registration records to “anonymized” medical encounter data; Narayanan and Shmatikov carried out a linkage attack against anonymized ranking data published by Netflix [65]. The work on presence in a forensic mix is due to Homer et al. [46]. The first reconstruction attacks were due to Dinur and Nissim [18]. Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-10-29 08:32:55 "},"2-Basic-Terms/Overview.html":{"url":"2-Basic-Terms/Overview.html","title":"二、基本术语","keywords":"","body":"二、基本术语 本节提出了差分隐私的形式化定义，并列举了它的一些关键特性。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-11-02 19:11:19 "},"2-Basic-Terms/The-model-of-computation.html":{"url":"2-Basic-Terms/The-model-of-computation.html","title":"计算模型","keywords":"","body":"2.1 计算模型 我们假设存在一个可信的和可信赖的数据提供者，他将个人的数据保存在数据库 DDD 中，通常由若干 N 行组成。数据库每一行包含单个个体的数据，而隐私目标是同时保护每个个体行，同时允许对整个数据库进行统计分析。 在非交互式或离线的模型中，数据提供者会一次性地生成某种对象，例如“合成数据库”、“摘要统计数据集合”或“净化数据库（经数据清洗的数据库）”。发布后，数据提供者不再扮演任何角色，原始数据可能会被销毁。 查询是应用于数据库的函数。交互式或在线模型允许数据分析员自适应地询问查询，根据观察到的对先前查询的响应来决定下一个查询的位置。 可信的管理员可以被一组个人运行的协议所代替，这些协议使用加密技术来实现安全多方协议。但在大多数情况下，我们对加密假设不感兴趣。第12节描述了这一模型和文献中研究的其他模型。 当所有的查询都提前知道时，非交互模型应该提供最佳的准确性，因为它能够在知道查询结构的情况下关联噪声。相反，当事先不知道有关查询的信息时，非交互式模型会带来严峻的挑战，因为它必须为所有可能的查询提供答案。正如我们将看到的，为了确保隐私，甚至是防止隐私灾难，准确度必然会随着问题的数量而下降，对所有可能的问题提供准确的答案将是不可行的。 差分隐私机制是一种算法，它将一个数据库或一组全体数据类型 X\\mathcal{X}X （所有可能的数据库行）、随机位和一组查询（可选）作为输入，并生成一个输出字符串。希望可以对输出字符串进行解码，以便对查询产生相对准确的答案。如果没有出现任何查询，那么我们就处于非交互式的情况下，希望输出字符串可以被解释为将来的查询提供答案。 在某些情况下，我们可能要求输出字符串是合成数据库。这种合成数据库是由所有可能的数据库行（X\\mathcal{X}X）中得到的多集合组成。这种情况下的解码方法是对合成数据库进行查询，然后应用一些简单的变换，如缩放因子的乘法，使其近似于查询的真实答案。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-07 09:10:17 "},"2-Basic-Terms/Towards-defining-private-data-analysis.html":{"url":"2-Basic-Terms/Towards-defining-private-data-analysis.html","title":"定义隐私数据分析","keywords":"","body":"2.2 定义隐私数据分析 在数据分析的背景下可以这样定义隐私，即：要求分析人员在分析完成后对数据集中的任何个人的了解不超过分析开始前的了解。这一目标的形式化也是很自然的，要求对手对个人的前后认知（即访问数据库之前和之后的认知）不应该“差别过多”，或者对数据库的访问不应该“过多”地改变对手对任何个人的认知。如果数据库能提供任何信息，那么这种隐私的概念是不可能实现的。例如，假设对手的先前的错误认知是每个人都有2个左脚。对统计数据库的访问告诉我们，几乎每个人都有一只左脚和一只右脚。攻击者现在每个人是否有两个左脚持完全不同的看法。 （上段中‘前后认知’原文为‘prior and posterior view’，此处我将其理解为攻击者对个人的描述与认识。故将其翻译为认知。） 第一段中对隐私的定义即“查询前后对任何个体的认知差别足够小（nothing is learned）”，这种定义方法具有一部分吸引力，是因为如果对个人什么都没学到，那么分析就不会伤害到个人隐私。然而，“吸烟导致癌症”的例子表明，这种定义是有缺陷的，罪魁祸首是辅助信息（X先生吸烟）。 这种用 “nothing is learned” 定义隐私的方法让人想起密码系统的语义安全性。粗略地说，语义安全性是指从密文中学不到任何关于明文的信息。也就是说，在看到密文之后，关于明文的任何已知信息在看到密文之前都是已知的。因此，如果有辅助信息说密文是“dog”或“cat”的加密，则密文不会泄漏有关“dog”或“cat”中的哪个已加密的更多信息。形式上，这是通过比较窃听者猜测“狗”和“猫”中哪个被加密的能力与攻击者模拟器（保护者对攻击者进行模拟）的猜测狗”和“猫”能力进行建模的，但这里攻击者模拟器只具有辅助信息，无法接触到密文。如果对于攻击者以及所有辅助信息（对手和模拟器都是私有的），对手模拟器与窃听者的猜测几率基本相同，则系统享有语义安全性。当然，为了使系统有用，合法的接收者必须能够正确地解密消息。否则，语义安全就可以轻松实现。 （个人理解：此处作者将 “nothing is learned” 这种隐私定义的方法与密码学“语义安全”做类比，密码系统中可暴露的密文与隐私保护发布的数据类似，对于窃听者（攻击者）相当于是完全可接触的。这就要求隐私系统与密码系统一样，拥有防范攻击者拥有辅助信息进行攻击的能力（背景知识攻击）。上文提到“窃听者和攻击者模拟器猜测几率是相同的”这一要求，表明了即使攻击者获取得到密文（发布的数据），并且攻击者有辅助信息的情况下，其猜测得到结果的概率与没有得到密文（发布的数据）猜测的结果一样。简单来说，攻击者只能用先验知识瞎猜，无法通过查询的信息得到后验知识。这样保证了攻击者想通过发布的数据得到个体的隐私数据是无用的。） 我们知道，在标准的计算假设下，语义安全的密码系统是存在，那么为什么我们不能构建语义安全的私有数据库机制，这种机制能保持单行秘密的同时得到查询的答案？ 首先，这个类比并不完美：在一个语义安全的密码系统中，有三个方面：消息发送者（加密明文消息的人）、消息接收者（解密密文的人）和窃听者。相比之下，在隐私数据分析的设置中，只有两个方面：管理者（类似于发送者）和数据分析者，这种数据分析者包括两种：1）接收对查询的信息响应（如合法接收者）2）试图从数据中获取对个人隐私有危害的信息（如窃听者）。由于合法接收者与窥探对手是同一方，因此与加密的类比存在缺陷：拒绝向对手提供所有信息意味着拒绝向数据分析者提供所有信息。 第二，和加密方案一样，我们要求隐私机制是有用的，这意味着它教给分析师一些她以前不知道的东西。这种教学对攻击者模拟器是不可用的；也就是说，攻击者模拟器不能“预测”分析员所学的知识。因此，我们可以将数据库视为随机（不可预测）的弱数据源，从中我们可以提取一些非常高质量的随机性，用作随机密码本（random pad）。这可以用于一种加密技术，在这种技术中，将秘密消息添加到一个随机值（“random pad”）中，以便生成一个字符串，该字符串的信息理论上隐藏了秘密。只有知道随机密码本的人才能知道这个秘密；对密码本一无所知的任何一方对这个秘密都一无所知，不管他或她的计算能力如何。给定对数据库的访问权限，分析员可以学习随机密码本，但是攻击者模拟器，没有给定对数据库的访问权限，对密码本一无所知。因此，将随机密码本加密秘密作为辅助信息，分析者可以解密秘密，但是攻击者模拟器对秘密一无所知。这导致攻击者/分析师学习秘密的能力和对手模拟器做同样事情的能力之间存在巨大差异，消除了任何远程类似语义安全的希望。 （个人理解：语义安全与隐私保护存在差异，无法完全类比，其一是因为密码系统是存在三方角色，而在隐私保护系统中，攻击者与分析者的角色对数据发布方来说是等同得到，这就造成了类比差异。其二，原因有待进一步理解。） 对于上述吸烟的导致癌症和隐私保护的类语义安全性两个例子来说，最大的障碍是攻击者拥有辅助信息。显然，即使在“合理”的辅助知识的背景下，隐私保障也必须保持，但把合理的辅助知识从实际的辅助知识中分离是有问题的。例如，使用政府数据库的分析师可能是一家大型搜索引擎公司的员工。什么“合理”假设的辅助知识信息能提供给这样的人？ （个人理解：允许额外的辅助信息的存在，但如何区别和量化这一合理性是个问题。对于本节，理解上仍然存在很多问题需要不断理解和修改。） Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-11-21 19:28:34 "},"2-Basic-Terms/Formalizing-differential-privacy/Formalizing-differential-privacy-Overview.html":{"url":"2-Basic-Terms/Formalizing-differential-privacy/Formalizing-differential-privacy-Overview.html","title":"形式化差分隐私","keywords":"","body":"2.3 形式化差分隐私 我们将从差分隐私的技术定义开始，然后继续解释它。差分隐私将通过过程提供隐私；特别是它将引入随机性。最早的隐私保护做法是使用随机响应技术，这是一种在社会科学中发展起来的技术，用于收集有关禁运或非法行为的统计信息（通过是否拥有财产 PPP 判断）。研究参与者通过下列做法报告他们是否有财产 PPP： 1.掷硬币。 2.如果是反面，那就如实回答。 3.如果是正面，则掷第二枚硬币，正面回答“是”，反面回答“否”。 “隐私”来源于对任何输出的合理否认；特别是，如果拥有财产 PPP 相当于从事非法行为，即使是“是”答案也不构成犯罪，因为无论被告是否实际拥有财产 PPP ，这个答案出现的概率至少为 1/41/41/4。准确度来自于对噪声产生过程的理解（随机分组中引入虚假的“是”和“否”答案）：预期的“是”答案的概率是：没有属性 PPP 的参与者概率的 1/41/41/4 加上有属性 PPP 的概率的 3/43/43/4 。因此，如果 ppp 是具有 ppp 属性的参与者的真实概率，则“是”答案的预期概率为 (1/4)(1−p)+(3/4)p=(1/4)+p/2(1/4)(1-p)+(3/4)p=(1/4)+p/2(1/4)(1−p)+(3/4)p=(1/4)+p/2。因此，我们可以将 ppp 估计为回答“是”的概率的两倍减去 1/21/21/2 ，即 2((1/4)+p/2)−1/22((1/4)+p/2)-1/22((1/4)+p/2)−1/2 (此处个人感觉有误) 。 注：此处令 PAP_APA​ 为真实的概率，PBP_BPB​ 为经过机制变换后得到的概率， 这样，可以由变换之后的概率PBP_BPB​得到真实概率PAP_APA​: PB=1/4(1−PA)+3/4PA=1/4+1/2PAPA=2∗PB−1/2 \\begin{aligned} P_B &= 1/4 (1 - P_A) + 3/4 P_A \\\\ &= 1/4 + 1/2 P_A \\\\ P_A &= 2 * P_B - 1/2 \\end{aligned} PB​PA​​=1/4(1−PA​)+3/4PA​=1/4+1/2PA​=2∗PB​−1/2​ （上述的例子可以得出，经过随机化之后，对个人数据是会有不确定性，无法得知个人是什么样的属性，但最后经过“抛硬币机制”处理后得到的总体概率却能还原出数据集原有总体概率。可以尝试，将原有的 PAP_APA​ 取任何值，都能从最后的 PBP_BPB​ 还原出来，但此时单个个体的属性是随机化的。） 随机化是必不可少的；更确切地说，任何非平凡的隐私都要对所有现有的或未来的辅助信息来源随机化处理（包括其他数据库、研究、网站、在线社区、闲话、报纸、政府统计等等）。下面我们来说明一个简单的混合参数。出于矛盾的原因，我们假设有一个非平凡的确定性算法。存在一个查询，并且有两个数据库在此查询下产生不同的输出。一次更改一行，我们看到存在一对仅在单行值上有所不同的数据库，在同一数据库上同一查询产生不同的输出。知道该数据库是这两个几乎完全相同的数据库其中之一的敌手将了解未知行中数据的值。 （注1：“非平凡的” 具有一定复杂度，需要一定脑力活动、加工过程才能得到的结果或结论。The antonym nontrivial is commonly used by engineers and mathematicians to indicate a statement or theorem that is not obvious or easy to prove. 摘自wikipedia） （注2:为引入随机化和相邻数据集做铺垫。） 因此，我们将需要讨论随机算法的输入和输出空间。 在本专论中，我们使用离散的概率空间。有时我们会将算法描述为从连续分布中采样，但是应始终以适当的方式将其离散化为有限精度（请参见后文的备注2.1）。通常，具有域 AAA 和（离散）范围 BBB 的随机算法将与从AAA到BBB上的概率单纯形的映射相关联，表示为Δ(B)\\Delta(B)Δ(B)： 定义2.1（概率单纯形） 给定一个离散集 BBB，将 BBB 上的概率单纯形，表示为 Δ(B)\\Delta(B)Δ(B) ，其定义为： Δ(B)={x∈R∣B∣:xi⩾0 for all i and ∑i=1∣B∣xi=1} \\Delta(B) = \\{ x \\in \\mathbb{R}^{|B|} : x_i \\geqslant 0\\ for\\ all\\ i\\ and\\ \\sum_{i=1}^{|B|}x_i = 1 \\} Δ(B)={x∈R∣B∣:xi​⩾0 for all i and i=1∑∣B∣​xi​=1} （个人理解：此处的 R\\mathbb{R}R 可以与数据库中的数据集合类比，xix_ixi​ 即映射后得到数据的类概率，将数据集映射到各个离散状态集合 BBB 的元素中，且这些映射产生离散点 xix_ixi​ 的概率之和为 111） （个人理解2：对概率单纯形定义进行拓展，即有一个包含 ∣B∣|B|∣B∣ 个分量的向量 x→\\overrightarrow{x}x，其分量之和为 111,如下： x→∈R∣B∣,x→=(x1,x2...x∣B∣)xi∈x→,∑i=1∣B∣xi=1,and xi⩾0 \\begin{aligned} \\overrightarrow{x} \\in \\mathbb{R}^{|B|},\\overrightarrow{x} = (x_1,x_2...x_{|B|}) \\\\ x_i \\in \\overrightarrow{x},\\sum_{i=1}^{|B|}x_i = 1,and \\ x_i \\geqslant 0 \\\\ \\end{aligned} x∈R∣B∣,x=(x1​,x2​...x∣B∣​)xi​∈x,i=1∑∣B∣​xi​=1,and xi​⩾0​ 例如硬币翻转随机响应机制则为： if B→={0,1},then R∣B∣=R×Rx→∈R×R (i.e:(x1,x2)∈x→)∑i=1∣B∣xi=∑i=12xi=x1+x2=1 \\begin{aligned} if \\ \\overrightarrow{B} = \\{ 0,1 \\},then \\ \\mathbb{R}^{|B|} = R \\times R \\\\ \\overrightarrow{x} \\in R \\times R \\ (i.e:(x_1,x_2) \\in \\overrightarrow{x}) \\\\ \\sum_{i=1}^{|B|}x_i = \\sum_{i=1}^{2}x_i = x_1 + x_2 = 1 \\end{aligned} if B={0,1},then R∣B∣=R×Rx∈R×R (i.e:(x1​,x2​)∈x)i=1∑∣B∣​xi​=i=1∑2​xi​=x1​+x2​=1​ ） 定义2.2（随机化算法） 具有域AAA和离散范围BBB的随机算法M\\mathcal{M}M 与 映射 M:A→Δ(B)M:A \\to \\Delta(B)M:A→Δ(B)相关联。 在输入a∈Aa∈Aa∈A时，算法M\\mathcal{M}M以概率M(a)bM(a)_bM(a)b​输出M(a)=b\\mathcal{M}(a)=bM(a)=b（b∈Bb∈Bb∈B）。概率空间在算法 M\\mathcal{M}M 的硬币翻转上。 （个人理解：M\\mathcal{M}M 是种映射算法（机制），将原始域数据成为其他离散形式（比如直方图）。上文的翻转硬币随机响应机制就是该定义中的 M\\mathcal{M}M 。）） （注3： 随机化算法中最后一句 “概率空间在算法 M\\mathcal{M}M 的硬币翻转上。”的原文是 “The probability space is over the coin flips of the algorithm M\\mathcal{M}M.”。通过查找资料，在国外论坛上找到了比较清楚的解释，原文如下： I have contacted the authors and they were very helpful. The original quote says it all: The book is about randomized algorithms that act on data sets. You can always view these as deterministic algorithms, which take two inputs -- the data set, and also a string of random bits. The definition of differential privacy has a probability operator, and what that remark means is that the probability is taken over the randomness of the random bit string (i.e. the internal randomness of the algorithm), holding the data set fixed. The key to this is that the definition says: \"the algorithm M\\mathcal{M}M outputs ... with probability ...\". This is a random choice, and the source of randomness is defined by the sentence \"The probability space is over the coin flips of the algorithm M\\mathcal{M}M\". 个人对其意思的理解与解释为：我们可以把随机机制看成是一个确定的函数，这个函数的输出结果是有两个参数决定：一个固定数据集和一个随机的比特串。这个比特串的目的就是为了是为了使输出结果随机（即，使得算法内部随机）。算法 M\\mathcal{M}M 的目的就是为了随机选择输出，这种随机性用数学术语表达即为：“概率空间在算法 M\\mathcal{M}M 的硬币翻转上。” 意思是对这种随机串的比特位进行随机翻转，从而造成的机制的随机性。 在后文的差分隐私定义中，同样提及了这个提法，因为定义中存在概率，而这个概率是由随机串的随机性决定的。 ） 我们将数据库 xxx 视为来自全集 X\\mathcal{X}X 的记录的集合。用它们的直方图表示数据库通常会很方便：x∈N∣X∣x \\in \\mathbb{N}^{|\\mathcal{X}|}x∈N∣X∣ ，其中每个项 xix_ixi​ 表示数据库 xxx 中类型 i∈Xi\\in\\mathcal{X}i∈X 元素的数量。（我们略微滥用了符号，让符号 N\\mathbb{N}N 表示所有非负整数的集合，包括零）。 在这个表示中，两个数据库 xxx 和 yyy 之间距离的自然度量将是它们的 ℓ1\\ell_1ℓ1​ 距离： 定义 2.3 (数据库之间距离) 将数据库的 ℓ1\\ell_1ℓ1​ 范数距离表示为 ∥x∥1\\Vert x\\Vert _1∥x∥1​ 其定义为: ∥x∥1=∑i=1∣X∣∣xi∣ \\Vert x\\Vert _1 = \\sum_{i=1}^{|\\mathcal{X}|}|x_i| ∥x∥1​=i=1∑∣X∣​∣xi​∣ 数据库 xxx 和 yyy 之间的 ℓ1\\ell_1ℓ1​ 距离为 ∥x−y∥1\\Vert x-y\\Vert _1∥x−y∥1​ 注意到 ∥x∥1\\Vert x\\Vert _1∥x∥1​ 是衡量数据库 xxx 的大小（也就是说，数据库 xxx 包含的记录数），而 ∥x−y∥1\\Vert x-y\\Vert _1∥x−y∥1​ 表示数据库 xxx 和 yyy 之间相差多少条记录。我们称这种记录相差为1的数据库为相邻数据集。 数据库也可以由行的多集（ X\\mathcal{X}X 的元素）甚至行的有序列表来表示(这是一组的特例,其中行号成为元素名称的一部分)。 在这种情况下，数据库之间的距离通常由汉明距离（即汉明距离不同）来衡量。 但是，除非另有说明，否则我们将使用上述直方图表示形式。 （但是请注意，即使直方图表示法在数学上更方便，在实际的实现中，多集表示通常也会更加简洁）。 现在，我们可以正式定义差分隐私了，这将直观地保证随机算法在相似输入数据库上的行为类似。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-29 09:29:57 "},"2-Basic-Terms/Formalizing-differential-privacy/Formalizing-differential-privacy_1.html":{"url":"2-Basic-Terms/Formalizing-differential-privacy/Formalizing-differential-privacy_1.html","title":"形式化差分隐私（1）","keywords":"","body":"2.3 形式化差分隐私（1） 定义 2.4 （差分隐私） 对于所有的S⊆Range(M)\\mathcal{S} \\subseteq Range(\\mathcal{M})S⊆Range(M) 且所有的 x,y∈N∣X∣x,y\\in \\mathbb{N}^{|\\mathcal{X}|}x,y∈N∣X∣ 有 ∥x−y∥1≤1\\Vert x-y\\Vert _1 \\leq 1∥x−y∥1​≤1，如果满足下列关系： Pr[M(x)∈S]≤exp⁡(ε)Pr[M(y)∈S]+δ \\text{Pr}[\\mathcal{M}(x) \\in \\mathcal{S}] \\leq \\exp(\\varepsilon)\\text{Pr}[\\mathcal{M}(y) \\in \\mathcal{S}] + \\delta Pr[M(x)∈S]≤exp(ε)Pr[M(y)∈S]+δ 则将这个域在 N∣X∣\\mathbb{N}^{|\\mathcal{X}|}N∣X∣ 的随机算法 M\\mathcal{M}M 称为 (ε,δ)(\\varepsilon,\\delta)(ε,δ) 差分隐私（ (ε,δ)(\\varepsilon,\\delta)(ε,δ)- Differentially private）。其中概率空间在算法M\\mathcal{M}M的硬币翻转上。 特别的，如果 δ=0\\delta=0δ=0 ，则将 M\\mathcal{M}M 称为 ε\\varepsilonε 差分隐私(即 ε–Differentially private\\varepsilon \\text{--} Differentially \\ privateε–Differentially private)。 通常，我们对 δ\\deltaδ 的值感兴趣，该值小于多项式数据库大小的倒数。 特别是，δ\\deltaδ 值接近 1/∥x∥11/\\Vert x\\Vert _11/∥x∥1​ 是非常危险（因为在第1节中讨论“少数人”原则）：这种做法通过发布少量数据库参与者的完整记录来“保护隐私”（以获得可用性）。 但是，即使 δ\\deltaδ 可以忽略不计，ε\\varepsilonε- 差分隐私和 (ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私之间也存在理论上的区别。 其中最主要的是量化顺序的转换。 ε\\varepsilonε- 差分隐私可确保对于机制 M(x)\\mathcal{M}(x)M(x) 的每次运行，在每个相邻数据库上同时观察到的输出的可能性几乎相同。相反，从事后观察值得出结论， (ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私对于每对相邻数据库x, yx, \\ yx, y，可能出现这样一种情况：值 M(x)\\mathcal{M}(x)M(x) 更可能由 xxx 产生。但是，给定一个输出 ξ∽M(x)\\xi \\backsim \\mathcal{M}(x)ξ∽M(x)，也可能会找到一个数据库yyy，使得 ξ\\xiξ 在 yyy 上产生的概率比数据库为 xxx 时的概率大得多。 即，分布 M(y)\\mathcal{M}(y)M(y) 中的 ξ\\xiξ 的概率可以实质上大于分布 M(x)\\mathcal{M}(x)M(x) 中的 ξ\\xiξ 的概率。 所以，机制质量： LM(x)∥M(y)(ξ)=ln⁡(Pr[M(x)=ξ]Pr[M(y)=ξ]) \\mathcal{L}_{\\mathcal{M}(x)\\Vert \\mathcal{M}(y)}^{(\\xi)} = \\ln(\\frac{\\text{Pr}\\lbrack \\mathcal{M}(x) = \\xi \\rbrack}{\\text{Pr}\\lbrack \\mathcal{M}(y) = \\xi \\rbrack}) LM(x)∥M(y)(ξ)​=ln(Pr[M(y)=ξ]Pr[M(x)=ξ]​) 对我们至关重要。我们将其称为：当机制输出为 ξ\\xiξ 时的隐私损失。 这种损失可能是正的（当事件在xxx之下比在yyy之下更有可能发生），也可能是负的（当事件在yyy之下比xxx之下更有可能）。正如我们将在引理3.17看到，(ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私确保对于所有相邻的xxx、yyy，隐私损失的绝对值小于等于 ε\\varepsilonε的概率至少为 1−δ1-\\delta1−δ。 与前文一样，概率空间位于机制M\\mathcal{M}M的硬币上。 差分隐私不受后处理的影响：在没有其他有关私有数据库的知识的情况下，数据分析人员无法计算私有算法M\\mathcal{M}M的输出函数，也无法使其差分隐私程度降低。 就是说，如果算法保护了个人的隐私，那么无论是在正式定义下，还是在任何直观的意义上，数据分析师都无法仅仅通过坐在角落里思考算法的输出来增加隐私损失。 形式上，具有（(ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私算法M\\mathcal{M}M的数据独立映射 fff 的合成也具有（(ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私： 命题 2.1（后处理） 令 M:N∣X∣→R\\mathcal{M}: \\mathbb{N}^{|\\mathcal{X}|} \\to RM:N∣X∣→R 是 (ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私随机算法。 令 f:R→R′f:R \\to R'f:R→R′为任意随机映射。 则 f∘M:N∣X∣→R′f \\circ \\mathcal{M}: \\mathbb{N}^{|\\mathcal{X}|} \\to R'f∘M:N∣X∣→R′ 是 (ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私。 【证明】 我们证明了一个确定性函数f:R→R′f:R \\to R'f:R→R′的命题。结果如下，因为任何随机映射都可以分解为确定性函数的凸组合，而差分隐私机制的凸组合是差分隐私的。 设任意一对相邻数据库 x,yx,yx,y 的 ∥x−y∥1≤1\\Vert x-y\\Vert _1 \\leq 1∥x−y∥1​≤1，且任意事件 S⊆R′S\\subseteq R'S⊆R′，设 T={r∈R:f(r)∈S}T = \\{ r \\in R: f(r) \\in S \\}T={r∈R:f(r)∈S} ，则： Pr[f(M(x))∈S]=Pr[M(x)∈T]≤exp⁡(ε)Pr[M(y)∈T]+δ=exp⁡(ε)Pr[f(M(y))∈S]+δ \\begin{aligned} \\text{Pr}\\lbrack f(\\mathcal{M}(x)) \\in S \\rbrack &= \\text{Pr}[\\mathcal{M}(x) \\in T]\\\\ & \\leq \\exp(\\varepsilon)\\text{Pr}[\\mathcal{M}(y) \\in T] + \\delta\\\\ &= \\exp(\\varepsilon)\\text{Pr}[f(\\mathcal{M}(y)) \\in S] + \\delta \\end{aligned} Pr[f(M(x))∈S]​=Pr[M(x)∈T]≤exp(ε)Pr[M(y)∈T]+δ=exp(ε)Pr[f(M(y))∈S]+δ​ 【命题 2.1 证毕】。 从定义2.4可以立即得出 (ε,0)(\\varepsilon,0)(ε,0)- 差分隐私的合成很简单：两个(ε,0)(\\varepsilon,0)(ε,0)- 差分隐私机制的合成是 (2ε,0)(2\\varepsilon,0)(2ε,0)- 差分隐私。这个定理再进一步拓展（即 定理3.16)： 设有 kkk 个差分隐私机制的合成，其中第 iii 个机制为 (εi,δi)(\\varepsilon_i,\\delta_i)(εi​,δi​)- 差分隐私。易知，当 1≤i≤k1 \\leq i \\leq k1≤i≤k时，kkk 个差分隐私机制合成的结果是 (∑i=1kεi,∑i=1kδi)(\\sum_{i=1}^{k}\\varepsilon_i,\\sum_{i=1}^{k}\\delta_i)(∑i=1k​εi​,∑i=1k​δi​)- 差分隐私。 【补充举例： 许多机器学习算法（例如，随机梯度下降）可以描述为对数据集中进行低敏感度查询序列，并且可以容忍查询得到的带噪声回答（“统计查询模型”。） 可以通过添加拉普拉斯噪声来回答每个查询。 通过合成和后处理，训练的模型是差分隐私的且可以安全输出。 】 （个人理解：由定义可知，机制的叠加不能增加差分隐私的隐私保护程度，相反会以线性方式增加 ε\\varepsilonε，进而增大隐私泄露的可能。详细的推导证明见 3.5节 ） 群隐私：(ε,0)(\\varepsilon,0)(ε,0)- 差分隐私机制的群隐私也遵循从定义2.4，隐私保证的强度随群的大小线性下降。 定理2.2 任意一个大小为 kkk 的群体，这个群体的机制 M\\mathcal{M}M 是 (ε,0)(\\varepsilon,0)(ε,0)- 差分隐私，则这个机制 M\\mathcal{M}M 会变成 (kε,0)(k\\varepsilon,0)(kε,0)- 差分隐私。也就是说，对于所有 ∥x−y∥1≤k\\Vert x-y\\Vert _1 \\leq k∥x−y∥1​≤k 和所有 S⊆Range(M)\\mathcal{S} \\subseteq Range(\\mathcal{M})S⊆Range(M) 有: Pr[M(x)∈S]≤exp⁡(kε)Pr[M(y)∈S] \\text{Pr}[\\mathcal{M}(x) \\in \\mathcal{S}] \\leq \\exp(k\\varepsilon)\\text{Pr}[\\mathcal{M}(y) \\in \\mathcal{S}]Pr[M(x)∈S]≤exp(kε)Pr[M(y)∈S] 概率空间在机制 M\\mathcal{M}M 的硬币翻转上。 例如，这解决了包括多个群体成员的调查中的隐私问题 [1]\\ ^{[1]} [1]。 更普遍的说，差分隐私的合成和群体隐私不是同一回事，第3.5.2节(定理3.20) 中改善了机制合成之后的隐私预算退化程度（实质上改善了 kkk 因子）。但是这个定理不适用于改善群体隐私造成的隐私预算增大，即使 δ=0\\delta=0δ=0。 （原文注[1]：然而，随着群体的扩大，隐私保障也随之恶化，这正是我们想要的：很明显，如果我们替换一个完全不同的调查群体，比如健康的青少年，来代替整个被调查的癌症患者群体。在这种替换下，如果我们查询哪部分人每天经常跑三英里，我们应该得到不同的答案。虽然与 (ε,δ)(\\varepsilon,\\delta)(ε,δ) 差分隐私保密性类似，但近似项 δ\\deltaδ 受到了很大的冲击，我们只得到大小为 kkk 的群体是(kε,ke(k−1)ε)(k\\varepsilon,ke^{(k-1)\\varepsilon})(kε,ke(k−1)ε)-差分隐私。注意，此处与隐私参数合成相加定理不同。） （个人理解：多个群体共用一个隐私保护机制，那么随着群体个数的增加，这个隐私保护机制的保护能力会随之下降。上文说明了，ε\\varepsilonε 会线性增加。比如，青少年、老年人、健康人、患不同疾病人等等共同使用一个差分隐私机制。显然各个群体有各个群体的特点，其结果必然会有差异，可想而知这种差异会对隐私保护机制参数造成影响。如上文所述，会呈现线性变化。详细的解释见后文差分隐私定义补充说明) Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2020-01-02 22:28:51 "},"2-Basic-Terms/Formalizing-differential-privacy/Formalizing-differential-privacy_2.html":{"url":"2-Basic-Terms/Formalizing-differential-privacy/Formalizing-differential-privacy_2.html","title":"形式化差分隐私（2）","keywords":"","body":"2.3 形式化差分隐私（2） 2.3.1 差分隐私的承诺 经济观点 ：差分隐私承诺保护个人信息免受任何额外的损害，这种损害的出现是因为他们的数据在私有数据库 xxx 中。如果他们的数据不是 xxx 的一部分，他们就不会遭到这些损害。尽管一旦差分隐私机制 M\\mathcal{M}M 的结果 M(x)\\mathcal{M}(x)M(x) 发布，个人信息确实可能会面临伤害。差分隐私承诺，他们选择参与数据发布并不会显著增加伤害的可能性。这是一个非常功利的隐私定义，因为当一个人决定是否将她的数据包含在差分隐私数据库中时，她会考虑这种差异：与不参与数据发布相比，她的个人信息在参与后遭到损害的概率。因为她无法控制数据库的其余内容，所以考虑到了差别隐私的承诺，她能确信从未来的损害来看，参与与不参与数据发布造成的影响几乎没什么差别。如果给予任何激励———从利他主义到金钱回报——差分隐私可能会说服她允许使用她的数据。这种直觉可以在效用理论的意义上被形式化，我们在这里简单地描述一下。 考虑一个对所有可能的未来事件集合有任意偏好的个体 iii ，我们用 A\\mathcal{A}A 来表示。这些偏好由一个效用函数 uiu_iui​ 来表示 ui:A→R⩾0u_i:\\mathcal{A} \\to \\mathbb{R}_{\\geqslant0}ui​:A→R⩾0​ ，我们说个体iii在 a∈Aa \\in \\mathcal{A}a∈A 的情况下,其效用为 ui(a)u_i(a)ui​(a)。假设 x∈N∣X∣x \\in \\mathbb{N}^{|\\mathcal{X}|}x∈N∣X∣ 是一个包含个体 iii 的私有数据的数据集，M\\mathcal{M}M 是一个 ε\\varepsilonε- 差分隐私算法。设 yyy 与 xxx 为相邻的数据集，它不包括个体 iii 的数据（∥x−y∥1≤1\\Vert x-y\\Vert _1 \\leq 1∥x−y∥1​≤1）。并设 f:Range(M)→Δ(A)f:Range(\\mathcal{M} ) \\to \\Delta(\\mathcal{A})f:Range(M)→Δ(A) 为（任意）函数，该函数决定未来事件 A\\mathcal{A}A 的分布，以机制M的输出为条件（注：此处Δ(A)\\Delta(\\mathcal{A})Δ(A)函数为定义 2.1（概率单纯形）函数）。通过差分隐私的保证以及 命题 2.1 保证的任意后处理的弹性，我们可以有： Ea∽f(M(x))[ui(a)]=∑a∈Aui(a)⋅Prf(M(x))[a]≤∑a∈Aui(a)⋅exp(ε)Prf(M(y))[a]=exp(ε)Ea∽f(M(y))[ui(a)] \\begin{aligned} \\mathbb{E}_{a \\backsim f(\\mathcal{M}(x))}[u_i(a)] &= \\sum_{a \\in \\mathcal{A}}u_i(a) \\cdotp \\underset{f(\\mathcal{M}(x))}{\\text{Pr}}[a] \\\\ &\\leq \\sum_{a \\in \\mathcal{A}} u_i(a) \\cdotp exp(\\varepsilon) \\underset{f(\\mathcal{M}(y))}{\\text{Pr}}[a] \\\\ &= exp(\\varepsilon)\\mathbb{E}_{a \\backsim f(\\mathcal{M}(y))}[u_i(a)] \\end{aligned} Ea∽f(M(x))​[ui​(a)]​=a∈A∑​ui​(a)⋅f(M(x))Pr​[a]≤a∈A∑​ui​(a)⋅exp(ε)f(M(y))Pr​[a]=exp(ε)Ea∽f(M(y))​[ui​(a)]​ 同理， Ea∽f(M(x))[ui(a)]⩾exp(−ε)Ea∽f(M(y))[ui(a)] \\mathbb{E}_{a \\backsim f(\\mathcal{M}(x))}[u_i(a)] \\geqslant exp(-\\varepsilon)\\mathbb{E}_{a \\backsim f(\\mathcal{M}(y))}[u_i(a)] Ea∽f(M(x))​[ui​(a)]⩾exp(−ε)Ea∽f(M(y))​[ui​(a)] 因此，通过保证 ε\\varepsilonε- 差分隐私，数据分析师可以向个人保证，其预期的未来效用不会受到超过 exp(ε)≈(1+ε)exp(\\varepsilon) \\approx (1+\\varepsilon)exp(ε)≈(1+ε) 因子的损害。（注：由 exe^xex 的泰勒展开得到的近似值）注意，这个承诺独立于个人的是效用函数 ui(a)u_i(a)ui​(a)，同时适用于可能具有完全不同效用函数的多个人。 【注：在哈弗大学的差分隐私课程中，将上述概括为： 攻击者从差分隐私提供的数据中知道关于个体什么信息，它都可以从其他个体的数据中学到一样的信息。 机制不能泄漏“个人特定”信息。 且不受对手所拥有辅助信息或计算能力的影响。 】 2.3.2 差分隐私不能保证什么 正如我们在吸烟导致癌症的例子中所看到的，尽管差分隐私是一个非常有力的保证，但它并不能保证无条件的免受伤害。它也不会在以前没有存在的地方创造隐私。更确切的说，差别隐私并不能保证一个人认为是自己秘密的东西仍然是秘密的。它只是确保一个人参与数据的发布本身不会被泄露，也不会导致泄露任何一个人参与的具体情况。从数据中得出的结论很可能反映了个人的统计信息。一项旨在发现特定疾病早期指标的健康调查可能会产生强有力的、甚至是结论性的结果；这些结论对于特定的个人来说并不是差分隐私被攻击的证据；这个人的个人信息甚至可能没有参与到数据集中（再次，差分隐私确保无论个人是否参与调查，这些结论性结果都以非常相似的概率获得。）特别是，如果调查告诉我们，特定的私人属性与公共可观察属性密切相关，那么这并不是对差分隐私的攻击，因为这种相同的相关性将以几乎相同的概率被观察到，而不受任何被调查者的存在或不存在的影响。 差分隐私的性质 引入并正式定义了差分隐私之后，我们将概括说明其关键的性质。 1.防范任意风险，防止个人数据被重新识别。 2.自动消除链接攻击，包括尝试使用所有过去，现在和将来的数据集以及其他形式和辅助信息源进行的所有攻击。 3.量化隐私损失。 差分隐私不是二元概念，它具有一定程度的隐私损失。这允许在不同技术之间进行比较：对于固定的隐私损失，哪种技术可以提供更好的准确性？ 对于固定的数据精度，哪种技术可以提供更好的隐私？ 4.差分隐私合成。也许最关键的是，损失的量化还允许对多次计算中的累积隐私损失进行分析和控制。了解合成下的差分隐私机制的行为，可以让我们从较简单的差分隐私模块设计到分析复杂的差分隐私算法。 5.群隐私。差分隐私允许对诸如家庭之类的群体造成的隐私损失进行分析和控制。 6.后处理中的闭包差异隐私不受后处理的影响：数据分析师在没有其他有关隐私数据库的知识的情况下，无法计算差分隐私算法M \\mathcal{M}M 的输出函数，也就无法使其具有差分隐私性。也就是说，无论有什么辅助信息，无论是在形式上的定义上，还是在任何直觉上，数据分析师都无法通过简单地坐在角落里思考算法的输出来增加隐私损失。 这些是差分隐私的性质。我们可以证明这些性质可逆吗？ 也就是说，这些性质或其中的某些子集是否暗含差分隐私？差分隐私能否在这些方面被削弱并且仍然有意义？这些是悬而未决的问题。 【注：在哈弗大学的差分隐私课程中，将上述概括为： 差分隐私无法保证对手不会推断出敏感属性。 差分隐私不保证分析结果不会“损害”个体。 差分隐私对于未定位到哪几行的信息不提供保护。 】 2.3.3 定义注解 隐私的粒度。 应当仔细审查有关差分隐私的声明，以确保承诺的保密性级别。差分隐私保证即使修改数据库中的单个条目，算法的行为也将大致保持不变。但是，什么构成数据库中的单个条目？考虑例如采用图数据库。 这样的数据库可能会编码一个社交网络：每个个体 i∈[n]i \\in [n]i∈[n] 由图中的一个顶点表示，而个体之间的联系则由边表示。 我们可以在与个体相对应的粒度级别上考虑差分隐私：也就是说，我们可能要求差分隐私算法对从图中添加或删除任何顶点不敏感。这提供了强大的隐私保证，但实际上在图中添加删除节点可能会造成比想象中更大的影响。 单个顶点的添加或删除可以在图中最多添加或删除 nnn 条边。我们希望从图中学习到信息，但是去除 nnn 条边不敏感性可能导致无法学习到有效信息。 另一方面，我们可以在对应于边的粒度级别上考虑差分隐私，并要求我们的算法仅对从图中添加或删除单个或少量边不敏感。当然，这是较弱的保证，但对于某些目的可能仍然足够。简单来说，如果我们承诺在每条边上具有 ε\\varepsilonε- 差分隐私，那么任何数据分析人员都不能得出关于图中 1/ε1/\\varepsilon1/ε 条边的任何子集存在与否的结论。在某些情况下，大批社交联系人可能不会被视为敏感信息：例如，一个人可能没有必要隐藏一个事实，即他的大部分联系对象都是他所在城市或工作场所中的某个人，因为他的住所和住所他工作的地方是公共信息。另一方面，可能存在少数高度敏感的社会联系人（例如，潜在的新雇主或亲密朋友）。在这种情况下，边隐私应足以保护敏感信息，同时也能比顶点隐私数据得到更全面地分析。假设一个人的朋友少于 1/ε1/\\varepsilon1/ε 个，边隐私将保护此类个人的敏感信息。 作为另一个示例，可以设计差分性私人电影推荐系统，以在单个电影的“事件”级别保护训练集中的数据，隐藏任何单个电影的观点/打分，而不是隐藏个人对于电影的热情。（如美国人对西部牛仔或血腥牛仔的热情），或对于“用户”级别数据来说，不隐藏个人整体观点和打分历史。 小ε\\varepsilonε差分隐私机制都很相像。当 ε\\varepsilonε 较小时，(ε,0)(\\varepsilon,0)(ε,0)- 差分隐私 断言，对于所有成对的相邻数据库 x,yx,yx,y和所有输出 ooo，对手无法根据观察 ooo 来区分哪个是真正的数据库。当 ε\\varepsilonε 较小时，未能成为(ε,0)(\\varepsilon,0)(ε,0)- 差分隐私没必要惊讶。例如，该机制可能是(2ε,0)(2\\varepsilon,0)(2ε,0)- 差分隐私。差分隐私的性质保证了具有不同但都很小的ε\\varepsilonε时，机制的表现是相似的。 但是 ε\\varepsilonε 设置多大的值算大？未能满足 (15,0)(15,0)(15,0)- 差分隐私仅表示存在相邻数据库，并且输出 ooo 的情况下，基于对数据库 x,yx,yx,y 的观察，输出 ooo 的概率大。而且ooo 的输出可能不太相同（这被 (ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私解决）；数据库xxx和yyy可能设计得很复杂，很可能出现在“现实世界”中；攻击者可能没有正确的辅助信息来识别已经发生了泄露的输出；攻击者也可能对数据库了解得不够多，无法确定其对称差异的值。因此，就像弱密码系统可能泄漏从消息的最低有效位到完整的解密密钥的任何东西一样，未能满足(ε,0)(\\varepsilon,0)(ε,0)- 或 (ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私可能导致从有效的无意义隐私泄露到整个数据库的完全泄漏。一个大的ε\\varepsilonε 以它自己的方式是大的。 其他形式 除了数据库之外，我们的隐私机制 M\\mathcal{M}M 通常还会将一些辅助参数 www 作为输入。 例如，www 可以在数据库 xxx 上指定查询 qwq_wqw​ 或查询集合Qw\\mathcal{Q}_wQw​。 机制 M(w,x)\\mathcal{M}(w,x)M(w,x) 可能（分别）对 qw(x)q_w(x)qw​(x) 或 Qw\\mathcal{Q}_wQw​ 中某些或所有查询进行差分隐私响应。 对于所有 δ⩾0\\delta \\geqslant 0δ⩾0，我们说如果每个w,M(w,⋅)w,\\mathcal{M}(w,\\cdot)w,M(w,⋅) 都满足(ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私，则机制M(⋅,⋅)\\mathcal{M}(\\cdot,\\cdot)M(⋅,⋅) 满足 (ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私。 www 参数中可能包含的参数的另一个示例是控制 δ=δ(k)\\delta = \\delta(k)δ=δ(k) 应该多小的安全性参数 kkk。也就是说，对于所有 kkk ，M(k,⋅)\\mathcal{M}(k,\\cdot)M(k,⋅) 应该是 (ε,δ(k)(\\varepsilon,\\delta(k)(ε,δ(k)- 差分隐私的。通常，在本专论中，我们要求 δ\\deltaδ 在 kkk 中的作用可以忽略不计，即 δ=k−w(1)\\delta = k^{-w(1)}δ=k−w(1)。因此，我们认为 δ\\deltaδ 在密码学意义上较小，而 ε\\varepsilonε 通常被认为是中等较小的常数。 在辅助参数 www 指定集合 Qw={q:Xn→R}\\mathcal{Q}_w=\\{q: \\mathcal{X}^n \\to \\mathbb{R} \\}Qw​={q:Xn→R} 的情况下，我们将机制 M\\mathcal{M}M 称为摘要生成器。摘要生成器输出一个（差分隐私）摘要 A\\mathcal{A}A，该摘要 A\\mathcal{A}A 可用于计算 Qw\\mathcal{Q}_wQw​ 中所有查询的答案。也就是说，我们需要有一个重构过程 RRR，以便对于指定查询 qv∈Qwq_v \\in \\mathcal{Q}_wqv​∈Qw​ 的每个输入 vvv，重构过程输出 R(A,v)∈RR(\\mathcal{A},v) \\in \\mathbb{R}R(A,v)∈R。通常，我们将要求高概率 M\\mathcal{M}M 产生一个摘要 A\\mathcal{A}A，以便使用 A\\mathcal{A}A 的重构过程可以计算出准确的答案。也就是说，对于全部或大部分（通过某种分布加权）查询 qv∈Qwq_v \\in \\mathcal{Q}_wqv​∈Qw​，误差 ∣R(A,v)−qv(x)∣|R(\\mathcal{A},v)-q_v(x)|∣R(A,v)−qv​(x)∣ 将受到限制。我们有时会滥用表示法，并参考重构过程，以实际查询 qqq（而不是其某些重新表示的 vvv ）作为输入，并输出R(A,q)R(\\mathcal{A},q)R(A,q)。 摘要的一个特殊情况是综合数据库。顾名思义，合成数据库的行与原始数据库的行具有相同的类型。合成数据库的优点是可以使用分析人员将在原始数据库上使用的相同软件进行分析，从而无需特殊的重构过程 R\\mathbb{R}R。 备注2.1 由于浮点数实现的微妙之处，在编程诸如 Laplace 机制之类的有价机制时，必须格外小心。否则，差分隐私可能会被破坏，因为数据库 xxx 上具有非零概率的输出，由于四舍五入的缘故，相邻数据库 yyy 上的概率可能为零。这只是在差异性隐私的情况下需要仔细检查浮点实现的一种方式，并且它不是唯一的。 (个人理解：多参数的输入机制的引入对查询及其查询子集作出更多操作，其中机制 M(w,x)\\mathcal{M}(w,x)M(w,x) 中的参数要与 (ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私中的隐私预算区分开来。对于后面的摘要生成器我的理解是对原始数据集的差分隐私化处理，将结果重新映射到数据集上。这有待后期深入理解和学习。) Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2020-01-06 16:30:40 "},"2-Basic-Terms/Formalizing-differential-privacy/Additional-definition-of-dp.html":{"url":"2-Basic-Terms/Formalizing-differential-privacy/Additional-definition-of-dp.html","title":"差分隐私定义补充说明","keywords":"","body":"2.3* 差分隐私定义补充说明 1、贝叶斯解释 令 X=(X1,...,Xn)∈XnX=(X_1,...,X_n)\\in \\mathcal{X}^nX=(X1​,...,Xn​)∈Xn 为随机变量，该随机变量是根据对手对数据集的“先验知识”而产生的分布。并令 X−i=(X1,...,Xi−1,⊥,Xi+1,...,Xn)X_{-i}=(X_1,...,X_{i-1},\\bot,X_{i+1},...,X_n)X−i​=(X1​,...,Xi−1​,⊥,Xi+1​,...,Xn​) 为去除掉第 iii 个人的数据的数据集（或将该人的数据替换为 dummy value）。 假设 M:Xn→Y\\mathcal{M}:\\mathcal{X}^n \\to YM:Xn→Y 是 ε\\varepsilonε-差分隐私的，令 y∈Yy\\in Yy∈Y 为任意可能的输出。则：对于每一个 xi∈Xx_i \\in \\mathcal{X}xi​∈X，有： Pr[Xi=xi∣M(X)=y]∈e±ε⋅Pr[Xi=xi∣M(X−i)=y] \\text{Pr}[X_i = x_i|\\mathcal{M}(X)=y]\\in e^{\\pm\\varepsilon}\\cdot \\text{Pr}[X_i = x_i|\\mathcal{M}(X_{-i})=y] Pr[Xi​=xi​∣M(X)=y]∈e±ε⋅Pr[Xi​=xi​∣M(X−i​)=y] 2、差分隐私定义的注解 1、在 定义 2.3 提及了差分隐私定义的变体形式。 数据库 xxx 视为来自全集 X\\mathcal{X}X 的记录的集合。用它们的直方图表示数据库通常会很方便：x∈N∣X∣x \\in \\mathbb{N}^{|\\mathcal{X}|}x∈N∣X∣ ，其中每个项 xix_ixi​ 表示数据库 xxx 中类型 i∈Xi\\in\\mathcal{X}i∈X 元素的数量。则，其相邻数据集定义为： ∥x∥1=∑i=1∣X∣∣xi∣ \\Vert x\\Vert _1 = \\sum_{i=1}^{|\\mathcal{X}|}|x_i| ∥x∥1​=i=1∑∣X∣​∣xi​∣ 这种情况适用于数据大小 nnn 是未知的情况。 如果数据大小 nnn 是已知时，也可表示为数据集 x∈Xnx\\in \\mathcal{X}^nx∈Xn。 2、在 隐私的粒度 中提及了社交网络数据，或称为图数据库。考虑要对一个社交网络数据进行差分隐私保护，则有以下两种数据集粒度选择： 数据集 GGG，包含节点和边 相邻数据集 111 : G∽G′G \\backsim G'G∽G′ 相差一个顶点（即个体） 相邻数据集 222 : G∽G′G \\backsim G'G∽G′ 相差一条边（即个体间关系） 哪种选择会提供更好的隐私保护？ 毫无疑问是相邻数据集 111 会提供更为强大的隐私保护能力。因为根据差分隐私定义，差分隐私算法保护个体存在的所有数据及该个体与其他个体的关系。从攻击者的角度看差分隐私算法提供的数据，就好像该个体从未出现在社交网络中，从关系网中被“抹杀”。显而易见这种形式能提供强大的隐私保护，但问题也随之而来。我们希望从社交网络中学习到有用的信息，但是去除单个个体所有条边可能导致无法学习到有效信息。 相对而言，对于边（关系）的隐私保护也能提供隐私性，但显然不如对于顶点（顶点）的保护大，但在对于某些应用来说是足够的。个体是显然存在的，更需要隐藏的是个体的关系网络。比如上下级关系，商业往来关系，主要联系人，亲密朋友等。在这种情况下，对边隐私保护足以保护敏感信息，同时使得数据得到更全面地分析。 在前文所述: 简单来说，如果我们承诺在每条边上具有 ε\\varepsilonε- 差分隐私，那么任何数据分析人员都不能得出关于图中 1/ε1/\\varepsilon1/ε 条边的任何子集存在与否的结论。在某些情况下，大批社交联系人可能不会被视为敏感信息：例如，一个人可能没有必要隐藏一个事实，即他的大部分联系对象都是他所在城市或工作场所中的某个人，因为他的住所和住所他工作的地方是公共信息。另一方面，可能存在少数高度敏感的社会联系人（例如，潜在的新雇主或亲密朋友）。在这种情况下，边隐私应足以保护敏感信息，同时也能比顶点隐私数据得到更全面地分析。假设一个人的朋友少于 1/ε1/\\varepsilon1/ε 个，边隐私将保护此类个人的敏感信息。 其中运用到了群隐私定理，假设每条边都是 ε\\varepsilonε- 差分隐私，我们可以根据群隐私定理得到，改变 1/ε1/\\varepsilon1/ε 条边，则是的图差分隐私机制是 (1,0)(1,0)(1,0)- 差分隐私。 3、差分隐私宽松定义（近似差分隐私） 在前文中（定义 2.4）是差分隐私的宽松定义，定义中的近似项 δ\\deltaδ 放宽了差分隐私机制要求。 下面先提及纯差分隐私定义（pure differential privacy）的自然底数 eee 的作用。最后给出定义。 最初我们对差分隐私的定义要求，任何一个个体的数据差异不对最后产生的结果造成过大的影响。也就是说，如果我们考虑在一行上不同的任何两个数据集 xxx 和 yyy (即 ∥x−y∥1\\Vert x-y \\Vert_1∥x−y∥1​)，则在 xxx 上机制输出分布应与 yyy 上机制的输出分布 “相似”。根据这个要求，很自然地就有了如下定义： Pr[M(x)∈S]≤(1+ε)Pr[M(y)∈S] \\text{Pr}[\\mathcal{M}(x) \\in \\mathcal{S}] \\leq (1+\\varepsilon)\\text{Pr}[\\mathcal{M}(y) \\in \\mathcal{S}] Pr[M(x)∈S]≤(1+ε)Pr[M(y)∈S] 注意：因为相邻数据集的概念是相对的，所以差分隐私定义都是对称的，即有： Pr[M(y)∈S]≤(1+ε)Pr[M(x)∈S] \\text{Pr}[\\mathcal{M}(y) \\in \\mathcal{S}] \\leq (1+\\varepsilon)\\text{Pr}[\\mathcal{M}(x) \\in \\mathcal{S}] Pr[M(y)∈S]≤(1+ε)Pr[M(x)∈S] 为什么要选择自然底数 eεe^\\varepsiloneε 来替换 1+ε1+\\varepsilon1+ε 呢？ 我们可以从上图两者对比看出，当 ε\\varepsilonε 接近 000 时，两者的曲线重合，使得在差分隐私参数小的时候，eεe^\\varepsiloneε 近似 1+ε1+\\varepsilon1+ε。同时这样选择，使用 eεe^\\varepsiloneε 更方便，因为其在乘法运算下能更好表示以隐私预算相加（eε1⋅eε1=eε1+ε1e^{\\varepsilon_1}\\cdot e^{\\varepsilon_1}=e^{\\varepsilon_1+\\varepsilon_1}eε1​⋅eε1​=eε1​+ε1​）。因此才有了 纯差分隐私定义 公式： Pr[M(y)∈S]≤eε⋅Pr[M(x)∈S] \\text{Pr}[\\mathcal{M}(y) \\in \\mathcal{S}] \\leq e^\\varepsilon\\cdot \\text{Pr}[\\mathcal{M}(x) \\in \\mathcal{S}] Pr[M(y)∈S]≤eε⋅Pr[M(x)∈S] 但是，我们会发现，纯差分隐私公式是很严格的。该公式要求：即使概率Pr[M(x)∈S]\\text{Pr}[\\mathcal{M}(x) \\in \\mathcal{S}]Pr[M(x)∈S] 和 Pr[M(y)∈S]\\text{Pr}[\\mathcal{M}(y) \\in \\mathcal{S}]Pr[M(y)∈S] 小到可以忽略不计，但仍然要求对于每一个输出，两者的概率相差都应该很小，从而满足纯差分隐私公式。换句话说，ε\\varepsilonε- 差分隐私是绝对的。对于任何一对数据库，攻击者获得任何一个个体的概率信息都不能超过一个小数值。当在数据库中添加或删除一个人时，算法的所有可能输出都会以相似的概率出现。 相比之下，是否能允许机制中出现不满足ε\\varepsilonε- 差分隐私定义的场景？即：机制有可能以小概率输出在这个数据集的个体信息，借由这种小概率输出隐私信息（privacy）一定程度上获得机制的有用性（utility）。但是攻击者会有很小的概率可以由此知道所涉及的个体是在数据集中。 对这种不满足 ε\\varepsilonε- 差分隐私定义场景，我们用前文中所描述的进行概括。机制中可能出现这样一种情况： 值 M(x)\\mathcal{M}(x)M(x) 更可能由 xxx 产生。但是，给定一个输出 ξ∽M(x)\\xi \\backsim \\mathcal{M}(x)ξ∽M(x)，可能会找到一个数据库yyy，使得 ξ\\xiξ 在 yyy 上产生的概率比数据库为 xxx 时的概率大得多。即，分布 M(y)\\mathcal{M}(y)M(y) 中的 ξ\\xiξ 的概率可以实质上大于分布 M(x)\\mathcal{M}(x)M(x) 中的 ξ\\xiξ 的概率。 可以看出，这种小概率事件就会破坏纯差分隐私定义，所以我们要引入一个近似项 δ\\deltaδ 放宽强定义，使得更多的机制能满足差分隐私（如高斯噪声、二项式噪声等） 所以引出了差分隐私宽松定义（近似差分隐私）的定义形式： Pr[M(x)∈S]≤exp⁡(ε)Pr[M(y)∈S]+δ \\text{Pr}[\\mathcal{M}(x) \\in \\mathcal{S}] \\leq \\exp(\\varepsilon)\\text{Pr}[\\mathcal{M}(y) \\in \\mathcal{S}] + \\delta Pr[M(x)∈S]≤exp(ε)Pr[M(y)∈S]+δ 这个定义本质上意味着那些破坏差分隐私的极小概率事件的发生概率 ≤δ\\leq \\delta≤δ。换句话说，可以将 (ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私解释为：机制至少能以 1−δ1-\\delta1−δ 的概率保证其为 ε\\varepsilonε- 差分隐私。 在引出这个定义之后，我们就会进一步探究数据集中有多少用户面临风险？可以使用参数为 (n,δ)(n,\\delta)(n,δ) 的二项式定律的期望值计算，每个用户都有可能发生这种情况的概率为 δ\\deltaδ 。因此，平均而言，这将发生 δ⋅n\\delta\\cdot nδ⋅n 。 如果不想让任何用户面临风险，就要使得 δ⋅n\\delta\\cdot nδ⋅n 尽可能小。如果 δ=1100⋅n\\delta=\\frac{1}{100\\cdot n}δ=100⋅n1​，则可以保证说 “以99％的概率，不会发生这种场景”。但是，如果 δ\\deltaδ 相对于数据量 nnn 不是一个可忽略的函数的话，从泄露隐私的期望上看，当 nnn 增大时，机制将使一些用户处于危险之中。 4、差分隐私合成与群隐私 在前文提到了“差分隐私合成与群隐私”的概念区别，但是文中对群隐私没有进行一个清楚定义与表达，特别是没对带有近似项 δ\\deltaδ 的情况进行说明。这边对群隐私概念做一个定义，并给予证明。并从直观上对两者概念进行区分。 【定义 群隐私(group privacy)】如果 M\\mathcal{M}M 是 (ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私，且对于所有成对数据集 x,y∈N∣X∣x,y\\in \\mathbb{N}^{|\\mathcal{X}|}x,y∈N∣X∣ 有 ∥x−y∥1≤k\\Vert x - y \\Vert_1\\leq k∥x−y∥1​≤k，此时机制 M\\mathcal{M}M 则为 (kε,k⋅e(k−1)ε⋅δ)(k\\varepsilon,k\\cdot e^{(k-1)\\varepsilon}\\cdot \\delta)(kε,k⋅e(k−1)ε⋅δ)- 差分隐私。 【证明】 我们使用混合参数进行证明。令 x0,...,xkx_0,...,x_kx0​,...,xk​ 为 kkk 个数据，且 x0=x,xk=yx_0=x,x_k=yx0​=x,xk​=y。对于 0≤i≤k0\\leq i\\leq k0≤i≤k ，有 ∥xi+1−xi∥1=1\\Vert x_{i+1}-x_i\\Vert_1=1∥xi+1​−xi​∥1​=1（即两者相差为一行记录）。因为M\\mathcal{M}M 是 (ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私，则对于所有的 S⊆Range(M)\\mathcal{S} \\subseteq Range(\\mathcal{M})S⊆Range(M) 有： Pr[M(x0)∈S]≤eεPr[M(x1)∈S]+δ≤eε(eεPr[M(x2)∈S]+δ)+δ⋮≤ekεPr[M(xk)∈S]+(1+eε+e2ε+,...,+e(k−1)ε)⋅δ≤ekεPr[M(xk)∈S]+k⋅e(k−1)ε⋅δ \\begin{aligned} \\text{Pr}[\\mathcal{M}(x_0)\\in \\mathcal{S}] &\\leq e^\\varepsilon\\text{Pr}[\\mathcal{M}(x_1)\\in \\mathcal{S}] + \\delta\\\\ &\\leq e^\\varepsilon(e^\\varepsilon\\text{Pr}[\\mathcal{M}(x_2)\\in \\mathcal{S}]+\\delta) + \\delta\\\\ &\\qquad \\vdots\\\\ &\\leq e^{k\\varepsilon}\\text{Pr}[\\mathcal{M}(x_k)\\in \\mathcal{S}] + (1+e^\\varepsilon+e^{2\\varepsilon}+,...,+e^{(k-1)\\varepsilon})\\cdot\\delta\\\\ &\\leq e^{k\\varepsilon}\\text{Pr}[\\mathcal{M}(x_k)\\in \\mathcal{S}] + k\\cdot e^{(k-1)\\varepsilon}\\cdot\\delta \\end{aligned} Pr[M(x0​)∈S]​≤eεPr[M(x1​)∈S]+δ≤eε(eεPr[M(x2​)∈S]+δ)+δ⋮≤ekεPr[M(xk​)∈S]+(1+eε+e2ε+,...,+e(k−1)ε)⋅δ≤ekεPr[M(xk​)∈S]+k⋅e(k−1)ε⋅δ​ 【证毕】 从直观上来说，差分隐私合成是多个差分隐私机制的叠加。举个例子就如对一个数据集用不同的查询语句进行查询，这种查询语句我们就可以把它当作一种机制。这种机制（查询）的叠加造成了隐私损失 ε\\varepsilonε 与近似项 δ\\deltaδ 线性增长。（定理 3.16）。 与合成定义不同的是，群隐私是指一个隐私保护机制（查询）应用于相差多条记录的两个数据集上。这个特点造成了群隐私的近似项如上面定义中提到的会发生很大变化。即使机制 M\\mathcal{M}M 是 ε\\varepsilonε- 差分隐私的，两者最后的隐私损失都为 kεk\\varepsilonkε 的情况下，其本质是不同。并且差分隐私合成可以将隐私损失进一步优化（定理 3.20），但群隐私无法做到。 这也解释了前文所述： 更普遍的说，差分隐私的合成和群体隐私不是同一回事，第3.5.2节(定理3.20) 中改善了机制合成之后的隐私预算退化程度（实质上改善了 kkk 因子）。但是这个定理不适用于改善群体隐私造成的隐私预算增大，即使 δ=0\\delta=0δ=0。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2020-01-06 16:30:29 "},"2-Basic-Terms/Bibliographic-notes.html":{"url":"2-Basic-Terms/Bibliographic-notes.html","title":"参考文献","keywords":"","body":"参考文献 The definition of differential privacy is due to Dwork et al. [23]; the precise formulation used here and in the literature first appears in [20] and is due to Dwork and McSherry. The term “differential privacy” was coined by Michael Schroeder. The impossibility of semantic secu- rity is due to Dwork and Naor [25]. Composition and group privacy for (ε,0)-differentially private mechanisms is first addressed in [23].Composition for (ε,δ)-differential privacy was first addressed in [21] (but see the corrected proof in Appendix B, due to Dwork and Lei [22]). The vulnerability of differential privacy to inappropriate implementa- tions of floating point numbers was observed by Mironov, who proposed a mitigation [63]. Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-11-03 13:00:07 "},"3-Basic-Techniques-and-Composition-Theorems/Overview.html":{"url":"3-Basic-Techniques-and-Composition-Theorems/Overview.html","title":"三、基本技术与合成定理","keywords":"","body":"三、基本技术与合成定理 本章在回顾了一些概率工具之后，介绍了拉普拉斯机制，该机制为实际（向量）值的查询提供了差分隐私。这种应用自然引出指数机制，这是一种用于从一组离散的候选输出中进行差分隐私选择的方法。然后，我们分析了由多种差分隐私机制构成造成的累积隐私损失。最后，我们提供了一种方法——稀疏矢量技术——主要用于报告可能非常大量的计算结果，但前提是只有少数几个是“有意义的”。 在本节中，我们描述了差异隐私中的一些最基本的技术，我们将再次使用它们。此处描述的技术构成了我们将要开发的所有其他算法的基本合成部分。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-11-25 14:22:36 "},"3-Basic-Techniques-and-Composition-Theorems/Useful-probabilistic-tools.html":{"url":"3-Basic-Techniques-and-Composition-Theorems/Useful-probabilistic-tools.html","title":"概率工具","keywords":"","body":"3.1 概率工具 以下集中不等式经常会有用。 我们以易于使用的形式而不是最强的形式陈述它们。 （注：集中不等式：集中不等式是数学中的一类不等式，描述了一个随机变量是否集中在某个取值附近。例如大数定律说明了一系列独立同分布随机变量的平均值在概率上趋近于它们的数学期望，这表示随着变量数目增大，平均值会集中在数学期望附近。） 定理3.1（加法形式的切尔诺夫界限、又称切尔诺夫不等式）定义 X1,X2,...,XmX_1,X_2 ,...,X_mX1​,X2​,...,Xm​ 为独立随机变量，对任意,iii 有 0≤Xi≤10\\leq X_i \\leq 10≤Xi​≤1 。定义 S=1m∑i=1mXiS = \\frac{1}{m}\\sum_{i=1}^{m}X_iS=m1​∑i=1m​Xi​ 为随机变量的均值，定义 μ=E[S]\\mu = \\mathbb{E}[S]μ=E[S] 为他们的期望均值。则可以得到如下不等式： Pr[S>μ+ε]≤e−2mε2Pr[Sμ−ε]≤e−2mε2 \\begin{aligned} \\text{Pr}[S > \\mu + \\varepsilon] &\\leq e^{-2m\\varepsilon^{2}}\\\\ \\text{Pr}[S Pr[S>μ+ε]Pr[Sμ−ε]​≤e−2mε2≤e−2mε2​ 定理3.2（乘法形式的切尔诺夫不等式）定义 X1,X2,...,XmX_1,X_2 ,...,X_mX1​,X2​,...,Xm​ 为独立随机变量，对任意,iii 有 0≤Xi≤10\\leq X_i \\leq 10≤Xi​≤1 。定义 S=1m∑i=1mXiS = \\frac{1}{m}\\sum_{i=1}^{m}X_iS=m1​∑i=1m​Xi​ 为随机变量的均值，定义 μ=E[S]\\mu = \\mathbb{E}[S]μ=E[S] 为他们的期望均值。则可以得到如下不等式： Pr[S>(1+ε)μ]≤e−mμε2/3Pr[S(1−ε)μ]≤e−mμε2/2 \\begin{aligned} \\text{Pr}[S > (1 + \\varepsilon)\\mu] &\\leq e^{-m\\mu\\varepsilon^{2}/3}\\\\ \\text{Pr}[S Pr[S>(1+ε)μ]Pr[S(1−ε)μ]​≤e−mμε2/3≤e−mμε2/2​ 当我们没有独立的随机变量时。我们仍然可以应用 AzumaAzumaAzuma 不等式： 定理3.3 Azuma不等式 令 fff 为 mmm 个随机变量 X1,...,XmX_1,...,X_mX1​,...,Xm​ 的方法，每一个 XiX_iXi​ 的值取自集合 AiA_iAi​ ，使得 E(f)\\mathbb{E}(f)E(f) 有界。用 cic_ici​ 表示 XiX_iXi​ 对 fff 的最大影响，即对于所有的 ai,ai′∈Aia_i,a_i^{\\prime} \\in A_iai​,ai′​∈Ai​， 有： ∣E[f∣X1,...,Xi−1,Xi=ai]∣−∣E[f∣X1,...,Xi−1,Xi=ai′]∣≤ci |\\mathbb{E}[f|X_1,...,X_{i-1},X_i=a_i]|-|\\mathbb{E}[f|X_1,...,X_{i-1},X_i=a_i^\\prime]| \\leq c_i ∣E[f∣X1​,...,Xi−1​,Xi​=ai​]∣−∣E[f∣X1​,...,Xi−1​,Xi​=ai′​]∣≤ci​ 则： Pr[f(Xi,...,Xm)≥E[f]+t]≤exp(−2t2∑i=1mci2) \\text{Pr}[f(X_i,...,X_m) \\geq \\mathbb{E}[f] + t ] \\leq exp(-\\frac{2t^2}{\\sum_{i=1}^{m}c_i^2}) Pr[f(Xi​,...,Xm​)≥E[f]+t]≤exp(−∑i=1m​ci2​2t2​) （注：Azuma不等式涉及随机过程中“鞅”（Martingale）的概念） 定理3.4 斯特林近似 n!n!n! 可以近似于 2nπ(n/e)n\\sqrt{2n\\pi}(n/e)^n2nπ​(n/e)n: 2nπ(n/e)ne1/(12n+1)n!2nπ(n/e)ne1/(12n) \\sqrt{2n\\pi}(n/e)^ne^{1/(12n+1)} 2nπ​(n/e)ne1/(12n+1)n!2nπ​(n/e)ne1/(12n) Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-11-23 10:57:42 "},"3-Basic-Techniques-and-Composition-Theorems/Randomized-response.html":{"url":"3-Basic-Techniques-and-Composition-Theorems/Randomized-response.html","title":"随机响应","keywords":"","body":"3.2 随机响应 让我们回想一下第2节中描述的用于评估非法行为发生频率的简单随机响应机制。 让XYZ成为这样的活动。 面对查询“您在过去一周内从事XYZ吗？”，指示受访者执行以下步骤： 1.掷硬币。 2.如果是反面，请如实回应。 3.如果是正面，那么再掷第二枚硬币，如果正面回答“是”，如果是反面则回答“否”。 随机响应背后的动机是，它提供了“合理的否认”。例如，可能提供了“是”响应，因为第一次和第二次硬币翻转都是正面，概率为1/4。换句话说，隐私是通过过程获得的，没有“好”或“坏”的响应。获得响应的过程会影响如何合理地解释它们。如下声明所示，这种随机响应是差分隐私的。 命题 3.5 上述随机响应方法是 (ln3,0)(ln3,0)(ln3,0)- 差分隐私的。 【证明】： 假设固定随机响应者为同一人。上述分析表明 Pr[Response=Yes∣Truth=Yes]=3/4\\text{Pr}[Response=Yes|Truth=Yes]=3/4Pr[Response=Yes∣Truth=Yes]=3/4 。具体来说，当事实是“是”时，如果第一个硬币出现反面（概率1/2）或第一个和第二个出现正面（概率1/4），则结果将是“是”，而 Pr[Response=Yes∣Truth=No]=1/4\\text{Pr}[Response=Yes|Truth=No]=1/4Pr[Response=Yes∣Truth=No]=1/4 （第一个出现在正面，第二个出现反面；概率为1/4）。同理，将类似的推理应用于事实为“否”的情况，我们获得： Pr[Response=Yes∣Truth=Yes]Pr[Response=Yes∣Truth=No]=Pr[Response=No∣Truth=No]Pr[Response=No∣Truth=Yes]=3/41/4=3=exp(ln3) \\begin{aligned} \\frac{\\text{Pr}[Response=Yes|Truth=Yes]}{\\text{Pr}[Response=Yes|Truth=No]} \\\\ = \\frac{\\text{Pr}[Response=No|Truth=No]}{\\text{Pr}[Response=No|Truth=Yes]} &= \\frac{3/4}{1/4} = 3 = exp(ln3) \\end{aligned} Pr[Response=Yes∣Truth=No]Pr[Response=Yes∣Truth=Yes]​=Pr[Response=No∣Truth=Yes]Pr[Response=No∣Truth=No]​​=1/43/4​=3=exp(ln3)​ 于是  ε=ln3\\ \\varepsilon = ln3 ε=ln3 上述随机响应方法是 (ln3,0)(ln3,0)(ln3,0)- 差分隐私的。 【命题 3.5证毕】 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-16 09:52:33 "},"3-Basic-Techniques-and-Composition-Theorems/The-laplace-mechanism.html":{"url":"3-Basic-Techniques-and-Composition-Theorems/The-laplace-mechanism.html","title":"Laplace机制","keywords":"","body":"3.3 Laplace 机制 数值查询是数据库最基础的数据查询类型，将其定义为：f:N∣X∣→Rkf:\\mathbb{N}^{|\\mathcal{X}|} \\to \\mathbb{R}^kf:N∣X∣→Rk。这个查询将数据库映射成为k个真实值。将查询的ℓ1\\ell_1ℓ1​敏感度作为衡量我们对查询回答的准确程度的参数之一。 定义3.1 （ℓ1\\ell_1ℓ1​敏感度） 方法 f:N∣X∣→Rkf:\\mathbb{N}^{|\\mathcal{X}|} \\to \\mathbb{R}^kf:N∣X∣→Rk 的 ℓ1\\ell_1ℓ1​敏感度为： Δf=max⁡x,y∈N∣X∣,∥x−y∥1=1∥f(x)−f(y)∥1 \\Delta f = \\max_{x,y\\in\\mathbb{N}^{|\\mathcal{X}|},\\Vert x-y\\Vert _1=1}\\Vert f(x)-f(y)\\Vert _1 Δf=x,y∈N∣X∣,∥x−y∥1​=1max​∥f(x)−f(y)∥1​ 【注：该处在哈佛大学的课程中表示为：Δf=max⁡x,y∈N∣X∣,∥x−y∥1=1∣f(x)−f(y)∣\\Delta f = \\max_{x,y\\in\\mathbb{N}^{|\\mathcal{X}|},\\Vert x-y\\Vert _1=1}| f(x)-f(y)|Δf=maxx,y∈N∣X∣,∥x−y∥1​=1​∣f(x)−f(y)∣，而《差分隐私算法基础》将 ℓ1\\ell_1ℓ1​敏感度表示为 定义 3.1 中形式。个人认为，对于以数值形式作为函数的输出，其敏感度计算应遵循哈弗大学课程中的计算公式。对于以分量形式作为函数的输出，应使用本书计算公式。下图使用哈佛大学课程中的举例，用平均值作为函数 fff： 】 函数 fff 的 ℓ1\\ell_1ℓ1​ 敏感度反映了单体的数据在最坏情况下可以改变函数 fff 的程度，因此，直观地讲，为了隐藏单个人的参与，我们必须引入响应的不确定性。确实，我们将这种动机形式化：函数的敏感性为我们对输出施加多少扰动以保护隐私提供了一个上界。自然而然地，一种噪声分布可带来差分隐私。 定义3.2（拉普拉斯分布） 以0为中心，以 bbb 为尺度的拉普拉斯分布，其概率密度函数的分布为： Lap(x∣b)=12bexp⁡(−∣x∣b) Lap(x|b) = \\frac{1}{2b}\\exp(-\\frac{|x|}{b}) Lap(x∣b)=2b1​exp(−b∣x∣​) 这个分布的方差是 σ2=2b2\\sigma^2=2b^2σ2=2b2 。我们有时会写 Lap(b)Lap(b)Lap(b) 来表示带尺度为 bbb 的Laplace分布，有时会滥用符号，写 Lap(b)Lap(b)Lap(b) 来表示随机变量 X∽Lap(b)X \\backsim Lap(b)X∽Lap(b)。 拉普拉斯分布是指数分布的对称版本。 我们现在定义拉普拉斯机制。顾名思义，拉普拉斯机制将简单地计算 fff，并用拉普拉斯分布的噪声扰动每个映射 fff 的输出结果(注：此处为个人理解补充进翻译中，原文省略了指代与解释。)。噪声的尺度将校准为 fff 的敏感度（除以 ε\\varepsilonε） [1]\\ ^{[1]} [1] （原书注[1]: 或者，使用方差校准为 Δfln(1/δ)/ε\\Delta fln(1/\\delta)/\\varepsilonΔfln(1/δ)/ε 的高斯噪声，可以实现 (ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私（请参阅附录A）。拉普拉斯机制的使用更为简洁，两种机制在合成下的行为类似（定理3.20）） 定义3.3 （拉普拉斯机制） 给定任意方法 f:N∣X∣→Rkf:\\mathbb{N}^{|\\mathcal{X}|} \\to \\mathbb{R}^kf:N∣X∣→Rk， 拉普拉斯机制定义为如下形式： ML(x,f(⋅),ε)=f(x)+(Y1,…,Yk) \\mathcal{M}_L(x,f(\\cdot),\\varepsilon)=f(x) + (Y_1,\\dots,Y_k) ML​(x,f(⋅),ε)=f(x)+(Y1​,…,Yk​) 此处的 YiY_iYi​ 是从 Lap(Δf/ε)Lap(\\Delta f/\\varepsilon)Lap(Δf/ε) 中提取的独立同分布随机变量。 定理 3.6 拉普拉斯机制是 (ε,0)(\\varepsilon,0)(ε,0)-差分隐私。 【证明】：设 x∈N∣X∣,y∈N∣X∣x \\in \\mathbb{N}^{|\\mathcal{X}|},y \\in \\mathbb{N}^{|\\mathcal{X}|}x∈N∣X∣,y∈N∣X∣，同时满足 ∥x−y∥1≤1\\Vert x-y\\Vert _1 \\leq 1∥x−y∥1​≤1 （即两者为相邻数据集）。并设 f(⋅)f(\\cdot)f(⋅) 是函数 f:N∣X∣→Rkf:\\mathbb{N}^{|\\mathcal{X}|} \\to \\mathbb{R}^kf:N∣X∣→Rk。用 pxp_xpx​ 表示概率密度函数 ML(x,f,ε)\\mathcal{M}_L(x,f,\\varepsilon)ML​(x,f,ε), pyp_ypy​ 表示概率密度函数 ML(y,f,ε)\\mathcal{M}_L(y,f,\\varepsilon)ML​(y,f,ε)。我们用任意点 zzz 比较这两者的概率密度： px(z)py(z)=∏i=1k(exp⁡(−ε∣f(x)i−zi∣Δf)exp⁡(−ε∣f(y)i−zi∣Δf))=∏i=1kexp⁡(ε(∣f(y)i−zi∣−∣f(x)i−zi∣)Δf)≤∏i=1kexp⁡(ε∣f(x)i−f(y)i∣Δf)=exp⁡(ε∥f(x)−f(y)∥1Δf)≤exp⁡(ε) \\begin{aligned} \\frac{p_x(z)}{p_y(z)} &= \\prod_{i=1}^{k}\\Bigg(\\frac{\\exp(-\\frac{\\varepsilon|f(x)_i-z_i|}{\\Delta f})}{\\exp(-\\frac{\\varepsilon|f(y)_i-z_i|}{\\Delta f})} \\Bigg)\\\\ &= \\prod_{i=1}^{k}\\exp\\Bigg( \\frac{\\varepsilon(|f(y)_i-z_i|-|f(x)_i-z_i|)}{\\Delta f} \\Bigg)\\\\ &\\leq \\prod_{i=1}^{k}\\exp\\Bigg(\\frac{\\varepsilon|f(x)_i-f(y)_i|}{\\Delta f} \\Bigg)\\\\ &= \\exp\\Bigg(\\frac{\\varepsilon\\Vert f(x)-f(y)\\Vert _1}{\\Delta f} \\Bigg)\\\\ &\\leq \\exp(\\varepsilon) \\end{aligned} py​(z)px​(z)​​=i=1∏k​(exp(−Δfε∣f(y)i​−zi​∣​)exp(−Δfε∣f(x)i​−zi​∣​)​)=i=1∏k​exp(Δfε(∣f(y)i​−zi​∣−∣f(x)i​−zi​∣)​)≤i=1∏k​exp(Δfε∣f(x)i​−f(y)i​∣​)=exp(Δfε∥f(x)−f(y)∥1​​)≤exp(ε)​ 第一个不等式由三角不等式推导得来，最后一个不等式是由敏感度定义得到，即：∥x−y∥1≤1\\Vert x-y\\Vert _1 \\leq 1∥x−y∥1​≤1。且由对称性可得 px(z)py(z)≥exp⁡(−ε)\\frac{p_x(z)}{p_y(z)} \\geq \\exp(-\\varepsilon)py​(z)px​(z)​≥exp(−ε)。 【定理 3.6 证毕】 【补充1: 由 Laplace机制的定义: ML(x,f(⋅),ε)=f(x)+(Y1,…,Yk) \\mathcal{M}_L(x,f(\\cdot),\\varepsilon)=f(x) + (Y_1,\\dots,Y_k) ML​(x,f(⋅),ε)=f(x)+(Y1​,…,Yk​) 为直观表示，使用随机变量的分量形式。即：Y→=(Y1,…,Yk)\\overrightarrow{Y}=(Y_1,\\dots,Y_k)Y=(Y1​,…,Yk​)，f:N∣X∣→Rk=f(x)→=(f(x)1,...,f(x)k)f:\\mathbb{N}^{|\\mathcal{X}|} \\to \\mathbb{R}^k=\\overrightarrow{f(x)}=(f(x)_1,...,f(x)_k)f:N∣X∣→Rk=f(x)​=(f(x)1​,...,f(x)k​)。 所以，根据定理 3.6中的条件： px(z→)=Pr[ML(x,f(⋅),ε)=z→]=Pr[f(x)→+Y→=z→]=Pr[Y→=z→−f(x)→]=ε2Δfexp⁡(−ε∣f(x)→−z→∣Δf)=ε2Δf∏i=1kexp⁡(−ε∣f(x)i−zi∣Δf) \\begin{aligned} p_x(\\overrightarrow{z}) &=\\text{Pr}[\\mathcal{M}_L(x,f(\\cdot),\\varepsilon)=\\overrightarrow{z}]\\\\ &= \\text{Pr}[\\overrightarrow{f(x)} + \\overrightarrow{Y} =\\overrightarrow{z}]\\\\ &=\\text{Pr}[\\overrightarrow{Y}=\\overrightarrow{z}-\\overrightarrow{f(x)}]\\\\ &= \\frac{\\varepsilon}{2\\Delta f}\\exp\\Big(-\\frac{\\varepsilon|\\overrightarrow{f(x)}-\\overrightarrow{z}|}{\\Delta f}\\Big)\\\\ &= \\frac{\\varepsilon}{2\\Delta f}\\prod_{i=1}^{k}\\exp\\Big(-\\frac{\\varepsilon|f(x)_i-z_i|}{\\Delta f}\\Big) \\end{aligned} px​(z)​=Pr[ML​(x,f(⋅),ε)=z]=Pr[f(x)​+Y=z]=Pr[Y=z−f(x)​]=2Δfε​exp(−Δfε∣f(x)​−z∣​)=2Δfε​i=1∏k​exp(−Δfε∣f(x)i​−zi​∣​)​ px(z)py(z)=∏i=1k(exp⁡(−ε∣f(x)i−zi∣Δf)exp⁡(−ε∣f(y)i−zi∣Δf))\\frac{p_x(z)}{p_y(z)} = \\prod_{i=1}^{k}\\Big(\\frac{\\exp(-\\frac{\\varepsilon|f(x)_i-z_i|}{\\Delta f})}{\\exp(-\\frac{\\varepsilon|f(y)_i-z_i|}{\\Delta f})} \\Big)py​(z)px​(z)​=∏i=1k​(exp(−Δfε∣f(y)i​−zi​∣​)exp(−Δfε∣f(x)i​−zi​∣​)​) 表示形式就是上式拉普拉斯分布的分量形式，即：px(z)=ε2Δf∏i=1kexp⁡(−ε∣f(x)i−zi∣Δf)p_x(z)= \\frac{\\varepsilon}{2\\Delta f}\\prod_{i=1}^{k}\\exp\\Big(-\\frac{\\varepsilon|f(x)_i-z_i|}{\\Delta f}\\Big)px​(z)=2Δfε​∏i=1k​exp(−Δfε∣f(x)i​−zi​∣​) 【补充2：由于 定义 2.3 (数据库之间距离) 定义了数据库 xxx 和 yyy 之间的 ℓ1\\ell_1ℓ1​ 距离为 ∥x−y∥1\\Vert x-y\\Vert _1∥x−y∥1​ ∥x−y∥1=∑i=1∣X∣∣xi−yi∣ \\Vert x-y\\Vert _1 = \\sum_{i=1}^{|\\mathcal{X}|}|x_i-y_i| ∥x−y∥1​=i=1∑∣X∣​∣xi​−yi​∣ 故：上述证明中的 ∏i=1kexp⁡(ε∣f(x)i−f(y)i∣Δf)=exp⁡(ε∥f(x)−f(y)∥1Δf)\\prod_{i=1}^{k}\\exp\\Big(\\frac{\\varepsilon|f(x)_i-f(y)_i|}{\\Delta f} \\Big)=\\exp\\Big(\\frac{\\varepsilon\\Vert f(x)-f(y)\\Vert _1}{\\Delta f} \\Big)∏i=1k​exp(Δfε∣f(x)i​−f(y)i​∣​)=exp(Δfε∥f(x)−f(y)∥1​​) 可由如下步骤得到： ∏i=1kexp⁡(ε∣f(x)i−f(y)i∣Δf)=exp⁡(ε∑i=1k∣f(x)i−f(y)i∣Δf)=exp⁡(ε∥f(x)−f(y)∥1Δf) \\begin{aligned} \\prod_{i=1}^{k}\\exp\\Bigg(\\frac{\\varepsilon|f(x)_i-f(y)_i|}{\\Delta f} \\Bigg) &= \\exp\\Bigg(\\frac{\\varepsilon\\sum_{i=1}^{k}|f(x)_i-f(y)_i|}{\\Delta f} \\Bigg)\\\\ &= \\exp\\Bigg(\\frac{\\varepsilon\\Vert f(x)-f(y)\\Vert _1}{\\Delta f} \\Bigg) \\end{aligned} i=1∏k​exp(Δfε∣f(x)i​−f(y)i​∣​)​=exp(Δfε∑i=1k​∣f(x)i​−f(y)i​∣​)=exp(Δfε∥f(x)−f(y)∥1​​)​ 】 例3.1 计数查询：计数查询是“数据库中有多少个元素满足属性 P？”形式的查询。我们将一次又一次地回到这些查询，有时是纯形式，有时使用小数形式返回这些查询（“数据库中某元素的占比是多少....？”），有时带有权重（线性查询），有时带有稍微复杂的形式。（例如，对数据库中的每个元素应用 h:N∣X∣→[0,1]h:\\mathbb{N}^{|\\mathcal{X}|} \\to [0,1]h:N∣X∣→[0,1] 并求和）。计数是一个非常强大的原语。它占据了统计查询学习模型中所有可学习的内容，以及许多标准的数据挖掘任务和基本统计​​信息。由于计数查询的敏感度为1（单个人的添加或删除最多可以影响 1 个计数），因此 定理3.6 的直接结果是，可以实现 (ε,0)(\\varepsilon,0)(ε,0)-差分隐私 计数通过添加尺度参数为 1/ε1/\\varepsilon1/ε 的噪声进行查询，即通过添加从 Lap(1/ε)Lap(1/\\varepsilon)Lap(1/ε) 分布提取的噪声进行查询。预期的失真或错误为 1/ε1/\\varepsilon1/ε，与数据库的大小无关。 固定但任意数量的 mmm 个计数查询列表可以视为向量值查询。缺少有关查询集的任何进一步信息时，此矢量值查询的敏感性的最坏情况范围是 mmm，因为单个个体可能会更改每个计数（敏感度为 mmm）。在这种情况下，可以通过将尺度参数为 m/εm/\\varepsilonm/ε 的噪声添加到每个查询的真实答案中来实现 (ε,0)(\\varepsilon,0)(ε,0)-差分隐私。 有时我们将响应大量（可能是任意的）查询的问题称为查询发布问题。 例3.2 直方图查询：在查询在结构上不相交的特殊（但很常见）情况下，我们可以做得更好——我们不必让噪声随查询的数量而变化。直方图查询就是一个例子。在这种类型的查询中，数据整体（ N∣X∣\\mathbb{N}^{|\\mathcal{X}|}N∣X∣ ）被划分为多个单元格，查询每个单元格中有多少数据库元素。由于单元格是不相交的，单个数据库元素的添加或删除会影响一个单元格中的计数，并且与该单元格的差异是 1，因此直方图查询的敏感度为1，可以对每个单元格中的真实计数增加源自 Lap(1/ε)Lap(1/\\varepsilon)Lap(1/ε) 分布的噪声来回答查询。 为了了解一般查询的 Laplace 机制的准确性，我们使用以下有用的事实： 事实 3.7： 如果 Y∽Lap(b)Y \\backsim Lap(b)Y∽Lap(b)，则： Pr[∣Y∣≥t⋅b]=exp⁡(−t) \\text{Pr}[|Y| \\geq t \\cdot b] = \\exp(-t) Pr[∣Y∣≥t⋅b]=exp(−t) 这个事实与布尔不等式（译者注：Union Bound，又称 Boole’s Inequality1>^{}1>）一起为我们提供了一个 关于拉普拉斯机制准确性的简单不等式： 定理 3.8 ：设 f:N∣X∣→Rk,y=ML(x,f(⋅),ε)f:\\mathbb{N}^{|\\mathcal{X}|} \\to \\mathbb{R}^k,y=\\mathcal{M}_L(x,f(\\cdot),\\varepsilon)f:N∣X∣→Rk,y=ML​(x,f(⋅),ε)。则 ∀δ∈(0,1]\\forall\\delta \\in (0,1]∀δ∈(0,1]： Pr[∥f(x)−y∥∞≥ln⁡(kδ)⋅(Δfε)]≤δ \\text{Pr}\\Big[\\Vert f(x)-y\\Vert _\\infty \\geq \\ln(\\frac{k}{\\delta})\\cdot(\\frac{\\Delta f}{\\varepsilon}) \\Big] \\leq \\delta Pr[∥f(x)−y∥∞​≥ln(δk​)⋅(εΔf​)]≤δ 【证明】 我们有： Pr[∥f(x)−y∥∞≥ln⁡(kδ)⋅(Δfε)]=Pr[max⁡i∈[k]∣Yi∣≥ln⁡(kδ)⋅(Δfε)]≤k⋅Pr[∣Yi∣≥ln⁡(kδ)⋅(Δfε)]=k⋅(δk)=δ \\begin{aligned} \\text{Pr}\\Big[\\Vert f(x)-y\\Vert _\\infty \\geq \\ln(\\frac{k}{\\delta})\\cdot(\\frac{\\Delta f}{\\varepsilon})\\Big] &= \\text{Pr}\\Big[\\max_{i \\in [k]}|Y_i|\\geq \\ln(\\frac{k}{\\delta})\\cdot(\\frac{\\Delta f}{\\varepsilon}) \\Big]\\\\ & \\leq k\\cdot \\text{Pr}\\Big[|Y_i|\\geq \\ln(\\frac{k}{\\delta})\\cdot(\\frac{\\Delta f}{\\varepsilon}) \\Big] \\\\ &= k\\cdot(\\frac{\\delta}{k})\\\\ &= \\delta \\end{aligned} Pr[∥f(x)−y∥∞​≥ln(δk​)⋅(εΔf​)]​=Pr[i∈[k]max​∣Yi​∣≥ln(δk​)⋅(εΔf​)]≤k⋅Pr[∣Yi​∣≥ln(δk​)⋅(εΔf​)]=k⋅(kδ​)=δ​ 在证明中，第二个和最后的不等式是从拉普拉斯分布和事实3.7中推导得到的。 （译者注 布尔不等式：指对于全部事件的概率不大于单个事件的概率总和，对于事件 A1,A2,A3...:P(⋃iAi)≤∑iP(Ai)A_1,A_2,A_3...: P(\\bigcup_{i}A_i)\\leq \\sum_iP(A_i)A1​,A2​,A3​...:P(⋃i​Ai​)≤∑i​P(Ai​)） 【补充3： 上一证明过程缺少 ℓ∞\\ell_\\inftyℓ∞​ 范数距离，又称切比雪夫距离，如下定义：∥x∥\\Vert x \\Vert∥x∥ 为 xxx 向量各个元素绝对值最大那个元素的绝对值，形式化为： ∥x∥∞=lim⁡k→∞(∑i=1n∣pi−qi∣k)1/k=max⁡i∈[k]∣pi−qi∣ \\Vert x\\Vert _{\\infty}=\\lim\\limits_{k \\to \\infty}\\Big(\\sum_{i=1}^n|p_i-q_i|^k \\Big)^{1/k}=\\max_{i \\in [k]}|p_i-q_i| ∥x∥∞​=k→∞lim​(i=1∑n​∣pi​−qi​∣k)1/k=i∈[k]max​∣pi​−qi​∣ 】 【补充4: （定理3.8补充证明） 由 Laplace 机制可知 Yi∽Lap(Δf/ε)Y_i \\backsim Lap(\\Delta f/\\varepsilon)Yi​∽Lap(Δf/ε) 和 y=ML(x,f(⋅ε)=f(x)+(Y1,…,Yk)y=\\mathcal{M}_L(x,f(\\cdot\\varepsilon)=f(x) + (Y_1,\\dots,Y_k)y=ML​(x,f(⋅ε)=f(x)+(Y1​,…,Yk​)，则: ∣f(x)−y∣=∣(Y1,...Yk)∣|f(x) - y|= |(Y_1,...Y_k)|∣f(x)−y∣=∣(Y1​,...Yk​)∣。 又因切比雪夫距离定义： ∥f(x)−y∥∞=maxi∈k∣f(x)−y∣=maxi∈k∣Yi∣ \\Vert f(x)-y\\Vert _\\infty=max_{i \\in k}|f(x)-y|=max_{i \\in k}|Y_i| ∥f(x)−y∥∞​=maxi∈k​∣f(x)−y∣=maxi∈k​∣Yi​∣ 故证明的第一步可以由此推导出： Pr[∥f(x)−y∥∞≥ln⁡(kδ)⋅(Δfε)]=Pr[max⁡i∈[k]∣Yi∣≥ln⁡(kδ)⋅(Δfε)] \\text{Pr}\\Big[\\Vert f(x)-y\\Vert _\\infty \\geq \\ln(\\frac{k}{\\delta})\\cdot(\\frac{\\Delta f}{\\varepsilon})\\Big] = \\text{Pr}\\Big[\\max_{i \\in [k]}|Y_i|\\geq \\ln(\\frac{k}{\\delta})\\cdot(\\frac{\\Delta f}{\\varepsilon}) \\Big] Pr[∥f(x)−y∥∞​≥ln(δk​)⋅(εΔf​)]=Pr[i∈[k]max​∣Yi​∣≥ln(δk​)⋅(εΔf​)] 证明第二步是因为 max⁡i∈[k]∣Yi∣≥ln⁡(kδ)⋅(Δfε)\\max_{i \\in [k]}|Y_i| \\geq \\ln(\\frac{k}{\\delta})\\cdot(\\frac{\\Delta f}{\\varepsilon})maxi∈[k]​∣Yi​∣≥ln(δk​)⋅(εΔf​) 的概率必然小于等于 ⋃i{∣Yi∣≥ln⁡(kδ)⋅(Δfε)}\\bigcup_{i} \\{|Y_i| \\geq \\ln(\\frac{k}{\\delta})\\cdot(\\frac{\\Delta f}{\\varepsilon})\\}⋃i​{∣Yi​∣≥ln(δk​)⋅(εΔf​)} 全体的概率，且由布尔不等式推导而来，即最大值 YiY_iYi​ 概率不大于单个事件的概率总和。又因为 事实3.7，推导如下： Pr[max⁡i∈[k]∣Yi∣≥ln⁡(kδ)⋅(Δfε)]≤Pr[⋃i{∣Yi∣≥ln⁡(kδ)⋅(Δfε)}]≤∑ik⋅Pr[∣Yi∣≥ln⁡(kδ)⋅(Δfε)]=∑ik⋅exp⁡(−ln(kδ))=k⋅(δk)=δ \\begin{aligned} \\text{Pr}\\Big[\\max_{i \\in [k]}|Y_i|\\geq \\ln(\\frac{k}{\\delta})\\cdot(\\frac{\\Delta f}{\\varepsilon})\\Big] & \\leq \\text{Pr}\\Big[ \\bigcup_{i} \\{|Y_i| \\geq \\ln(\\frac{k}{\\delta})\\cdot(\\frac{\\Delta f}{\\varepsilon}) \\}\\Big]\\\\ &\\leq \\sum_i^k \\cdot \\text{Pr}\\Big[|Y_i| \\geq \\ln(\\frac{k}{\\delta})\\cdot(\\frac{\\Delta f}{\\varepsilon})\\Big]\\\\ &= \\sum_i^k \\cdot \\exp\\big(-ln(\\frac{k}{\\delta})\\big)\\\\ &= k\\cdot(\\frac{\\delta}{k})\\\\ &= \\delta \\end{aligned} Pr[i∈[k]max​∣Yi​∣≥ln(δk​)⋅(εΔf​)]​≤Pr[i⋃​{∣Yi​∣≥ln(δk​)⋅(εΔf​)}]≤i∑k​⋅Pr[∣Yi​∣≥ln(δk​)⋅(εΔf​)]=i∑k​⋅exp(−ln(δk​))=k⋅(kδ​)=δ​ 】 例3.3 名字频度： 假设我们要使用 2010 年人口普查参与者数据，并从数据中统计出给定的 10,000 个名字里各个名字的频度。 这个问题可以用查询 f:N∣X∣→R10000f:\\mathbb{N}^{|\\mathcal{X}|} \\to \\mathbb{R}^{10000}f:N∣X∣→R10000 表示。 这是一个直方图查询，因此灵敏度Δf=1\\Delta f = 1Δf=1 ，因为每个人最多只能有一个名字。当频度查询是 (1,0)(1,0)(1,0)- 差分隐私的，并且概率要为 95％ ，我们使用上面的定理可以计算所有10,000名字的频度，其估计的相加误差不会超过 ln⁡(10000/0.05)≈12.2\\ln (10000/0.05) \\thickapprox 12.2ln(10000/0.05)≈12.2。 对于一个人口超过 300,000,000 的国家来说，这是非常低的错误！ 【补充5: 由例3.3可知：k=10000,Δf=1,ε=1,δ=1−0.95=0.05k=10000,\\Delta f = 1,\\varepsilon=1,\\delta = 1 - 0.95 = 0.05k=10000,Δf=1,ε=1,δ=1−0.95=0.05，并由定理3.8可以推得上述结论： Pr[max⁡i∈[10000]∣Yi∣≥ln⁡(100000.05)⋅(11)]≤0.05then   Pr[max⁡i∈[10000]∣Yi∣≤ln⁡(100000.05)⋅(11)]≥1−0.05Pr[max⁡i∈[10000]∣Yi∣≤ln⁡(100000.05)]≥0.95 \\begin{aligned} \\text{Pr}\\Big[\\max_{i \\in [10000]}|Y_i|\\geq \\ln(\\frac{10000}{0.05})\\cdot(\\frac{1}{1})\\Big] &\\leq 0.05\\\\ then \\ \\ \\ \\text{Pr}\\Big[\\max_{i \\in [10000]}|Y_i| \\leq \\ln(\\frac{10000}{0.05})\\cdot(\\frac{1}{1})\\Big] &\\geq 1 - 0.05\\\\ \\text{Pr}\\Big[\\max_{i \\in [10000]}|Y_i| \\leq \\ln(\\frac{10000}{0.05})\\Big] &\\geq 0.95 \\end{aligned} Pr[i∈[10000]max​∣Yi​∣≥ln(0.0510000​)⋅(11​)]then   Pr[i∈[10000]max​∣Yi​∣≤ln(0.0510000​)⋅(11​)]Pr[i∈[10000]max​∣Yi​∣≤ln(0.0510000​)]​≤0.05≥1−0.05≥0.95​ 】 差分隐私选择 例3.3中的任务是一个差分隐私选择：输出空间是离散的，任务是产生一个“最佳”答案。在例3.3中是选择输出是常用名频度最多的直方图单元。 例3.4 最常见的医学疾病 假设我们希望知道哪一个疾病是在一组被调查者的医疗史中最常见的，因此，需要对这些调查者进行一系列调查，调查其个人是否曾经接受过这些疾病的诊断。由于个人可能得过许多疾病，所以这些调查的敏感性可能很高。尽管如此，正如我们接下来描述的，这个任务可以通过在每个计数中添加 Lap(1/ε)Lap(1/\\varepsilon)Lap(1/ε) 噪声来解决（注意噪声的小范围，它独立于疾病的总数）。关键是 mmm 个噪音计数本身不会被发布（尽管“获胜”计数可以释放，没有额外的隐私损失）。（个人理解：m 个噪音计数即存在 m 种常见疾病，需要对 m 个常见疾病的统计计数增加噪声，并返回增加完噪声之后的最大值，即最常见的疾病。原文中的“获胜”应该指的是增加噪声最后的最大值。因为增加噪声之前与之后的最大值可能不一致。） Report Noisy Max 请考虑以下简单算法，目的是为了确定在 m 个计数查询中哪个具有最高数值：将独立生成的拉普拉斯噪声 Lap(1/ε)Lap(1/\\varepsilon)Lap(1/ε) 添加到每个计数中，并返回增加完噪声后最大计数索引（我们忽略之间可能的关联）。将此算法称为 Report Noisy Max。 注意到 “Report Noisy Max” 算法中的“信息最小化”原理：仅公开与最大值相对应的索引，而不是发布所有噪声计数并让分析人员找到最大值及其索引。 由于一个人的数据会影响所有计数，因此计数向量具有较高的 ℓ1\\ell_1ℓ1​ 灵敏度，即 Δf=m\\Delta f = mΔf=m，如果我们想使用拉普拉斯机制发布所有计数，则需要更多的噪声。 命题 3.9 Report Noisy Max 算法是 (ε,0)(\\varepsilon,0)(ε,0)-差分隐私。 【证明】 令 D=D′∪{a}D=D' \\cup \\{a\\}D=D′∪{a} 。 令 c,c′c,c'c,c′ 分别表示数据库为 D,D′D,D'D,D′的计数向量。我们使用两个属性： 1.计数的单调性。对于所有 j∈[m],cj≥cj′j \\in [m],c_j \\geq c'_jj∈[m],cj​≥cj′​ 2.利普希茨（Lipschitz）属性。对于所有 j∈[m],cj′+1≥cjj \\in [m],c'_j + 1 \\geq c_jj∈[m],cj′​+1≥cj​（注：Lipschitz连续，要求函数图像的曲线上任意两点连线的斜率一致有界，就是任意的斜率都小于同一个常数，这个常数就是 Lipschitz 常数，此处Lipschitz常数为1。） 定义任意 i∈[m]i \\in [m]i∈[m]，我们将限制 i 分别从 DDD 或 D′D'D′ 提取比率的上下界。 定义 r−ir_{-i}r−i​，从 Lap(1/ε)m−1Lap(1/\\varepsilon)^{m-1}Lap(1/ε)m−1 用于除第 i 个计数外的所有噪声计数。我们会单独讨论每个 r−ir_{-i}r−i​。我们使用符号 Pr[i∣ξ]\\text{Pr}[i|\\xi]Pr[i∣ξ] 表示 Report Noisy Max 算法在以 ξ\\xiξ 的条件下，输出为 i 的概率。 我们首先讨论 Pr[i∣D,r−i]≤eεPr[i∣D′,r−i]\\text{Pr}[i|D,r_{-i}] \\leq e^{\\varepsilon}\\text{Pr}[i|D',r_{-i}]Pr[i∣D,r−i​]≤eεPr[i∣D′,r−i​] 的情况。 定义： r∗=min⁡ri:ci+ri>cj+rj ∀j≠i r^* = \\min_{r_i}:c_i + r_i > c_j + r_j \\ \\forall j \\neq i r∗=ri​min​:ci​+ri​>cj​+rj​ ∀j≠i 注意，上面已经定义了 r−ir_{-i}r−i​ 的情况。当且仅当 ri≥r∗r_i \\geq r^*ri​≥r∗ 时， i 为数据库 DDD 的最大统计噪声输出。 对于所有 1≤j̸=i≤m1 \\leq j \\not ={i} \\leq m1≤j̸=i≤m： ci+r∗>cj+rj⟹(1+ci′)+r∗≥ci+r∗>cj+rj≥cj′+rj⟹ci′+(r∗+1)>cj′+rj \\begin{aligned} c_i + r^* &> c_j + r_j\\\\ \\implies (1 + c'_i) + r^* \\geq c_i + r^* &> c_j + r_j \\geq c'_j + r_j\\\\ \\implies c'_i + (r^* + 1) &> c'_j + r_j \\end{aligned} ci​+r∗⟹(1+ci′​)+r∗≥ci​+r∗⟹ci′​+(r∗+1)​>cj​+rj​>cj​+rj​≥cj′​+rj​>cj′​+rj​​ 因此，如果 ri≥r∗+1r_i \\geq r^* + 1ri​≥r∗+1 ，则当数据库为 D' 时，第 i 个统计计数是最大的，且噪声向量为 (ri,r−i)(r_i,r_{-i})(ri​,r−i​)。下面的概率取决于 ri∽Lap(1/ε)r_i \\backsim Lap(1/\\varepsilon)ri​∽Lap(1/ε) 的选择。 Pr[ri≥1+r∗]≥e−εPr[ri≥r∗]=e−εPr[i∣D,r−i]⟹Pr[i∣D′,r−i]≥Pr[ri≥1+r∗]≥e−εPr[ri≥r∗]=e−εPr[i∣D,r−i] \\begin{aligned} \\text{Pr}[r_i \\geq 1 + r^*] &\\geq e^{-\\varepsilon}\\text{Pr}[r_i \\geq r^*] = e^{-\\varepsilon}\\text{Pr}[i|D,r_{-i}]\\\\ \\implies \\text{Pr}[i|D',r_{-i}] \\geq \\text{Pr}[r_i \\geq 1 + r^*] &\\geq e^{-\\varepsilon}\\text{Pr}[r_i \\geq r^*] = e^{-\\varepsilon}\\text{Pr}[i|D,r_{-i}] \\end{aligned} Pr[ri​≥1+r∗]⟹Pr[i∣D′,r−i​]≥Pr[ri​≥1+r∗]​≥e−εPr[ri​≥r∗]=e−εPr[i∣D,r−i​]≥e−εPr[ri​≥r∗]=e−εPr[i∣D,r−i​]​ 上面等式两边乘上 eεe^{\\varepsilon}eε 第一种情况证明完毕。 证明第二种情况，即 Pr[i∣D′,r−i]≤eεPr[i∣D,r−i]\\text{Pr}[i|D',r_{-i}] \\leq e^{\\varepsilon}\\text{Pr}[i|D,r_{-i}]Pr[i∣D′,r−i​]≤eεPr[i∣D,r−i​] 定义： r∗=min⁡ri:ci′+ri>cj′+rj ∀j≠i r^* = \\min_{r_i}:c'_i + r_i > c'_j + r_j \\ \\forall j \\neq i r∗=ri​min​:ci′​+ri​>cj′​+rj​ ∀j≠i 注意，上面已经定义了 r−ir_{-i}r−i​ 的情况。当且仅当 ri≥r∗r_i \\geq r^*ri​≥r∗ 时， i 为数据库 DDD 的最大统计噪声输出。 对于所有 1≤j̸=i≤m1 \\leq j \\not ={i} \\leq m1≤j̸=i≤m： ci′+r∗>cj+rj⟹1+ci′+r∗>1+cj′+rj⟹ci′+(r∗+1)>(1+cj′)+rj⟹ci+(r∗+1)≥ci′+(r∗+1)>(1+cj′)+rj≥cj+rj \\begin{aligned} c'_i + r^* &> c_j + r_j\\\\ \\implies 1 + c'_i + r^* &> 1 + c'_j + r_j\\\\ \\implies c'_i + (r^* + 1) &> (1 + c'_j) + r_j\\\\ \\implies c_i + (r^* + 1) \\geq c'_i + (r^* + 1) &> (1 + c'_j) + r_j \\geq c_j + r_j\\\\ \\end{aligned} ci′​+r∗⟹1+ci′​+r∗⟹ci′​+(r∗+1)⟹ci​+(r∗+1)≥ci′​+(r∗+1)​>cj​+rj​>1+cj′​+rj​>(1+cj′​)+rj​>(1+cj′​)+rj​≥cj​+rj​​ 因此，如果 ri≥r∗+1r_i \\geq r^* + 1ri​≥r∗+1 ，则当数据库为 D 时，第 i 个统计计数是最大的，且噪声向量为 (ri,r−i)(r_i,r_{-i})(ri​,r−i​)。下面的概率取决于 ri∽Lap(1/ε)r_i \\backsim Lap(1/\\varepsilon)ri​∽Lap(1/ε) 的选择。 Pr[i∣D,r−i]≥Pr[ri≥r∗+1]≥e−εPr[ri≥r∗]=e−εPr[i∣D′,r−i] \\begin{aligned} \\text{Pr}[i|D,r_{-i}] \\geq \\text{Pr}[r_i \\geq r^* + 1 ] \\geq e^{-\\varepsilon}\\text{Pr}[r_i \\geq r^*] = e^{-\\varepsilon}\\text{Pr}[i|D',r_{-i}] \\end{aligned} Pr[i∣D,r−i​]≥Pr[ri​≥r∗+1]≥e−εPr[ri​≥r∗]=e−εPr[i∣D′,r−i​]​ 上面等式两边乘上 eεe^{\\varepsilon}eε 第二种情况证明完毕。 【命题 3.9 证毕】 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-25 08:00:04 "},"3-Basic-Techniques-and-Composition-Theorems/The-exponential-mechanism.html":{"url":"3-Basic-Techniques-and-Composition-Theorems/The-exponential-mechanism.html","title":"Exponential机制","keywords":"","body":"3.4 指数机制 我们在“最常见的名称”和“最常见的疾病”例子中，提到了响应的“效用”（名称或医疗疾病），在响应中我们使用拉普拉斯噪声估计了计数，并报告了噪声最大值。在这两个示例中，响应的效用都与生成的噪声值直接相关。也就是说，我们应使用与噪声大小相同的标度和相同的单位对名称或疾病的普遍程度进行适当地测量。 指数机制是为我们希望选择“最佳”响应而设计的，但将噪声直接添加到计算量可以完全破坏其价值，例如在拍卖中设定价格，目标是使收入最大化，但在最优价格中加入少量的正噪声（为了保护出价的隐私）可能会导致显著减少由此产生的收入，如下南瓜竞拍例子。 例 3.5（南瓜竞拍） 假设我们有大量的南瓜和四个竞标者：A，F，I，K，其中A，F，I分别出价1.00美元和K出价3.01美元，最优价格是多少？在 3.01美元时，收入为 3.01美元（只符合K的价格，故只有K买，收入3.01美元）；在 3.00美元 和 1.00美元 时候，收入为 3.00美元（只有A、F、I符合价格，收入共3美元）；但在 3.02美元时，收入为零（无人符合价格）！ 指数机制是使用任意效用函数（或任意非数字范围）回答查询的天然模块（原文为“building block”积木，此处翻译为模块），同时保留了差异隐私。给定任意范围 R\\mathcal{R}R，将指数机制定义为某些效用函数 u:N∣X∣×R→Ru:\\mathbb{N}^{|\\mathcal{X}|} \\times \\mathcal{R} \\to \\mathbb{R}u:N∣X∣×R→R，它将数据库输出对映射到效用分数。直观地讲，对于固定的数据库 xxx，用户更喜欢该机制输出 R\\mathcal{R}R 的某些元素具有最大的效用得分。请注意，当我们谈论效用分数 u:N∣X∣×R→Ru:\\mathbb{N}^{|\\mathcal{X}|} \\times \\mathcal{R} \\to \\mathbb{R}u:N∣X∣×R→R 的敏感度时，我们只关心 uuu 相对于其数据库参数的敏感性；效用函数 uuu 可以是任意敏感的： Δu=max⁡r∈R max⁡x,y:∥x−y∥1≤1∣u(x,r)−u(y,r)∣ \\Delta u = \\max_{r \\in \\mathcal{R}} \\ \\max_{x,y:\\Vert x-y\\Vert _1 \\leq 1}|u(x,r)-u(y,r)| Δu=r∈Rmax​ x,y:∥x−y∥1​≤1max​∣u(x,r)−u(y,r)∣ 从直观上来看指数机制，其思想是输出每个可能的 r∈Rr \\in \\mathcal{R}r∈R ，其概率与 exp⁡(εu(x,r)/Δu)\\exp(\\varepsilon u(x,r)/\\Delta u)exp(εu(x,r)/Δu) 成正比，这样隐私损失才能约为： ln⁡(exp⁡(εu(x,r)/Δu)exp⁡(εu(y,r)/Δu))=ε[u(x,r)−u(y,r)]/Δu≤ε \\ln \\Big(\\frac{\\exp(\\varepsilon u(x,r)/\\Delta u)}{\\exp(\\varepsilon u(y,r)/\\Delta u)}\\Big)=\\varepsilon[u(x,r)-u(y,r)]/\\Delta u \\leq \\varepsilon ln(exp(εu(y,r)/Δu)exp(εu(x,r)/Δu)​)=ε[u(x,r)−u(y,r)]/Δu≤ε （注：此处原文公式有误，翻译为更正后的公式） (个人理解：根据效用函数敏感度 Δu\\Delta uΔu 的定义可知，数据库 x,yx,yx,y 是相邻数据库，相差为 1，则可以构造构造一个机制，将效用得分和与输出概率关联，使得满足 ε\\varepsilonε-差分隐私定义的隐私损失。由 2.3节中的隐私损失（机制质量) 可得出：当机制正比于 exp⁡(εu(x,r)/Δu),(Pr[M(x)=ξ]∝exp⁡(εu(x,r)/Δu))\\exp(\\varepsilon u(x,r)/\\Delta u),(\\text{Pr}\\lbrack \\mathcal{M}(x) = \\xi \\rbrack \\propto \\exp(\\varepsilon u(x,r)/\\Delta u))exp(εu(x,r)/Δu),(Pr[M(x)=ξ]∝exp(εu(x,r)/Δu))， 该机制的隐私损失是 ε\\varepsilonε LM(x)∥M(y)(ξ)=ln⁡(Pr[M(x,u)=r]Pr[M(y,u)=r])=ln⁡(exp⁡(εu(x,r)/Δu)exp⁡(εu(y,r)/Δu)) \\mathcal{L}_{\\mathcal{M}(x)\\Vert \\mathcal{M}(y)}^{(\\xi)} = \\ln(\\frac{\\text{Pr}\\lbrack \\mathcal{M}(x,u) = r \\rbrack}{\\text{Pr}\\lbrack \\mathcal{M}(y,u) = r \\rbrack}) = \\ln \\Big(\\frac{\\exp(\\varepsilon u(x,r)/\\Delta u)}{\\exp(\\varepsilon u(y,r)/\\Delta u)}\\Big) LM(x)∥M(y)(ξ)​=ln(Pr[M(y,u)=r]Pr[M(x,u)=r]​)=ln(exp(εu(y,r)/Δu)exp(εu(x,r)/Δu)​) ) 这种直观的观点忽略了归一化项的某些影响，该归一化项出现的原因是，当有额外的人出现在数据库中，导致某些元素 r∈Rr \\in \\mathcal{R}r∈R 的效用减小而其他元素的效用增大。接下来定义的实际机制将一半的隐私预算用于归一化项的更改。 （个人理解：上述公式仅仅表明，当只有一个的回答 rrr 时，其隐私损失是符合差分隐私定义中的 ε\\varepsilonε。 但当可能有很多个回答时，我们就需要考虑到一个回答占总体回答概率的多少，即上段中提到的 “ 当有额外的人出现在数据库中，导致某些元素 r∈Rr \\in \\mathcal{R}r∈R 的效用减小而其他元素的效用增大 ” 。此处的归一化项（Normalization Term）指的是所有可能出现回答 r′∈Rr' \\in \\mathcal{R}r′∈R 的概率总和，类比离散变量的概率公式，Pr[ME(x,u,R)=r]=exp⁡(εu(x,r)2Δu)∑r′∈Rexp⁡(εu(x,r′)2Δu)\\text{Pr}[\\mathcal{M}_E(x,u,\\mathcal{R})=r] = \\frac{\\exp(\\frac{\\varepsilon u(x,r)}{2\\Delta u})}{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(x,r')}{2\\Delta u})}Pr[ME​(x,u,R)=r]=∑r′∈R​exp(2Δuεu(x,r′)​)exp(2Δuεu(x,r)​)​ 。这也解释了后文指数分布证明中的概率。） 定义3.4（指数机制） 指数机制 ME(x,u,R)\\mathcal{M}_E(x,u,\\mathcal{R})ME​(x,u,R) 选择并输出元素 r∈Rr \\in \\mathcal{R}r∈R 的概率与 exp⁡(εu(x,r)2Δu)\\exp\\big(\\frac{\\varepsilon u(x,r)}{2\\Delta u}\\big)exp(2Δuεu(x,r)​) 成正比。 指数机制可以在较大的任意域上定义复杂的分布，因此当 uuu 的范围在问题的自然参数中超多项式大时，可能无法有效地实现指数机制。 回到南瓜的例子，对数据库 xxx 上的价格 ppp 的效用就是当价格为 ppp 且需求曲线如 xxx 所示时获得的利润。重要的是，潜在价格的范围应与实际出价无关。否则，将存在一个价格，其中一个数据集中的权重为非零，而相邻集合中的权重为零，这违反了差分隐私。 定理 3.10 指数机制满足 (ε,0)(\\varepsilon,0)(ε,0) -差分隐私。 【证明】 为了清楚起见，我们假设指数机制的范围 R\\mathcal{R}R 是有限的，但这是不必要的。在所有的差分隐私证明中，我们考虑指数机制的一个实例,即在两个相邻的数据库 x∈N∣X∣,y∈N∣X∣,∥x−y∥1≤1x \\in \\mathbb{N}^{|\\mathcal{X}|},y \\in \\mathbb{N}^{|\\mathcal{X}|},\\Vert x-y\\Vert _1 \\leq 1x∈N∣X∣,y∈N∣X∣,∥x−y∥1​≤1上输出某个元素 r∈Rr \\in \\mathcal{R}r∈R 的概率之比。 Pr[ME(x,u,R)=r]Pr[ME(y,u,R)=r]=(exp⁡(εu(x,r)2Δu)∑r′∈Rexp⁡(εu(x,r′)2Δu))(exp⁡(εu(y,r)2Δu)∑r′∈Rexp⁡(εu(y,r′)2Δu))=(exp⁡(εu(x,r)2Δu)exp⁡(εu(y,r)2Δu))⋅(∑r′∈Rexp⁡(εu(y,r′)2Δu)∑r′∈Rexp⁡(εu(x,r′)2Δu))=exp⁡(ε(u(x,r′)−u(y,r′))2Δu)⋅(∑r′∈Rexp⁡(εu(y,r′)2Δu)∑r′∈Rexp⁡(εu(x,r′)2Δu))≤exp⁡(ε2)⋅exp⁡(ε2)⋅(∑r′∈Rexp⁡(εu(x,r′)2Δu)∑r′∈Rexp⁡(εu(x,r′)2Δu))=exp⁡(ε) \\begin{aligned} \\frac{\\text{Pr}[\\mathcal{M}_E(x,u,\\mathcal{R})=r]}{\\text{Pr}[\\mathcal{M}_E(y,u,\\mathcal{R})=r]} &= \\frac{\\Big(\\frac{\\exp(\\frac{\\varepsilon u(x,r)}{2\\Delta u})}{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(x,r')}{2\\Delta u})}\\Big)}{\\Big(\\frac{\\exp(\\frac{\\varepsilon u(y,r)}{2\\Delta u})}{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(y,r')}{2\\Delta u})}\\Big)}\\\\ &= \\Big(\\frac{\\exp(\\frac{\\varepsilon u(x,r)}{2\\Delta u})}{\\exp(\\frac{\\varepsilon u(y,r)}{2\\Delta u})}\\Big) \\cdot \\Big(\\frac{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(y,r')}{2\\Delta u})}{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(x,r')}{2\\Delta u})} \\Big)\\\\ &= \\exp\\Big(\\frac{\\varepsilon(u(x,r')-u(y,r'))}{2\\Delta u} \\Big)\\cdot \\Big(\\frac{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(y,r')}{2\\Delta u})}{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(x,r')}{2\\Delta u})}\\Big)\\\\ &\\leq \\exp(\\frac{\\varepsilon}{2})\\cdot\\exp(\\frac{\\varepsilon}{2})\\cdot\\Big(\\frac{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(x,r')}{2\\Delta u})}{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(x,r')}{2\\Delta u})}\\Big)\\\\ &= \\exp(\\varepsilon) \\end{aligned} Pr[ME​(y,u,R)=r]Pr[ME​(x,u,R)=r]​​=(∑r′∈R​exp(2Δuεu(y,r′)​)exp(2Δuεu(y,r)​)​)(∑r′∈R​exp(2Δuεu(x,r′)​)exp(2Δuεu(x,r)​)​)​=(exp(2Δuεu(y,r)​)exp(2Δuεu(x,r)​)​)⋅(∑r′∈R​exp(2Δuεu(x,r′)​)∑r′∈R​exp(2Δuεu(y,r′)​)​)=exp(2Δuε(u(x,r′)−u(y,r′))​)⋅(∑r′∈R​exp(2Δuεu(x,r′)​)∑r′∈R​exp(2Δuεu(y,r′)​)​)≤exp(2ε​)⋅exp(2ε​)⋅(∑r′∈R​exp(2Δuεu(x,r′)​)∑r′∈R​exp(2Δuεu(x,r′)​)​)=exp(ε)​ 同样，对称情况也成立 Pr[ME(y,u,R)=r]Pr[ME(x,u,R)=r]≥exp⁡(−ε)\\frac{\\text{Pr}[\\mathcal{M}_E(y,u,\\mathcal{R})=r]}{\\text{Pr}[\\mathcal{M}_E(x,u,\\mathcal{R})=r]} \\geq \\exp(-\\varepsilon)Pr[ME​(x,u,R)=r]Pr[ME​(y,u,R)=r]​≥exp(−ε) 【定理 3.10 证毕】 【补充：原文中，上述公式个人认为有问题，证明的公式中符号有误，下面是个人更正，同时增加证明过程辅助理解。该证明需要用到上文关于指数机制隐私损失部分证明结论，其结论如下： LM(x)∥M(y)(ξ)=ln⁡(Pr[ME(x,u,R)=r]Pr[ME(y,u,R)=r])=ln⁡(exp⁡(εu(x,r)/Δ2u)exp⁡(εu(y,r)/Δ2u))=ε[u(x,r)−u(y,r)]/Δ2u≤ε/2⟹exp⁡(εu(x,r)/Δ2u)≤eε/2⋅exp⁡(εu(y,r)/Δ2u)⟹∑r′∈Rexp⁡(εu(x,r′)2Δu)≤eε/2⋅∑r′∈Rexp⁡(εu(y,r′)2Δu) \\begin{aligned} \\mathcal{L}_{\\mathcal{M}(x)\\Vert \\mathcal{M}(y)}^{(\\xi)} = \\ln(\\frac{\\text{Pr}[\\mathcal{M}_E(x,u,\\mathcal{R})=r]}{\\text{Pr}[\\mathcal{M}_E(y,u,\\mathcal{R})=r]}) &= \\ln \\Big(\\frac{\\exp(\\varepsilon u(x,r)/\\Delta 2u)}{\\exp(\\varepsilon u(y,r)/\\Delta 2u)}\\Big) \\\\ &=\\varepsilon[u(x,r)-u(y,r)]/\\Delta 2u \\leq \\varepsilon/2\\\\ \\implies \\exp(\\varepsilon u(x,r)/\\Delta 2u) &\\leq e^{\\varepsilon/2} \\cdot \\exp(\\varepsilon u(y,r)/\\Delta 2u)\\\\ \\implies \\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(x,r')}{2\\Delta u}) &\\leq e^{\\varepsilon/2} \\cdot \\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(y,r')}{2\\Delta u}) \\end{aligned} LM(x)∥M(y)(ξ)​=ln(Pr[ME​(y,u,R)=r]Pr[ME​(x,u,R)=r]​)⟹exp(εu(x,r)/Δ2u)⟹r′∈R∑​exp(2Δuεu(x,r′)​)​=ln(exp(εu(y,r)/Δ2u)exp(εu(x,r)/Δ2u)​)=ε[u(x,r)−u(y,r)]/Δ2u≤ε/2≤eε/2⋅exp(εu(y,r)/Δ2u)≤eε/2⋅r′∈R∑​exp(2Δuεu(y,r′)​)​ 很自然的我们由 Δu=max⁡r∈R max⁡x,y:∥x−y∥1≤1∣u(x,r)−u(y,r)∣\\Delta u = \\max_{r \\in \\mathcal{R}} \\ \\max_{x,y:\\Vert x-y\\Vert _1 \\leq 1}|u(x,r)-u(y,r)|Δu=maxr∈R​ maxx,y:∥x−y∥1​≤1​∣u(x,r)−u(y,r)∣ 可以推知 u(x,r)−u(y,r)≤Δuu(x,r)-u(y,r) \\leq \\Delta uu(x,r)−u(y,r)≤Δu ，经过放缩之后得到结论。具体如下： Pr[ME(x,u,R)=r]Pr[ME(y,u,R)=r]=(exp⁡(εu(x,r)2Δu)∑r′∈Rexp⁡(εu(x,r′)2Δu))(exp⁡(εu(y,r)2Δu)∑r′∈Rexp⁡(εu(y,r′)2Δu))=(exp⁡(εu(x,r)2Δu)exp⁡(εu(y,r)2Δu))⋅(∑r′∈Rexp⁡(εu(y,r′)2Δu)∑r′∈Rexp⁡(εu(x,r′)2Δu))=exp⁡(ε(u(x,r)−u(y,r))2Δu)⋅(∑r′∈Rexp⁡(εu(y,r′)2Δu)∑r′∈Rexp⁡(εu(x,r′)2Δu))≤exp⁡(ε2)⋅(exp⁡(ε2)⋅∑r′∈Rexp⁡(εu(x,r′)2Δu)∑r′∈Rexp⁡(εu(x,r′)2Δu))=exp⁡(ε) \\begin{aligned} \\frac{\\text{Pr}[\\mathcal{M}_E(x,u,\\mathcal{R})=r]}{\\text{Pr}[\\mathcal{M}_E(y,u,\\mathcal{R})=r]} &= \\frac{\\Big(\\frac{\\exp(\\frac{\\varepsilon u(x,r)}{2\\Delta u})}{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(x,r')}{2\\Delta u})}\\Big)}{\\Big(\\frac{\\exp(\\frac{\\varepsilon u(y,r)}{2\\Delta u})}{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(y,r')}{2\\Delta u})}\\Big)}\\\\ &= \\Big(\\frac{\\exp(\\frac{\\varepsilon u(x,r)}{2\\Delta u})}{\\exp(\\frac{\\varepsilon u(y,r)}{2\\Delta u})}\\Big) \\cdot \\Big(\\frac{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(y,r')}{2\\Delta u})}{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(x,r')}{2\\Delta u})} \\Big)\\\\ &= \\exp\\Big(\\frac{\\varepsilon(u(x,r)-u(y,r))}{2\\Delta u} \\Big)\\cdot \\Big(\\frac{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(y,r')}{2\\Delta u})}{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(x,r')}{2\\Delta u})}\\Big)\\\\ &\\leq \\exp(\\frac{\\varepsilon}{2})\\cdot\\Big(\\frac{\\exp(\\frac{\\varepsilon}{2}) \\cdot \\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(x,r')}{2\\Delta u})}{\\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(x,r')}{2\\Delta u})}\\Big)\\\\ &= \\exp(\\varepsilon) \\end{aligned} Pr[ME​(y,u,R)=r]Pr[ME​(x,u,R)=r]​​=(∑r′∈R​exp(2Δuεu(y,r′)​)exp(2Δuεu(y,r)​)​)(∑r′∈R​exp(2Δuεu(x,r′)​)exp(2Δuεu(x,r)​)​)​=(exp(2Δuεu(y,r)​)exp(2Δuεu(x,r)​)​)⋅(∑r′∈R​exp(2Δuεu(x,r′)​)∑r′∈R​exp(2Δuεu(y,r′)​)​)=exp(2Δuε(u(x,r)−u(y,r))​)⋅(∑r′∈R​exp(2Δuεu(x,r′)​)∑r′∈R​exp(2Δuεu(y,r′)​)​)≤exp(2ε​)⋅(∑r′∈R​exp(2Δuεu(x,r′)​)exp(2ε​)⋅∑r′∈R​exp(2Δuεu(x,r′)​)​)=exp(ε)​ 此处也解释了为什么要用 2Δu2\\Delta u2Δu ，其目的是为了弥补归一化项对机制造成的影响，如若不使用 2Δu2\\Delta u2Δu ，易推知机制的隐私损失为 2ε2\\varepsilon2ε 】 指数机制通常可以提供强大的效用保证，因为随着效用得分的下降，它会指数级折减结果。对于给定的数据库 xxx 和给定的效用函数：u:N∣X∣×R→Ru:\\mathbb{N}^{|\\mathcal{X}|} \\times \\mathcal{R} \\to \\mathbb{R}u:N∣X∣×R→R ，令 OPTu(x)=max⁡r∈Ru(x,r)\\text{OPT}_u(x)=\\max_{r \\in \\mathcal{R}}u(x,r)OPTu​(x)=maxr∈R​u(x,r) 表示任何元素 r∈Rr \\in \\mathcal{R}r∈R 相对于数据库 xxx 的最大效用得分。我们将限制指数机制返回 R\\mathcal{R}R 的“良好”元素的概率，其中“良好”将根据 OPTu(x)\\text{OPT}_u(x)OPTu​(x) 进行度量。这种做法的结果是，返回元素 rrr 的效用得分不太可能低于 OPTu(x)\\text{OPT}_u(x)OPTu​(x) 超过 O(Δu/ε)log⁡∣R∣O(\\Delta u/\\varepsilon)\\log|\\mathcal{R}|O(Δu/ε)log∣R∣ 可加因子。 定理 3.11 固定数据库 xxx，令 ROPT=r∈R:u(x,r)=OPTu(x)\\mathcal{R}_{\\text{OPT}}={r \\in \\mathcal{R}:u(x,r)=\\text{OPT}_u(x)}ROPT​=r∈R:u(x,r)=OPTu​(x) 表示 R\\mathcal{R}R 中获得效用分数 OPTu(x)\\text{OPT}_u(x)OPTu​(x) 的元素集合。则： Pr[u(ME(x,u,R))≤OPTu(x)−2Δuε(ln⁡(∣R∣∣ROPT∣)+t)]≤e−t \\text{Pr}\\Big[u(\\mathcal{M}_E(x,u,\\mathcal{R})) \\leq \\text{OPT}_u(x)-\\frac{2\\Delta u}{\\varepsilon}\\Big(\\ln \\Big(\\frac{|\\mathcal{R}|}{|\\mathcal{R}_{\\text{OPT}}|}\\Big)+t\\Big)\\Big] \\leq e^{-t} Pr[u(ME​(x,u,R))≤OPTu​(x)−ε2Δu​(ln(∣ROPT​∣∣R∣​)+t)]≤e−t 【证明】 Pr[u(ME(x,u,R))≤c]≤∣R∣exp⁡(εc/2Δu)∣ROPT∣exp⁡(εOPTu(x)/2Δu)=∣R∣∣ROPT∣exp⁡(ε(c−OPTu(x))2Δu) \\begin{aligned} \\text{Pr}\\Big[u(\\mathcal{M}_E(x,u,\\mathcal{R})) \\leq c\\Big] &\\leq \\frac{|\\mathcal{R}|\\exp(\\varepsilon c / 2\\Delta u)}{|\\mathcal{R}_{\\text{OPT}}|\\exp(\\varepsilon \\text{OPT}_u(x)/2\\Delta u)}\\\\ &= \\frac{|\\mathcal{R}|}{|\\mathcal{R}_{\\text{OPT}}|}\\exp\\Big(\\frac{\\varepsilon(c-\\text{OPT}_u(x))}{2\\Delta u} \\Big) \\end{aligned} Pr[u(ME​(x,u,R))≤c]​≤∣ROPT​∣exp(εOPTu​(x)/2Δu)∣R∣exp(εc/2Δu)​=∣ROPT​∣∣R∣​exp(2Δuε(c−OPTu​(x))​)​ 这个不等式是由这样一个观察结果得出的：每一个 r∈R,u(x,r)≤cr\\in \\mathcal{R},u(x,r)\\leq cr∈R,u(x,r)≤c 所具有最大的未归一化概率质量为 exp⁡(εc/2Δu)\\exp(\\varepsilon c/2\\Delta u)exp(εc/2Δu) 1>^{}1>，因此这类“坏”元素 rrr 的整个集合的未归一化概率质量总和最大为 ∣R∣exp⁡(εc/2Δu)|\\mathcal{R}|\\exp(\\varepsilon c/2\\Delta u)∣R∣exp(εc/2Δu) 。与此相反，我们知道至少存在 ∣ROPT∣≥1|\\mathcal{R}_{\\text{OPT}}|\\geq 1∣ROPT​∣≥1 个元素具有 u(x,r)=OPTu(x)u(x,r)=\\text{OPT}_u(x)u(x,r)=OPTu​(x) ，并且因此未归一化概率质量为 exp⁡(εOPTu(x)/2Δu)\\exp(\\varepsilon \\text{OPT}_u(x)/2\\Delta u)exp(εOPTu​(x)/2Δu) ，因此这是正规化项的下界。 【定理 3.11 证毕】 这个定理是通过插入c的适当值得出的。 （注：概率质量（probability mass）：离散随机变量在各特定取值上的概率，概率质量函数是对离散随机变量定义的，本身代表该值的概率；概率密度函数是对连续随机变量定义的，本身不是概率，只有对连续随机变量的概率密度函数在某区间内进行积分后才是概率。其定义为：假设 XXX 是一个定义在可数样本空间 SSS 上的离散随机变量 S⊆RS \\subseteq \\mathbb{R}S⊆R，则其概率质量函数 fX(x)f_{X}(x)fX​(x) 为: fX(x)={textPr(X=x),x∈S,0,x∈R\\S. f_{X}(x)={\\begin{cases}\\\\text{Pr}(X=x),&x\\in S,\\\\0,&x\\in {\\mathbb {R}}\\backslash S.\\end{cases}} fX​(x)=⎩⎪⎨⎪⎧​textPr(X=x),0,​x∈S,x∈R\\S.​ ） 【补充 根据定义，每一个 r∈Rr\\in \\mathcal{R}r∈R，且其效用得分是 u(x,r)≤cu(x,r)\\leq cu(x,r)≤c ，所有这些 rrr 的未归一化概率质量最大不超过 exp⁡(εc/2Δu)\\exp(\\varepsilon c/2\\Delta u)exp(εc/2Δu)。那么这些 rrr 的概率总和为：∣R∣exp⁡(εc/2Δu)|\\mathcal{R}|\\exp(\\varepsilon c/2\\Delta u)∣R∣exp(εc/2Δu)。我们又知道，ROPT⊆R\\mathcal{R}_{\\text{OPT}} \\subseteq \\mathcal{R}ROPT​⊆R，所以 ∣ROPT∣exp⁡(εOPTu(x)/2Δu)≤∑r′∈Rexp⁡(εu(x,r′)2Δu)|\\mathcal{R}_{\\text{OPT}}|\\exp(\\varepsilon \\text{OPT}_u(x)/2\\Delta u) \\leq \\sum_{r'\\in \\mathcal{R}}\\exp(\\frac{\\varepsilon u(x,r')}{2\\Delta u})∣ROPT​∣exp(εOPTu​(x)/2Δu)≤∑r′∈R​exp(2Δuεu(x,r′)​)。分子增大，分母减少，故下面不等式成立： Pr[u(ME(x,u,R))≤c]≤∣R∣exp⁡(εc/2Δu)∣ROPT∣exp⁡(εOPTu(x)/2Δu) \\text{Pr}\\Big[u(\\mathcal{M}_E(x,u,\\mathcal{R})) \\leq c\\Big] \\leq \\frac{|\\mathcal{R}|\\exp(\\varepsilon c / 2\\Delta u)}{|\\mathcal{R}_{\\text{OPT}}|\\exp(\\varepsilon \\text{OPT}_u(x)/2\\Delta u)} Pr[u(ME​(x,u,R))≤c]≤∣ROPT​∣exp(εOPTu​(x)/2Δu)∣R∣exp(εc/2Δu)​ 我们将不等式右边变形推导得到： Pr[u(ME(x,u,R))≤c]≤∣R∣exp⁡(εc/2Δu)∣ROPT∣exp⁡(εOPTu(x)/2Δu)=exp⁡(ln⁡(∣R∣∣ROPT∣)+ε(c−OPTu(x))2Δu) \\begin{aligned} \\text{Pr}\\Big[u(\\mathcal{M}_E(x,u,\\mathcal{R})) \\leq c\\Big] &\\leq \\frac{|\\mathcal{R}|\\exp(\\varepsilon c / 2\\Delta u)}{|\\mathcal{R}_{\\text{OPT}}|\\exp(\\varepsilon \\text{OPT}_u(x)/2\\Delta u)}\\\\ &= \\exp\\Big(\\ln\\big(\\frac{|\\mathcal{R}|}{|\\mathcal{R}_{\\text{OPT}}|}\\big) + \\frac{\\varepsilon(c-\\text{OPT}_u(x))}{2\\Delta u} \\Big)\\\\ \\end{aligned} Pr[u(ME​(x,u,R))≤c]​≤∣ROPT​∣exp(εOPTu​(x)/2Δu)∣R∣exp(εc/2Δu)​=exp(ln(∣ROPT​∣∣R∣​)+2Δuε(c−OPTu​(x))​)​ 令 −t=ln⁡(∣R∣∣ROPT∣)+ε(c−OPTu(x))2Δu-t = \\ln\\big(\\frac{|\\mathcal{R}|}{|\\mathcal{R}_{\\text{OPT}}|}\\big) + \\frac{\\varepsilon(c-\\text{OPT}_u(x))}{2\\Delta u}−t=ln(∣ROPT​∣∣R∣​)+2Δuε(c−OPTu​(x))​ 求得 c=OPTu(x)−2Δuε(ln⁡(∣R∣∣ROPT∣)+t)c=\\text{OPT}_u(x)-\\frac{2\\Delta u}{\\varepsilon}\\Big(\\ln \\Big(\\frac{|\\mathcal{R}|}{|\\mathcal{R}_{\\text{OPT}}|}\\Big)+t\\Big)c=OPTu​(x)−ε2Δu​(ln(∣ROPT​∣∣R∣​)+t) 将 ccc 带入不等式即为 定理3.11 所示。 】 由于我们总是有 ∣ROPT∣≥1|\\mathcal{R}_{\\text{OPT}}|\\geq 1∣ROPT​∣≥1，我们可以更普遍地使用以下简单的推论： 推论 3.12 定义一个数据库 xxx，我们有： Pr[u(ME(x,u,R))≤OPTu(x)−2Δuε(ln⁡(∣R∣)+t)]≤e−t \\text{Pr}\\Big[u(\\mathcal{M}_E(x,u,\\mathcal{R})) \\leq \\text{OPT}_u(x)-\\frac{2\\Delta u}{\\varepsilon}(\\ln (|\\mathcal{R}|)+t)\\Big] \\leq e^{-t} Pr[u(ME​(x,u,R))≤OPTu​(x)−ε2Δu​(ln(∣R∣)+t)]≤e−t 从定理3.11和推论3.12的证明中可以看出，指数机制特别容易分析。 例3.6（二选一） 考虑一个简单的问题，即确定 A 和 B 两种疾病中哪一种更常见。假设疾病A的真实计数为 0，疾病B的真实计数为 c>0c>0c>0。我们的效用概念将与实际计数联系起来，这样计数越大的疾病种类将具有更高的效用，且Δu=1\\Delta u=1Δu=1。因此，A 的效用为 0，B 的效用为 c。使用指数机制，我们可以立即应用推论3.12，输出错误结果 A 概率至多为 2e−c(ε/(2Δu))=2e−cε/22e^{-c(\\varepsilon / (2\\Delta u))}=2e^{-c\\varepsilon/2}2e−c(ε/(2Δu))=2e−cε/2 。 【补充：此处 A 的效用为 0 ，则 Pr[u(ME(x,u,R))≤0]\\text{Pr}\\Big[u(\\mathcal{M}_E(x,u,\\mathcal{R}))\\leq 0\\Big]Pr[u(ME​(x,u,R))≤0] 由 推论3.12，令 OPTu(x)−2Δuε(ln⁡(∣R∣)+t)=0\\text{OPT}_u(x)-\\frac{2\\Delta u}{\\varepsilon}(\\ln (|\\mathcal{R}|)+t) = 0OPTu​(x)−ε2Δu​(ln(∣R∣)+t)=0，由于 ∣R∣=2,Δu=1,OPTu(x)=c|\\mathcal{R}|=2,\\Delta u=1,\\text{OPT}_u(x)=c∣R∣=2,Δu=1,OPTu​(x)=c 可以推得 t=cε/2−ln⁡2t = c\\varepsilon/2-\\ln2t=cε/2−ln2，即： Pr[u(ME(x,u,R))≤OPTu(x)−2ε(ln⁡2+t)]≤e−tPr[u(ME(x,u,R))≤0]≤e−(cε/2−ln⁡2)Pr[u(ME(x,u,R))≤0]≤2e−cε/2 \\begin{aligned} \\text{Pr}\\Big[u(\\mathcal{M}_E(x,u,\\mathcal{R})) \\leq \\text{OPT}_u(x)-\\frac{2}{\\varepsilon}(\\ln2+t)\\Big] &\\leq e^{-t}\\\\ \\text{Pr}\\Big[u(\\mathcal{M}_E(x,u,\\mathcal{R})) \\leq 0\\Big] &\\leq e^{-(c\\varepsilon/2-\\ln2)}\\\\ \\text{Pr}\\Big[u(\\mathcal{M}_E(x,u,\\mathcal{R})) \\leq 0\\Big] &\\leq 2e^{-c\\varepsilon/2}\\\\ \\end{aligned} Pr[u(ME​(x,u,R))≤OPTu​(x)−ε2​(ln2+t)]Pr[u(ME​(x,u,R))≤0]Pr[u(ME​(x,u,R))≤0]​≤e−t≤e−(cε/2−ln2)≤2e−cε/2​ 】 分析 Report Noisy Max 似乎更为复杂，因为它需要了解当添加到 A 的计数中的噪声为正而添加到 B 的计数中的噪声为负时（概率为1/4）情况下会发生什么。 如果向数据集中添加元素不能导致函数值减小，则函数在数据集中是单调的。计数查询是单调的；通过向买家集合提供固定价格而获得的收入也是单调的。 考虑 Report One-Sided Noisy Arg-Max 机制，在单调效用的情况下，该机制将噪声（从参数为 ε/Δu\\varepsilon/\\Delta uε/Δu 的单侧指数分布中提取）添加到每个潜在输出的效用函数中；或在非单调效用的情况下，添加的噪声从参数为 ε/2Δu\\varepsilon/2\\Delta uε/2Δu 的单侧指数分布中提取，并报告最大结果。 使用该算法，其隐私证明几乎与 Report Noisy Max 相同（但当效用函数是非单调时造成了两倍损失），我们立即在上面的示例3.6中推得，选中输出疾病 A 的概率比结果 B 小，输出疾病 A 的概率等于参数为 c(ε/Δu)=cεc(\\varepsilon/\\Delta u)=c\\varepsilonc(ε/Δu)=cε 的指数分布。 定理3.13 当 Report One-Sided Noisy Arg-Max 机制使用参数 ε/2Δu\\varepsilon/2\\Delta uε/2Δu 运行时，在输出上会产生与指数机制相同的分布。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-16 09:52:42 "},"3-Basic-Techniques-and-Composition-Theorems/Composition-theorems/Composition-theorems.html":{"url":"3-Basic-Techniques-and-Composition-Theorems/Composition-theorems/Composition-theorems.html","title":"合成定理","keywords":"","body":"3.5 合成定理 既然我们已经有了几个用于设计差分隐私算法的模块，那么了解如何将它们结合起来设计更复杂的算法就变得非常重要。为了使用这些工具，我们希望两个差分隐私算法的组合本身是差分隐私的。事实上，两个差分隐私算法的组合确实是差分隐私的。当然，参数 ε\\varepsilonε 和 δ\\deltaδ 必然会降级的——考虑使用拉普拉斯机制重复计算相同的统计，每次都是ε-差分隐私。每一个机制实例给出的答案的平均值最终会收敛到统计的真实值，因此我们不能避免我们的隐私保护强度会随着重复使用而降低。在这一节中，我们给出了一些定理，说明当组合差分隐私子程序时，参数 ε\\varepsilonε 和 δ\\deltaδ 是如何精确组合的。 让我们先从一个简单的预热开始：我们将看到独立的 (ε1,0)(\\varepsilon_1,0)(ε1​,0) -差分隐私算法和（(ε2,0)(\\varepsilon_2,0)(ε2​,0) -差分隐私算法使用在一起是 (ε1+ε2,0)(\\varepsilon_1 + \\varepsilon_2,0)(ε1​+ε2​,0) -差分隐私算法。 定理 3.14 设 M1:N∣X∣→R1\\mathcal{M}_1:\\mathbb{N}^{|\\mathcal{X}|}\\to \\mathcal{R}_1M1​:N∣X∣→R1​ 为 ε1\\varepsilon_1ε1​ -差分隐私算法，设 M2:N∣X∣→R2\\mathcal{M}_2:\\mathbb{N}^{|\\mathcal{X}|}\\to \\mathcal{R}_2M2​:N∣X∣→R2​ 为 ε2\\varepsilon_2ε2​ -差分隐私算法。然后将它们的组合，定义为M1,2:N∣X∣→R1×R2\\mathcal{M}_{1,2}:\\mathbb{N}^{|\\mathcal{X}|}\\to \\mathcal{R}_1 \\times \\mathcal{R}_2M1,2​:N∣X∣→R1​×R2​ ，映射为： M1,2(x)=(M1(x),M2(x))\\mathcal{M}_{1,2}(x) = (\\mathcal{M}_{1}(x),\\mathcal{M}_{2}(x))M1,2​(x)=(M1​(x),M2​(x)) 是 (ε1+ε2)(\\varepsilon_1 + \\varepsilon_2)(ε1​+ε2​) -差分隐私算法。 【证明】 令 x,y∈N∣X∣x,y \\in \\mathbb{N}^{|\\mathcal{X}|}x,y∈N∣X∣ 满足 ∥x−y∥1≤1\\Vert x-y\\Vert _1 \\leq 1∥x−y∥1​≤1，任意 (r1,r2)∈R1×R2(r_1,r_2) \\in \\mathcal{R}_1 \\times \\mathcal{R}_2(r1​,r2​)∈R1​×R2​，则： Pr[M1,2(x)=(r1,r2)]Pr[M1,2(y)=(r1,r2)]=Pr[M1(x)=r1]Pr[M2(x)=r2]Pr[M1(y)=r1]Pr[M2(x)=r2]=(Pr[M1(x)=r1]Pr[M1(y)=r1])(Pr[M2(x)=r2]Pr[M2(x)=r2])≤exp⁡(ε1)exp⁡(ε2)=exp⁡(ε1+ε2) \\begin{aligned} \\frac{\\text{Pr}[\\mathcal{M}_{1,2}(x)=(r_1,r_2)]}{\\text{Pr}[\\mathcal{M}_{1,2}(y)=(r_1,r_2)]} &= \\frac{\\text{Pr}[\\mathcal{M}_{1}(x)=r_1]\\text{Pr}[\\mathcal{M}_{2}(x)=r_2]}{\\text{Pr}[\\mathcal{M}_{1}(y)=r_1]\\text{Pr}[\\mathcal{M}_{2}(x)=r_2]}\\\\ &= \\Big(\\frac{\\text{Pr}[\\mathcal{M}_{1}(x)=r_1]}{\\text{Pr}[\\mathcal{M}_{1}(y)=r_1]}\\Big)\\Big(\\frac{\\text{Pr}[\\mathcal{M}_{2}(x)=r_2]}{\\text{Pr}[\\mathcal{M}_{2}(x)=r_2]}\\Big)\\\\ &\\leq \\exp(\\varepsilon_1)\\exp(\\varepsilon_2)\\\\ &= \\exp(\\varepsilon_1+\\varepsilon_2) \\end{aligned} Pr[M1,2​(y)=(r1​,r2​)]Pr[M1,2​(x)=(r1​,r2​)]​​=Pr[M1​(y)=r1​]Pr[M2​(x)=r2​]Pr[M1​(x)=r1​]Pr[M2​(x)=r2​]​=(Pr[M1​(y)=r1​]Pr[M1​(x)=r1​]​)(Pr[M2​(x)=r2​]Pr[M2​(x)=r2​]​)≤exp(ε1​)exp(ε2​)=exp(ε1​+ε2​)​ 由对称性，Pr[M1,2(y)=(r1,r2)]Pr[M1,2(x)=(r1,r2)]≥exp⁡(−(ε1+ε2))\\frac{\\text{Pr}[\\mathcal{M}_{1,2}(y)=(r_1,r_2)]}{\\text{Pr}[\\mathcal{M}_{1,2}(x)=(r_1,r_2)]} \\geq \\exp(-(\\varepsilon_1+\\varepsilon_2))Pr[M1,2​(x)=(r1​,r2​)]Pr[M1,2​(y)=(r1​,r2​)]​≥exp(−(ε1​+ε2​)) 也成立。 【定理 3.14 证毕】 组合定理能反复应用以获得以下推论： 推论 3.15 令 Mi:N∣X∣→Ri\\mathcal{M}_i:\\mathbb{N}^{|\\mathcal{X}|}\\to \\mathcal{R}_iMi​:N∣X∣→Ri​ 是 (εi,0)(\\varepsilon_i,0)(εi​,0)- 差分隐私算法（i∈[k]i \\in [k]i∈[k]）。如果将 M[k]:N∣X∣→∏i=1kRi\\mathcal{M}_{[k]}:\\mathbb{N}^{|\\mathcal{X}|}\\to \\prod_{i=1}^{k}\\mathcal{R}_iM[k]​:N∣X∣→∏i=1k​Ri​ 定义为 ：(M1(x),...,Mk(x))(\\mathcal{M}_{1}(x),...,\\mathcal{M}_{k}(x))(M1​(x),...,Mk​(x))，则 M[k]\\mathcal{M}_{[k]}M[k]​ 是 (∑i=1kεi,0)(\\sum_{i=1}^{k}\\varepsilon_i,0)(∑i=1k​εi​,0)- 差分隐私。 该定理推广到 (ε,δ)(\\varepsilon,\\delta)(ε,δ)-差分隐私的证明见附录B： 定理 3.16 令 Mi:N∣X∣→Ri\\mathcal{M}_i:\\mathbb{N}^{|\\mathcal{X}|}\\to \\mathcal{R}_iMi​:N∣X∣→Ri​ 是 (εi,δi)(\\varepsilon_i,\\delta_i)(εi​,δi​)- 差分隐私算法（i∈[k]i \\in [k]i∈[k]）。如果将 M[k]:N∣X∣→∏i=1kRi\\mathcal{M}_{[k]}:\\mathbb{N}^{|\\mathcal{X}|}\\to \\prod_{i=1}^{k}\\mathcal{R}_iM[k]​:N∣X∣→∏i=1k​Ri​ 定义为 ：(M1(x),...,Mk(x))(\\mathcal{M}_{1}(x),...,\\mathcal{M}_{k}(x))(M1​(x),...,Mk​(x))，则 M[k]\\mathcal{M}_{[k]}M[k]​ 是 (∑i=1kεi,∑i=1kδi)(\\sum_{i=1}^{k}\\varepsilon_i,\\sum_{i=1}^{k}\\delta_i)(∑i=1k​εi​,∑i=1k​δi​)- 差分隐私。 差异隐私的一个优点：其合成是“自动”的，因为获得的限制是保持不变，无需数据提供者对其做任何修改。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2020-01-02 22:06:52 "},"3-Basic-Techniques-and-Composition-Theorems/Composition-theorems/Composition-some-technicalities.html":{"url":"3-Basic-Techniques-and-Composition-Theorems/Composition-theorems/Composition-some-technicalities.html","title":"3.5.1 合成：一些技术细节","keywords":"","body":"3.5.1 合成：一些技术细节 在本节的剩余部分，我们将证明一个更复杂的合成定理。为此，我们需要一些定义和引理，用分布之间的距离度量重新表述差异隐私。在下面的分数量中，如果分母为零，那么我们将分数的值定义为无穷大（分子总是正的）。 定义3.5（KL散度） KL-散度（Kullback-Leibler divergence），又称为 相对熵。两个随机变量 YYY 和 ZZZ（取同一域的值）之间的 KL-散度定义为： D(Y∣∣Z)=Ey∽Y[ln⁡Pr[Y=y]Pr[Z=y]]=∑y∽YPr(y)ln⁡Pr[Y=y]Pr[Z=y]=∫−∞∞pY(y)ln⁡pY(y)pZ(y)dy \\begin{aligned} D(Y||Z) = \\mathbb{E}_{y \\backsim Y}\\Big[\\ln\\frac{\\text{Pr}[Y=y]}{\\text{Pr}[Z=y]}\\Big] &= \\sum_{y \\backsim Y}\\text{Pr}(y)\\ln\\frac{\\text{Pr}[Y=y]}{\\text{Pr}[Z=y]}\\\\ &=\\int_{-\\infty}^{\\infty}p_Y(y)\\ln\\frac{p_Y(y)}{p_Z(y)}dy \\end{aligned} D(Y∣∣Z)=Ey∽Y​[lnPr[Z=y]Pr[Y=y]​]​=y∽Y∑​Pr(y)lnPr[Z=y]Pr[Y=y]​=∫−∞∞​pY​(y)lnpZ​(y)pY​(y)​dy​ (译者注：增加离散和连续的两种相对熵等价定义) 已知 D(Y∣∣Z)≥0D(Y||Z)\\geq 0D(Y∣∣Z)≥0 ，且当且仅当 YYY 和 ZZZ 分布相同时具有相等性。然而，DDD 是非对称的，不满足三角不等式，甚至可以是无限的，特别是当 Supp(Y)\\text{Supp}(Y)Supp(Y) 不包含在 Supp(Z)\\text{Supp}(Z)Supp(Z) 中时。 （译者注：支撑集 Supp()\\text{Supp}()Supp()：其实就是函数的非零部分子集，比如 ReLU 函数的支撑集就是 (0,+∞)(0, +\\infty)(0,+∞)，一个概率分布的支撑集就是所有概率密度非零部分的集合。具体定义见 维基百科） 定义3.6（最大散度） 取自同一域的两个随机变量 YYY 和 ZZZ 之间的最大散度定义为： D∞(Y∣∣Z)=max⁡S⊆Supp(Y)[ln⁡Pr[Y∈S]Pr[Z∈S]] D_{\\infty}(Y||Z) = \\max_{S \\subseteq \\text{Supp}(Y)}\\Big[\\ln\\frac{\\text{Pr}[Y\\in S]}{\\text{Pr}[Z \\in S]}\\Big] D∞​(Y∣∣Z)=S⊆Supp(Y)max​[lnPr[Z∈S]Pr[Y∈S]​] YYY 和 ZZZ 之间的 δ\\deltaδ-近似最大散度定义为： D∞δ(Y∣∣Z)=max⁡S⊆Supp(Y):Pr[Y∈S]≥δ[ln⁡Pr[Y∈S]−δPr[Z∈S]] D_{\\infty}^{\\delta}(Y||Z) = \\max_{S \\subseteq \\text{Supp}(Y):\\text{Pr}[Y \\in S] \\geq \\delta}\\Big[\\ln\\frac{\\text{Pr}[Y\\in S]-\\delta}{\\text{Pr}[Z \\in S]}\\Big] D∞δ​(Y∣∣Z)=S⊆Supp(Y):Pr[Y∈S]≥δmax​[lnPr[Z∈S]Pr[Y∈S]−δ​] 备注3.1 请注意，机制 M\\mathcal{M}M 为： 1.当且仅当在每对相邻数据库 x,y,D∞(M(x)∣∣M(y))≤ε,D∞(M(y)∣∣M(x))≤εx,\\enspace y,\\enspace D_{\\infty}(\\mathcal{M}(x)||\\mathcal{M}(y)) \\leq \\varepsilon, \\enspace D_{\\infty}(\\mathcal{M}(y)||\\mathcal{M}(x)) \\leq \\varepsilonx,y,D∞​(M(x)∣∣M(y))≤ε,D∞​(M(y)∣∣M(x))≤ε时，机制为 ε\\varepsilonε -差分隐私。 2.当且仅当在每两个相邻的数据库 x,yD∞δ(M(x)∣∣M(y))≤εD∞δ(M(y)∣∣M(x))≤εx,\\enspace y \\enspace D_{\\infty}^{\\delta}(\\mathcal{M}(x)||\\mathcal{M}(y)) \\leq \\varepsilon \\enspace D_{\\infty}^{\\delta}(\\mathcal{M}(y)||\\mathcal{M}(x)) \\leq \\varepsilonx,yD∞δ​(M(x)∣∣M(y))≤εD∞δ​(M(y)∣∣M(x))≤ε 时，机制为 (ε,δ)(\\varepsilon,\\delta)(ε,δ) -差分隐私。 另一个有用的距离度量是两个随机变量 YYY 和 ZZZ 之间的统计距离，定义为: Δ(Y,Z)=defmax⁡S∣Pr[Y∈S]−Pr[Z∈S]∣ \\Delta(Y,Z) \\overset{\\text{def}}{=} \\max_{S}|\\text{Pr}[Y \\in S]-\\text{Pr}[Z \\in S]| Δ(Y,Z)=defSmax​∣Pr[Y∈S]−Pr[Z∈S]∣ 如果 Δ(Y,Z)≤δ\\Delta(Y,Z)\\leq \\deltaΔ(Y,Z)≤δ,我们说 Y,ZY,ZY,Z 是δ-接近（原文“δ-close”）的， 我们将根据确切的最大散度和统计距离重新制定最大散度公式： 引理 3.17 1.当且仅当存在一个随机变量 Y′Y'Y′ 满足 Δ(Y,Y′)≤δ,D∞(Y′∣∣Z)≤ε\\Delta(Y,Y')\\leq \\delta,\\enspace D_{\\infty}(Y'||Z)\\leq \\varepsilonΔ(Y,Y′)≤δ,D∞​(Y′∣∣Z)≤ε 时，D∞δ(Y∣∣Z)≤εD_{\\infty}^{\\delta}(Y||Z)\\leq \\varepsilonD∞δ​(Y∣∣Z)≤ε 成立 2.当且仅当存在随机变量 Y′,Z′Y',Z'Y′,Z′ 满足 Δ(Y,Y′)≤δ/(eε+1),Δ(Z,Z′)≤δ/(eε+1),D∞(Y′∣∣Z′)≤ε\\Delta(Y,Y')\\leq \\delta/(e^{\\varepsilon}+1),\\enspace\\Delta(Z,Z')\\leq \\delta/(e^{\\varepsilon}+1),\\enspace D_{\\infty}(Y'||Z')\\leq \\varepsilonΔ(Y,Y′)≤δ/(eε+1),Δ(Z,Z′)≤δ/(eε+1),D∞​(Y′∣∣Z′)≤ε 时，D∞δ(Y∣∣Z)≤ε,D∞δ(Z∣∣Y)≤εD_{\\infty}^{\\delta}(Y||Z)\\leq \\varepsilon,\\enspace D_{\\infty}^{\\delta}(Z||Y)\\leq \\varepsilonD∞δ​(Y∣∣Z)≤ε,D∞δ​(Z∣∣Y)≤ε 成立。 【证明】 略 引理 3.18 假设随机变量 Y,ZY,\\enspace ZY,Z 满足 D∞(Y∣∣Z)≤εD∞(Z∣∣Y)≤εD_{\\infty}(Y||Z)\\leq \\varepsilon \\enspace D_{\\infty}(Z||Y)\\leq \\varepsilonD∞​(Y∣∣Z)≤εD∞​(Z∣∣Y)≤ε，则 D(Y∣∣Z)≤ε⋅(eε−1)D(Y||Z)\\leq \\varepsilon\\cdot(e^{\\varepsilon}-1)D(Y∣∣Z)≤ε⋅(eε−1)。 【证明】 由KL散度性质可知：任意 Y,ZY,\\enspace ZY,Z 有 D(Y∣∣Z)≥0D(Y||Z)\\geq0D(Y∣∣Z)≥0，所以 D(Y∣∣Z)D(Y||Z)D(Y∣∣Z) 可以被 D(Y∣∣Z)+D(Z∣∣Y)D(Y||Z) + D(Z||Y)D(Y∣∣Z)+D(Z∣∣Y) 约束： D(Y∣∣Z)≤D(Y∣∣Z)+D(Z∣∣Y)=∑yPr[Y=y]⋅(ln⁡Pr[Y=y]Pr[Z=y]+ln⁡Pr[Z=y]Pr[Y=y])+(Pr[Z=y]−Pr[Y=y])⋅(ln⁡Pr[Z=y]Pr[Y=y])≤∑y[0+∣Pr[Z=y]−Pr[Y=y]∣⋅ε]=ε⋅∑y[max⁡{Pr[Y=y],Pr[Z=y]}−min⁡{Pr[Y=y],Pr[Z=y]}]≤ε⋅∑y[(eε−1)⋅min⁡{Pr[Y=y],Pr[Z=y]}]≤ε(eε−1) \\begin{aligned} D(Y||Z) &\\leq D(Y||Z) + D(Z||Y)\\\\ &= \\sum_y \\text{Pr}[Y=y]\\cdot\\Big(\\ln\\frac{\\text{Pr}[Y=y]}{\\text{Pr}[Z=y]}+\\ln\\frac{\\text{Pr}[Z=y]}{\\text{Pr}[Y=y]}\\Big)\\\\ &\\enspace \\enspace + (\\text{Pr}[Z=y]-\\text{Pr}[Y=y])\\cdot \\Big(\\ln\\frac{\\text{Pr}[Z=y]}{\\text{Pr}[Y=y]}\\Big)\\\\ &\\leq \\sum_y[0+|\\text{Pr}[Z=y]-\\text{Pr}[Y=y]|\\cdot\\varepsilon]\\\\ &= \\varepsilon\\cdot\\sum_y[\\max\\{\\text{Pr}[Y=y],\\text{Pr}[Z=y]\\}\\\\ &\\enspace \\enspace -\\min\\{\\text{Pr}[Y=y],\\text{Pr}[Z=y]\\}]\\\\ &\\leq \\varepsilon\\cdot\\sum_y[(e^\\varepsilon-1)\\cdot\\min\\{\\text{Pr}[Y=y],\\text{Pr}[Z=y]\\}]\\\\ &\\leq \\varepsilon(e^\\varepsilon-1) \\end{aligned} D(Y∣∣Z)​≤D(Y∣∣Z)+D(Z∣∣Y)=y∑​Pr[Y=y]⋅(lnPr[Z=y]Pr[Y=y]​+lnPr[Y=y]Pr[Z=y]​)+(Pr[Z=y]−Pr[Y=y])⋅(lnPr[Y=y]Pr[Z=y]​)≤y∑​[0+∣Pr[Z=y]−Pr[Y=y]∣⋅ε]=ε⋅y∑​[max{Pr[Y=y],Pr[Z=y]}−min{Pr[Y=y],Pr[Z=y]}]≤ε⋅y∑​[(eε−1)⋅min{Pr[Y=y],Pr[Z=y]}]≤ε(eε−1)​ 【引理 3.18 证毕】 【补充：该证明过程省略许多步骤，会造成迷惑，这边对证明过程加以补充。首先第一步的推导，由定义可得: D(Y∣∣Z)+D(Z∣∣Y)=∑yPr[Y=y]⋅ln⁡Pr[Y=y]Pr[Z=y]+∑yPr[Z=y]⋅ln⁡Pr[Z=y]Pr[Y=y]=∑yPr[Y=y]⋅ln⁡Pr[Y=y]Pr[Z=y]+∑y{Pr[Z=y]+Pr[Y=y]−Pr[Y=y]}⋅ln⁡Pr[Z=y]Pr[Y=y]=∑y[Pr[Y=y]⋅(ln⁡Pr[Y=y]Pr[Z=y]+ln⁡Pr[Z=y]Pr[Y=y])+(Pr[Z=y]−Pr[Y=y])⋅(ln⁡Pr[Z=y]Pr[Y=y])] \\begin{aligned} D(Y||Z) + D(Z||Y) &= \\sum_y \\text{Pr}[Y=y]\\cdot\\ln\\frac{\\text{Pr}[Y=y]}{\\text{Pr}[Z=y]}+\\sum_y \\text{Pr}[Z=y]\\cdot\\ln\\frac{\\text{Pr}[Z=y]}{\\text{Pr}[Y=y]}\\\\ &= \\sum_y \\text{Pr}[Y=y]\\cdot\\ln\\frac{\\text{Pr}[Y=y]}{\\text{Pr}[Z=y]}\\\\ &\\enspace \\enspace +\\sum_y \\Big\\{\\text{Pr}[Z=y]+\\text{Pr}[Y=y]-\\text{Pr}[Y=y]\\Big\\}\\cdot\\ln\\frac{\\text{Pr}[Z=y]}{\\text{Pr}[Y=y]}\\\\ &= \\sum_y \\Big[\\text{Pr}[Y=y]\\cdot\\Big(\\ln\\frac{\\text{Pr}[Y=y]}{\\text{Pr}[Z=y]}+\\ln\\frac{\\text{Pr}[Z=y]}{\\text{Pr}[Y=y]}\\Big)\\\\ &\\enspace \\enspace + (\\text{Pr}[Z=y]-\\text{Pr}[Y=y])\\cdot \\Big(\\ln\\frac{\\text{Pr}[Z=y]}{\\text{Pr}[Y=y]}\\Big)\\Big]\\\\ \\end{aligned} D(Y∣∣Z)+D(Z∣∣Y)​=y∑​Pr[Y=y]⋅lnPr[Z=y]Pr[Y=y]​+y∑​Pr[Z=y]⋅lnPr[Y=y]Pr[Z=y]​=y∑​Pr[Y=y]⋅lnPr[Z=y]Pr[Y=y]​+y∑​{Pr[Z=y]+Pr[Y=y]−Pr[Y=y]}⋅lnPr[Y=y]Pr[Z=y]​=y∑​[Pr[Y=y]⋅(lnPr[Z=y]Pr[Y=y]​+lnPr[Y=y]Pr[Z=y]​)+(Pr[Z=y]−Pr[Y=y])⋅(lnPr[Y=y]Pr[Z=y]​)]​ 由于 D∞(Z∣∣Y)≤εD_{\\infty}(Z||Y)\\leq \\varepsilonD∞​(Z∣∣Y)≤ε ，由 最大散度 的定义可知： ln⁡Pr[Z=y]Pr[Y=y]≤D∞(Z∣∣Y)=max⁡S⊆Supp(Y)[ln⁡Pr[Z=y]Pr[Y=y]]≤ε \\ln\\frac{\\text{Pr}[Z=y]}{\\text{Pr}[Y=y]} \\leq D_{\\infty}(Z||Y) = \\max_{S \\subseteq \\text{Supp}(Y)}\\Big[\\ln\\frac{\\text{Pr}[Z=y]}{\\text{Pr}[Y=y]}\\Big]\\leq \\varepsilon lnPr[Y=y]Pr[Z=y]​≤D∞​(Z∣∣Y)=S⊆Supp(Y)max​[lnPr[Y=y]Pr[Z=y]​]≤ε 且由两式相减必小于等于两式相减的绝对值，故： D(Y∣∣Z)≤∑y[0+∣Pr[Z=y]−Pr[Y=y]∣⋅ε] D(Y||Z) \\leq \\sum_y[0+|\\text{Pr}[Z=y]-\\text{Pr}[Y=y]|\\cdot\\varepsilon] D(Y∣∣Z)≤y∑​[0+∣Pr[Z=y]−Pr[Y=y]∣⋅ε] 又因绝对值公式 max⁡{f(x),g(x)}−min⁡{f(x),g(x)}=∣f(x)−g(x)∣\\max\\{f(x),g(x)\\}-\\min\\{f(x),g(x)\\}=|f(x)-g(x)|max{f(x),g(x)}−min{f(x),g(x)}=∣f(x)−g(x)∣，可以得到： ∑y[0+∣Pr[Z=y]−Pr[Y=y]∣⋅ε]=ε⋅∑y[max⁡{Pr[Y=y],Pr[Z=y]}−min⁡{Pr[Y=y],Pr[Z=y]}] \\begin{aligned} \\sum_y[0+|\\text{Pr}[Z=y]-\\text{Pr}[Y=y]|\\cdot\\varepsilon] &= \\varepsilon\\cdot\\sum_y[\\max\\{\\text{Pr}[Y=y],\\text{Pr}[Z=y]\\}\\\\ &\\enspace \\enspace -\\min\\{\\text{Pr}[Y=y],\\text{Pr}[Z=y]\\}] \\end{aligned} y∑​[0+∣Pr[Z=y]−Pr[Y=y]∣⋅ε]​=ε⋅y∑​[max{Pr[Y=y],Pr[Z=y]}−min{Pr[Y=y],Pr[Z=y]}]​ 当 Pr[Y=y]>Pr[Z=y]\\text{Pr}[Y=y]>\\text{Pr}[Z=y]Pr[Y=y]>Pr[Z=y] 时，max⁡{Pr[Y=y],Pr[Z=y]}=Pr[Y=y]\\max\\{\\text{Pr}[Y=y],\\text{Pr}[Z=y]\\} = \\text{Pr}[Y=y]max{Pr[Y=y],Pr[Z=y]}=Pr[Y=y]，由给定条件和 最大散度 定义可知： ln⁡Pr[Y=y]Pr[Z=y]≤D∞(Y∣∣Z)=max⁡S⊆Supp(Y)[ln⁡Pr[Y=y]Pr[Z=y]]≤ε⟹Pr[Y=y]≤eε⋅Pr[Z=y]⟹max⁡{Pr[Y=y],Pr[Z=y]}≤eε⋅Pr[Z=y] \\begin{aligned} \\ln\\frac{\\text{Pr}[Y=y]}{\\text{Pr}[Z=y]} &\\leq D_{\\infty}(Y||Z) = \\max_{S \\subseteq \\text{Supp}(Y)}\\Big[\\ln\\frac{\\text{Pr}[Y=y]}{\\text{Pr}[Z=y]}\\Big]\\leq \\varepsilon\\\\ &\\implies \\text{Pr}[Y=y] \\leq e^{\\varepsilon}\\cdot \\text{Pr}[Z=y]\\\\ &\\implies \\max\\{\\text{Pr}[Y=y],\\text{Pr}[Z=y]\\} \\leq e^{\\varepsilon}\\cdot \\text{Pr}[Z=y] \\end{aligned} lnPr[Z=y]Pr[Y=y]​​≤D∞​(Y∣∣Z)=S⊆Supp(Y)max​[lnPr[Z=y]Pr[Y=y]​]≤ε⟹Pr[Y=y]≤eε⋅Pr[Z=y]⟹max{Pr[Y=y],Pr[Z=y]}≤eε⋅Pr[Z=y]​ 反之：Pr[Y=y]Pr[Z=y]\\text{Pr}[Y=y]Pr[Y=y]Pr[Z=y] 亦然，max⁡{Pr[Y=y],Pr[Z=y]}≤eε⋅Pr[Y=y]\\max\\{\\text{Pr}[Y=y],\\text{Pr}[Z=y]\\} \\leq e^{\\varepsilon}\\cdot \\text{Pr}[Y=y]max{Pr[Y=y],Pr[Z=y]}≤eε⋅Pr[Y=y]，所以两者中的最大值必然小于等于两者最小值乘上 eεe^\\varepsiloneε，形式化表示为： max⁡{Pr[Y=y],Pr[Z=y]}≤eε⋅min⁡{Pr[Y=y],Pr[Z=y]}\\max\\{\\text{Pr}[Y=y],\\text{Pr}[Z=y]\\}\\leq e^\\varepsilon\\cdot\\min\\{\\text{Pr}[Y=y],\\text{Pr}[Z=y]\\}max{Pr[Y=y],Pr[Z=y]}≤eε⋅min{Pr[Y=y],Pr[Z=y]} 故有： ε⋅∑y[max⁡{Pr[Y=y],Pr[Z=y]}−min⁡{Pr[Y=y],Pr[Z=y]}]≤ε⋅∑y[(eε−1)⋅min⁡{Pr[Y=y],Pr[Z=y]}] \\begin{aligned} \\varepsilon\\cdot\\sum_y[\\max\\{\\text{Pr}[Y=y],\\text{Pr}[Z=y]\\} &-\\min\\{\\text{Pr}[Y=y],\\text{Pr}[Z=y]\\}]\\\\ &\\leq \\varepsilon\\cdot\\sum_y[(e^\\varepsilon-1)\\cdot\\min\\{\\text{Pr}[Y=y],\\text{Pr}[Z=y]\\}]\\\\ \\end{aligned} ε⋅y∑​[max{Pr[Y=y],Pr[Z=y]}​−min{Pr[Y=y],Pr[Z=y]}]≤ε⋅y∑​[(eε−1)⋅min{Pr[Y=y],Pr[Z=y]}]​ 】 引理3.19（Azuma不等式） 令 C1,...CkC_1,...C_kC1​,...Ck​ 为实值变量满足任意一个 i∈[k],Pr[∣Ci∣≤α]=1i\\in[k],\\text{Pr}[|C_i|\\leq \\alpha]=1i∈[k],Pr[∣Ci​∣≤α]=1，且对于每一个 (c1,...,ci−1)∈Supp(C1,...Ci−1)(c_1,...,c_{i-1})\\in \\text{Supp}(C_1,...C_{i-1})(c1​,...,ci−1​)∈Supp(C1​,...Ci−1​) 我们有： E[Ci∣C1=c1,...,Ci−1=ci−1]≤β \\mathbb{E}[C_i|C_1=c_1,...,C_{i-1}=c_{i-1}]\\leq\\beta E[Ci​∣C1​=c1​,...,Ci−1​=ci−1​]≤β 对于任一 z>0z > 0z>0，我们有： Pr[∑i=1kCi>kβ+zk⋅α]≤e−z2/2 \\text{Pr}\\Big[\\sum_{i=1}^kC_i>k\\beta + z\\sqrt{k}\\cdot\\alpha\\Big] \\leq e^{-z^2/2} Pr[i=1∑k​Ci​>kβ+zk​⋅α]≤e−z2/2 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-16 09:54:06 "},"3-Basic-Techniques-and-Composition-Theorems/Composition-theorems/Advanced-composition.html":{"url":"3-Basic-Techniques-and-Composition-Theorems/Composition-theorems/Advanced-composition.html","title":"3.5.2 高级合成技术","keywords":"","body":"3.5.2 高级合成技术 除了允许参数降级得更慢之外，我们希望我们的定理能够处理更复杂的合成形式。但是，在开始之前，我们必须讨论合成对我们的确切含义。我们希望我们的定义涵盖以下两个有趣的场景： 1.在同一数据库上重复使用差分隐私算法。这允许多次重复使用相同的机制，以及从任意私有模块中模块化构造差分私有算法。 2.在不同的数据库上重复使用差分隐私算法，这些数据库可能包含与同一个人的有关信息。这使我们能够推断出一个个体的隐私累积损失，其数据可能分布在多个数据集上，每个数据库可能使用不同的差分隐私算法独立运行。由于新数据库一直在创建，并且对手实际上可能会影响这些新数据库的合成，因此与重复查询单个固定数据库相比，这是一个根本不同的问题。 我们希望对合成进行建模，在这些合成中攻击者可以适应性地影响未来机制输入的数据库以及对这些未来机制的查询。令 F\\mathcal{F}F 为一系列数据库访问机制。（例如 F\\mathcal{F}F 可以是所有 ε−\\varepsilon-ε−差分隐私机制的集合）对于概率对手 AAA，我们考虑两个实验，实验0和实验1定义如下。 对机制集合 F\\mathcal{F}F 与对手 AAA 进行实验 b ： i=1,...,ki=1,...,ki=1,...,k AAA 输出两个相邻数据集 xi0,xi1x_i^0,x_i^1xi0​,xi1​，和机制 Mi∈F\\mathcal{M}_i \\in \\mathcal{F}Mi​∈F，且参数为 wiw_iwi​。 AAA 接收 yi∈RMi(wi,xi,b)y_i \\in_{R} \\mathcal{M}_i(w_i,x_{i,b})yi​∈R​Mi​(wi​,xi,b​) 我们允许上面的对手 AAA 在整个实验中都是有状态的，因此，它可以根据先前机制的输出自适应地选择数据库，机制和参数。我们将 AAA 对实验的观点定义为 {0，1}（AAA coin tosses）和所有机制输出 (y1,...,yk)(y_1,...,y_k)(y1​,...,yk​) 共同组成。（xij,Mi,wix_i^j,\\mathcal{M}_i,w_ixij​,Mi​,wi​ 都可以从这些看法中重建。） 出于直观表示，我们假设一个对手总是选择 xi0x_i^0xi0​ 保存 Bob 的数据，而 xi1x_i^1xi1​ 与 xi0x_i^0xi0​ 的区别只有 Bob 的数据在 xi1x_i^1xi1​ 中被删除。然后实验0 可以被认为是“真实世界”，Bob 允许他的数据在许多数据发布中使用；实验1 是“理想世界”，这些数据发布的结果不依赖于 Bob 的数据。就像差分隐私定义所要求的那样，我们对隐私的定义仍然要求这两个实验相互“接近”。对 Bob 的直观保证是，即使给了攻击者所有 kkk 个机制的输出，对手仍然“不知道” Bob 的数据是否被使用了。 定义 3.7 如果对每个对手 AAA，我们都有 D∞(V0∣∣V1)≤εD_{\\infty}(V^0||V^1)\\leq \\varepsilonD∞​(V0∣∣V1)≤ε，则数据库访问机制族 F\\mathcal{F}F 满足 kkk 倍自适应合成下的 ε\\varepsilonε-差分隐私(ε-differential privacy under k-fold adaptive composition)，其中 VbV^bVb 表示在上述 kkk 倍合成实验b 中 AAA 的观点。 在 kkk 倍自适应组合下的 (ε,δ)(\\varepsilon,\\delta)(ε,δ)-差分隐私要求D∞δ(V0∣∣V1)≤εD_{\\infty}^{\\delta}(V^0||V^1)\\leq \\varepsilonD∞δ​(V0∣∣V1)≤ε。 （个人理解：这里攻击者有很大的能力，可以构造查询，构造机制，构造参数等。合成差分隐私数据库需要防范的是，即使攻击者有如此多的攻击“能力”，也能保证攻击者无法辨别用户数据是否参与发布。 那么合成机制防范攻击的标准是什么？换句话说，合成机制最多能允许多少隐私损失？首先，作者在这里设置了两种情况（实验）代表攻击者对用户隐私的猜测，即：用户数据参与机制合成发布（实验0）、用户数据不参与合成发布（实验1）。其次，由于攻击者拥有很高的“权限”，所以可以获取合成数据库不同机制生成的不同结果（上文里的 (y1,...,yk)(y_1,...,y_k)(y1​,...,yk​) ）。所以攻击者通过不同的结果，来判断两种实验发生的概率（对两种实验的观点）。若攻击者认为，用户参与到合成机制的发布中，那么 V0V^0V0 的概率会远高于 V1V^1V1，反之亦然。只有两者概率相差大，才能作出判断，若两者概率接近，那么攻击者无法作出判断。这点可以与前几章提到的普通差分隐私概念做类比，将普通差分隐私概念扩展到了合成机制。 最后，作者使用 3.5.1节 定义3.6 (最大散度) 对合成机制下的差分隐私作出了定义，即：kkk 倍自适应合成下的 ε\\varepsilonε-差分隐私 ） 定理 3.20（高级合成）。 对于所有 ε,δ,δ′≥0\\varepsilon,\\delta,\\delta'\\geq 0ε,δ,δ′≥0，(ε,δ)(\\varepsilon,\\delta)(ε,δ)-差分隐私机制类满足 kkk 倍自适应合成下的 (ε′,kδ+δ′)(\\varepsilon',k\\delta+\\delta')(ε′,kδ+δ′)-差分隐私，其中： ε′=2kln⁡(1/δ′)⋅ε+kε(eε−1) \\varepsilon' = \\sqrt{2k\\ln(1/\\delta')}\\cdot\\varepsilon + k\\varepsilon(e^{\\varepsilon}-1) ε′=2kln(1/δ′)​⋅ε+kε(eε−1) 【证明】 略 如果我们希望针对给定的 ε′,δ′\\varepsilon',\\delta'ε′,δ′ 确保 (ε′,kδ+δ′)(\\varepsilon',k\\delta+\\delta')(ε′,kδ+δ′)-差分隐私，则一个直接有用的推论告诉我们一个 ε\\varepsilonε 的安全选择（ 用在 kkk 种机制中的每一个机制）。 推论 3.21 给定目标隐私参数 0ε′1,δ′>0000ε′1,δ′>0 ，为确保在 kkk 个机制上的累积隐私损失是 (ε′,kδ+δ′)(\\varepsilon',k\\delta+\\delta')(ε′,kδ+δ′)，只要每个机制是 (ε,δ)(\\varepsilon,\\delta)(ε,δ)-差分隐私即可，其中： ε=ε′22kln⁡(1/δ′) \\varepsilon = \\frac{\\varepsilon'}{2\\sqrt{2k\\ln(1/\\delta')}} ε=22kln(1/δ′)​ε′​ 【证明】 当 ε′1\\varepsilon'ε′1 时，我们期望 ε∗≤ε′\\varepsilon^*\\leq\\varepsilon'ε∗≤ε′。由定理 3.20 可知对于所有 δ′\\delta'δ′ 合成的隐私损失为 (ε∗,kδ+δ′)(\\varepsilon^*,k\\delta+\\delta')(ε∗,kδ+δ′) ，其中 ε∗=2kln⁡(1/δ′)⋅ε+kε2\\varepsilon^*=\\sqrt{2k\\ln(1/\\delta')}\\cdot\\varepsilon + k\\varepsilon^2ε∗=2kln(1/δ′)​⋅ε+kε2。 【推论 3.21 证毕】 （此处有疑问，是通过 (eε−1)∽ε⟹ε∗=2kln⁡(1/δ′)⋅ε+kε2(e^\\varepsilon-1) \\backsim \\varepsilon \\implies \\varepsilon^*=\\sqrt{2k\\ln(1/\\delta')}\\cdot\\varepsilon + k\\varepsilon^2(eε−1)∽ε⟹ε∗=2kln(1/δ′)​⋅ε+kε2 然后解一元二次方程得出公式？） 注意，上面的推论给出了如何在差分隐私合成中设置所需的隐私参数 ε\\varepsilonε 的粗略指导。当我们关心优化常数（在处理实际实现时会这样做）时，可以通过直接应用合成定理来更紧密地设置 ε\\varepsilonε。 例 3.7 假设，Bob是一个 k=10000 (ε0,0)k=10000 \\ (\\varepsilon_0,0)k=10000 (ε0​,0)-差异隐私数据库的成员。假设这些数据库之间没有协调（任何给定数据库的管理员可能甚至不知道其他数据库的存在）。那么 ε0\\varepsilon_0ε0​ 的值应该是什么才能在他数据的生命周期里，Bob 的累积隐私损失以 ε=1\\varepsilon=1ε=1 为界，且概率至少为1−e−321-e^{-32}1−e−32呢 ?。定理3.20 指出，假设不同的私有数据库之间没有协调，取 δ′=e−32\\delta'=e^{-32}δ′=e−32，就足以使 ε0≤1/801ε_0\\leq1/801ε0​≤1/801。这在本质上是针对任意对手的最佳选择。 那么，我们可以用非平凡（注：此处个人理解为高精度）精确度回答多少个查询呢？在一个大小为 nnn 的数据库中，如果添加的噪声为 o(n)o(n)o(n) 阶，我们可以说精度是非平凡的。定理3.20指出，对于 ε,δ\\varepsilon,\\deltaε,δ 固定值，可以以非平凡精确度回答接近 n2n^2n2 的计数查询。类似地，当回答接近 nnn 个查询时，仍有噪声 o(n)o(\\sqrt{n})o(n​) （即噪声小于采样误差）。通过数据库间协调添加到单个响应中的噪声，我们将看到有可能显著改善精确度。在某些情况下，即使是噪声仅略大于 n\\sqrt{n}n​ 的指数数量的查询也可以得到处理。事实证明，这种协调是必要的：没有协调，高级合成定理中的上下界几乎是相近的（tight bound）。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-16 09:52:18 "},"3-Basic-Techniques-and-Composition-Theorems/Composition-theorems/Laplace-versus-Gauss.html":{"url":"3-Basic-Techniques-and-Composition-Theorems/Composition-theorems/Laplace-versus-Gauss.html","title":"3.5.3 拉普拉斯vs高斯","keywords":"","body":"3.5.3 拉普拉斯 VS 高斯 代替添加拉普拉斯噪声的另一种方法是添加高斯噪声。在这种情况下，我们不是将噪声缩放到 ℓ1\\ell_1ℓ1​ 灵敏度 Δf\\Delta fΔf，而是缩放到 ℓ2\\ell_2ℓ2​ 灵敏度： 定义3.8（ℓ2\\ell_2ℓ2​-敏感度） 一个方法 f:N∣X∣→Rkf:\\mathbb{N}^{|\\mathcal{X}|}\\to\\mathbb{R}^kf:N∣X∣→Rk 的 ℓ2\\ell_2ℓ2​-敏感度为： Δ2(f)=max⁡x,y∈N∣X∣,∥x−y∥1=1∥f(x)−f(y)∥2 \\Delta_2(f)=\\max_{x,y\\in\\mathbb{N}^{|\\mathcal{X}|},\\Vert x-y\\Vert _1=1}\\Vert f(x)-f(y)\\Vert _2 Δ2​(f)=x,y∈N∣X∣,∥x−y∥1​=1max​∥f(x)−f(y)∥2​ 参数为 bbb 的高斯机制在每个 kkk 协调中添加方差为 bbb 的零均值高斯噪声。以下定理在附录A中得到了证明。 定理 3.22。设 ε∈(0,1)\\varepsilon\\in(0,1)ε∈(0,1) 是任意的。当 c2>2ln⁡(1.25/δ)c^2>2\\ln(1.25/\\delta)c2>2ln(1.25/δ) 时，参数 σ≥cΔ2(f)/ε\\sigma\\geq c\\Delta_2(f)/\\varepsilonσ≥cΔ2​(f)/ε 的高斯机制是 (ε,δ)(\\varepsilon,\\delta)(ε,δ)-差分隐私的。 高斯噪声的优点之一是为隐私而添加的噪声与其他噪声源具有相同的类型；另外，两个高斯的和是高斯的，因此隐私机制对统计分析的影响可能更容易理解和修正。 这两种机制在组合下产生相同的累积损失，因此即使对于每个单独合成来说,隐私保证较弱，但在许多计算中的累积影响是可比较的。此外，如果 δ\\deltaδ 足够小（例如，亚多项式），在实践中，我们将永远不会遇到差分隐私保证的不足之处。 也就是说，相对于拉普拉斯噪声，高斯噪声在理论上是有缺点的。考虑 Report Noisy Max（带有拉普拉斯噪声）算法下，每个候选输出在数据库 xxx 上的效用得分与其在相邻数据集 yyy 上的效用分数相同。该机制产生 (ε,0)(\\varepsilon,0)(ε,0)-差分隐私，与候选输出的数量无关。如果我们使用高斯噪声并报告最大值，并且如果候选值的数量比 1/δ1/\\delta1/δ 大，那么我们将精确地选择发生概率小于 δδδ 的具有大高斯噪声的事件。当我们远离高斯分布的尾时，我们不再能保证在 x,yx,yx,y 数据库的观测概率的差别在 e±εe^{\\pm\\varepsilon}e±ε 因子内。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-17 15:21:18 "},"3-Basic-Techniques-and-Composition-Theorems/Composition-theorems/Remarks-on-composition.html":{"url":"3-Basic-Techniques-and-Composition-Theorems/Composition-theorems/Remarks-on-composition.html","title":"3.5.4 合成定理注释","keywords":"","body":"3.5.4 合成定理注释 分析差分隐私算法合成下累积的隐私丢失的能力使我们能够掌握差分隐私数据库可以提供什么功能。 一些发现是有序的。 弱量化 假设对手始终选择 xi0x_i^0xi0​ 保留 Bob 的数据，并且选择 xi1x_i^1xi1​ 相同的数据库（但是不包含Bob的数据）。带有适当的参数选择的 定理3.20，告诉我们：对手（这个对手的能力包括了知道数据库对，甚至能选择数据库对）在确定 b∈0,1b\\in{0,1}b∈0,1 的值时几乎没有优势。这是天生的弱量化。我们可以确保对手不太可能将现实与任何给定的替代方案区分开，但是我们不能为所有替代方案同时确保这一点。如果有一个数不胜数的数据库，但 Bob 仅是其中10,000个的成员，那么我们不会同时保护 Bob 在剩余数据库的缺失。 这类似于 (ε,δ)(\\varepsilon,\\delta)(ε,δ)-差分隐私的定义中的量化，在该定义中，我们预先确定了一对相邻的数据库，并认为很有可能这两个数据库的输出几乎相等。 人类和幽灵 直观地说，一个(ε,0)(\\varepsilon,0)(ε,0)-差分隐私数据库（库中每条记录只有少量的位），比另一个相同 ε\\varepsilonε 值的差分隐私数据库（这个库包含数据量大，甚至包含我们的整个病史）的保护性差。我们的隐私预算 ε\\varepsilonε 告诉我们关于数据库的同一件事：它们在存储数据的复杂性和敏感性方面有根本区别，但这有什么意义上呢？答案在于合成定理。想象一个由两种生物组成的世界：鬼魂和人类。两种类型的生物行为相同，以相同的方式与他人互动，写作、学习、工作、笑、爱、哭、繁殖、生病、康复和衰老都以相同的方式。唯一的区别是，幽灵在数据库中没有记录，而人类有。隐私攻击者的目标是确定给定的50岁“目标”是幽灵还是人类。的确，给了对手50年来做这件事情。攻击者不需要保持被动，例如，她可以组织临床试验并招募自己选择的患者，可以创建人员来填充数据库，有效地创建最坏情况（针对隐私）的数据库，她可以在25岁时将目标暴露于化学品中，在35岁时再次暴露于化学品中等等操作。她可以知道有关目标的所有信息，可以将其输入任何数据库。如果目标是人类，她就能知道目标会在哪个数据库中。合成定理告诉我们，每个数据库的隐私保证-无论数据类型，复杂性和敏感性如何-都对人类/幽灵比特提供了可比的保护。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-05 21:25:48 "},"3-Basic-Techniques-and-Composition-Theorems/The-sparse-vector-technique/The-sparse-vector-technique-Overview.html":{"url":"3-Basic-Techniques-and-Composition-Theorems/The-sparse-vector-technique/The-sparse-vector-technique-Overview.html","title":"稀疏向量技术","keywords":"","body":"3.6 稀疏向量技术 拉普拉斯机制可用于回答自适应选择的低敏感度查询，并且从我们的合成定理中我们知道，隐私参数与所回答的查询数量（或其平方根）成比例地降低。不幸的是，经常会发生我们有大量问题要回答的问题，即使使用 3.5节 中的高级合成定理，也有太多问题无法使用独立的扰动技术来提供合理的隐私保证。但是，在某些情况下，我们只会关心知道高于某个阈值的查询的标识。在这种情况下，我们希望通过放弃对明显低于阈值的查询的数字答案，而仅报告这些查询确实低于阈值，从而获得本质的分析。（如果我们这样选择的话，我们也将能够获得阈值以上查询的数字值，而只需花费额外的费用）。这类似于我们在3.3节中的“Report Noisy Max”机制中所做的事情，实际上，对于非交互式或脱机情况，可以选择迭代该算法或指数机制。 在本节中，我们显示如何在在线设置中分析此方法。该技术很简单：添加噪音并仅报告噪声值是否超过阈值。本节中，我们的重点是分析隐私只会随着实际高于阈值的查询数量而降低，而不会随着查询总数的增加而降低。如果我们知道位于阈值以上的查询集比查询总数小得多（也就是说，如果答案向量稀疏的话），那么将可以大量节省（隐私参数）。 更详细地讲，我们将考虑一系列事件（每个查询一个），如果在数据库上评估的查询超过给定（已知的、公共的）阈值，则会发生这些事件。我们的目标是释放一个位向量，以指示每个事件是否已发生。在提出每个查询时，该机制将计算一个噪声响应，并将其与（众所周知的）阈值进行比较，如果超过了该阈值，则将揭示此事实。由于隐私证明（定理3.24）中的技术原因，该算法适用于阈值 TTT 的噪声版本 T^\\hat{T}T^。虽然 TTT 是公开的，但噪声版本 T^\\hat{T}T^ 不是。 并非对每个可能的查询都造成隐私损失，后文的分析将仅针对接近或高于阈值的查询值导致隐私损失。 设置 设 mmm 表示灵敏度为 1 的查询总数，可以自适应地选择。在不丧失通用性的情况下，有一个预先固定的阈值 TTT（或者每个查询可以有自己的阈值，但结果不变）。我们将在查询值中添加噪声，并将结果与 TTT 进行比较。正向的结果意味着噪声查询值超过了阈值。我们期望 ccc （少量）个噪声值超过阈值，并且我们只释放高于阈值的噪声值。算法将 ccc 用作其停止条件。 我们将首先分析在超过阈值查询的 c=1c=1c=1 之后算法停止的情况，并表明无论查询的总序列有多长，该算法都是 ε\\varepsilonε-差分隐私的。然后利用我们的合成定理分析 c>1c>1c>1 的情形，并推导出 (ε,0)(\\varepsilon,0)(ε,0) 和(ε,δ)(\\varepsilon,\\delta)(ε,δ)-差分隐私的界。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-10 13:48:51 "},"3-Basic-Techniques-and-Composition-Theorems/The-sparse-vector-technique/AboveThreshold.html":{"url":"3-Basic-Techniques-and-Composition-Theorems/The-sparse-vector-technique/AboveThreshold.html","title":"3.6.1 高于阈值算法","keywords":"","body":"3.6 稀疏向量算法：高于阈值算法 我们首先论证了 AboveThreshold 算法是私有的，并且是准确的，该算法专门针对一个高于阈值的查询。 （注：上面算法中 ⊥\\bot⊥ 为永假含义; ⊤\\top⊤ 为永真含义。根据上章节描述，个人理解其含义应为：⊤\\top⊤ 释放回答，⊥\\bot⊥ 拒绝回答） 定理 3.23 AboveThreshold 算法是 (ε,0)(\\varepsilon,0)(ε,0)- 差分隐私的。 【证明】 固定任意两个相邻数据库 DDD 和 D′D'D′。设 AAA 为表示 AboveThreshold算法 (D,fi,T,ε)(D,{f_i},T,\\varepsilon)(D,fi​,T,ε) 输出的随机变量，设 A′A'A′ 为表示 AboveThreshold算法 (D′,fi,T,ε)(D',{f_i},T,\\varepsilon)(D′,fi​,T,ε) 输出的随机变量。算法的输出是这些随机变量的一些实现，即：a∈{⊥,⊤}ka \\in \\{\\bot,\\top\\}^ka∈{⊥,⊤}k，其形式是对于所有的 ik,ai=⊥,ak=⊤iik,ai​=⊥,ak​=⊤ 。算法内部有两种类型的随机变量：噪声阈值 T^\\hat{T}T^ 和对 kkk 个查询的扰动 {νi}i=1k\\{\\nu_i\\}_{i=1}^k{νi​}i=1k​。在下面的分析中，我们将固定（任意的）ν1,...,νk−1\\nu_1,...,\\nu_{k-1}ν1​,...,νk−1​ 的值。并且 νk\\nu_kνk​ 和 T^\\hat{T}T^ 具有随机性。定义以下量，该量代表在 DDD 上估计任何查询 f1,...,fk−1f_1,...,f_{k-1}f1​,...,fk−1​ 的最大噪声值： g(D)=max⁡ik(fi(D)+νi) g(D) = \\max_{ig(D)=ikmax​(fi​(D)+νi​) 在下文中，我们将滥用表示法，将 Pr[T^=t]\\text{Pr}[\\hat{T}=t]Pr[T^=t] 写为 T^\\hat{T}T^ 在 ttt 处的概率密度函数的简写（νk\\nu_kνk​ 也类似这样的表示），并写 1[x]\\mathbf{1}[x]1[x] 表示事件 xxx 的指示函数 1>\\ ^{} 1>。注意固定 νi,...,νk−1\\nu_i,...,\\nu_{k-1}νi​,...,νk−1​ 的值（这使 g(D)g(D)g(D) 为确定量），我们有： PrT^,νk[A=a]=PrT^,νk[T^>g(D) and fk(D)+νk>T^]=PrT^,νk[T^∈(g(D),fk(D)+νk]]=∫−∞∞∫−∞∞Pr[νk=v]  ⋅Pr[T^=t]1[t∈(g(D),fk(D)+v]]dvdt=∗ \\begin{aligned} \\underset{\\hat{T},\\nu_k}{\\text{Pr}}[A=a] &= \\underset{\\hat{T}, \\nu_k}{\\text{Pr}}[\\hat{T} > g(D) \\ \\text{and} \\ f_k(D)+ \\nu_k > \\hat{T}]\\\\ &= \\underset{\\hat{T}, \\nu_k}{\\text{Pr}}[\\hat{T} \\in (g(D),f_k(D)+ \\nu_k]]\\\\ &= \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\text{Pr}[ \\nu_k=v]\\\\ &\\ \\enspace \\ \\cdot \\text{Pr}[\\hat{T}=t]\\mathbf{1}[t\\in (g(D),f_k(D)+v]]dvdt\\\\ &= * \\end{aligned} T^,νk​Pr​[A=a]​=T^,νk​Pr​[T^>g(D) and fk​(D)+νk​>T^]=T^,νk​Pr​[T^∈(g(D),fk​(D)+νk​]]=∫−∞∞​∫−∞∞​Pr[νk​=v]  ⋅Pr[T^=t]1[t∈(g(D),fk​(D)+v]]dvdt=∗​ 我们现在对变量做一些变换，定义： v^=v+g(D)−g(D′)+fk(D)−fk(D′)t^=t+g(D)−g(D′) \\begin{aligned} \\hat{v} &= v+g(D)-g(D')+f_k(D)-f_k(D')\\\\ \\hat{t} &= t + g(D) - g(D') \\end{aligned} v^t^​=v+g(D)−g(D′)+fk​(D)−fk​(D′)=t+g(D)−g(D′)​ 注意，对于任何 D,D′D,D'D,D′，有 ∣v^−v∣≤2,∣t^−t∣≤1|\\hat{v}-v|\\leq 2,|\\hat{t}-t|\\leq 1∣v^−v∣≤2,∣t^−t∣≤1 。这是因为每个查询 fi(D)f_i(D)fi​(D) 的敏感度都是 111 的，因此量 g(D)g(D)g(D) 的敏感度也是 111 。应用变量的这种变化，我们有： ∗=∫−∞∞∫−∞∞Pr[νk=v^]⋅Pr[T^=t^]1[(t+g(D)−g(D′)) ∈(g(D),fk(D′)+v+g(D)−g(D′]]dvdt=∫−∞∞∫−∞∞Pr[νk=v^]⋅Pr[T^=t^]1[t∈(g(D′),fk(D′)+v]]dvdt≤∫−∞∞∫−∞∞exp⁡(ε/2)Pr[νk=v]⋅exp⁡(ε/2)Pr[T^=t]1[t∈(g(D′),fk(D′)+v]]dvdt=exp⁡(ε)PrT^,νk[T^>g(D′) and fk(D′)+νk>T^]=exp⁡(ε)PrT^,νk[A′=a] \\begin{aligned} * &= \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\text{Pr}[\\nu_k=\\hat{v}]\\cdot\\text{Pr}[\\hat{T}=\\hat{t}]\\mathbf{1}[(t+g(D)-g(D'))\\\\ &\\ \\qquad \\qquad \\enspace \\in(g(D),f_k(D')+v+g(D)-g(D']]dvdt\\\\ &= \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\text{Pr}[\\nu_k=\\hat{v}]\\cdot\\text{Pr}[\\hat{T}=\\hat{t}]\\mathbf{1}[t\\in(g(D'),f_k(D')+v]]dvdt\\\\ & \\leq \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\exp(\\varepsilon/2)\\text{Pr}[\\nu_k=v]\\\\ &\\enspace \\enspace \\cdot \\exp(\\varepsilon/2)\\text{Pr}[\\hat{T}=t]\\mathbf{1}[t\\in(g(D'),f_k(D')+v]]dvdt\\\\ &= \\exp(\\varepsilon)\\underset{\\hat{T},\\nu_k}{\\text{Pr}}[\\hat{T} > g(D') \\ \\text{and} \\ f_k(D')+ \\nu_k > \\hat{T}]\\\\ &= \\exp(\\varepsilon)\\underset{\\hat{T},\\nu_k}{\\text{Pr}}[A'=a] \\end{aligned} ∗​=∫−∞∞​∫−∞∞​Pr[νk​=v^]⋅Pr[T^=t^]1[(t+g(D)−g(D′)) ∈(g(D),fk​(D′)+v+g(D)−g(D′]]dvdt=∫−∞∞​∫−∞∞​Pr[νk​=v^]⋅Pr[T^=t^]1[t∈(g(D′),fk​(D′)+v]]dvdt≤∫−∞∞​∫−∞∞​exp(ε/2)Pr[νk​=v]⋅exp(ε/2)Pr[T^=t]1[t∈(g(D′),fk​(D′)+v]]dvdt=exp(ε)T^,νk​Pr​[T^>g(D′) and fk​(D′)+νk​>T^]=exp(ε)T^,νk​Pr​[A′=a]​ 不等式来自 ∣v^−v∣|\\hat{v}-v|∣v^−v∣ 和 ∣t^−t∣|\\hat{t}-t|∣t^−t∣的界，以及 Laplace 分布的概率密度函数。 【定理 3.23 证毕】 【补充：对上述证明过程中的不等式步骤拓展解释。由 Laplace 分布概率密度函数（ vvv 的尺度参数为 4/ε4/\\varepsilon4/ε）可知： Pr[νk=v^]=12⋅4εexp⁡(−∣v^∣4/ε)Pr[νk=v]=12⋅4εexp⁡(−∣v∣4/ε) \\begin{aligned} \\text{Pr}[\\nu_k = \\hat{v}] &= \\frac{1}{2\\cdot\\frac{4}{\\varepsilon}}\\exp\\big(-\\frac{|\\hat{v}|}{4/\\varepsilon}\\big)\\\\ \\text{Pr}[\\nu_k = v] &= \\frac{1}{2\\cdot\\frac{4}{\\varepsilon}}\\exp\\big(-\\frac{|v|}{4/\\varepsilon}\\big)\\\\ \\end{aligned} Pr[νk​=v^]Pr[νk​=v]​=2⋅ε4​1​exp(−4/ε∣v^∣​)=2⋅ε4​1​exp(−4/ε∣v∣​)​ 由于 ∣v^−v∣≤2|\\hat{v}-v|\\leq 2∣v^−v∣≤2，并且由绝对值不等式，可以作出如下推导： Pr[νk=v^]Pr[νk=v]=exp⁡(∣v∣−∣v^∣4ε)≤exp⁡(∣v−v^∣4ε)≤exp⁡(24ε)=exp⁡(ε2)⟹Pr[νk=v^]≤exp⁡(ε2)⋅Pr[νk=v] \\begin{aligned} \\frac{\\text{Pr}[\\nu_k = \\hat{v}]}{\\text{Pr}[\\nu_k = v]} &= \\exp\\bigg(\\frac{|v|-|\\hat{v}|}{\\frac{4}{\\varepsilon}}\\bigg)\\\\ &\\leq \\exp\\bigg(\\frac{|v-\\hat{v}|}{\\frac{4}{\\varepsilon}}\\bigg)\\\\ &\\leq \\exp\\bigg(\\frac{2}{\\frac{4}{\\varepsilon}}\\bigg)\\\\ &= \\exp\\big(\\frac{\\varepsilon}{2}\\big)\\\\ \\implies \\text{Pr}[\\nu_k = \\hat{v}] &\\leq \\exp\\big(\\frac{\\varepsilon}{2}\\big)\\cdot \\text{Pr}[\\nu_k = v] \\end{aligned} Pr[νk​=v]Pr[νk​=v^]​⟹Pr[νk​=v^]​=exp(ε4​∣v∣−∣v^∣​)≤exp(ε4​∣v−v^∣​)≤exp(ε4​2​)=exp(2ε​)≤exp(2ε​)⋅Pr[νk​=v]​ 同样的方法应用于 T^\\hat{T}T^ 上，其 Laplace 分布的尺度参数为 2/ε2/\\varepsilon2/ε，且 ∣t^−t∣≤1|\\hat{t}-t|\\leq 1∣t^−t∣≤1 】 （译者注 指示函数：是定义在某集合 XXX 上的函数，表示其中有哪些元素属于某一子集 AAA。集合 XXX 的子集 AAA 的指示函数是函数 1A:X→{0,1}\\mathbf{1}_{A}:X\\to \\lbrace 0,1\\rbrace1A​:X→{0,1}，定义为： 1A(x)={1ifx∈A,0ifx∉A. \\mathbf{1} _{A}(x)= \\begin{cases} 1 &\\text{if}\\enspace x \\in A,\\\\ 0 &\\text{if}\\enspace x \\notin A. \\end{cases} 1A​(x)={10​ifx∈A,ifx∉A.​ 详见：指示函数定义 ） 定义3.9（准确度） 一个算法它的应答流 a1,...,∈{⊤,⊥}∗a_1,...,\\in \\{\\top,\\bot\\}^{*}a1​,...,∈{⊤,⊥}∗ 作为对 kkk 个查询流 f1,...,fkf_1,...,f_kf1​,...,fk​ 的响应。如果除了概率最大为 β\\betaβ 之外，这个算法在 fkf_kfk​ 之前不停止，并且对于所有 ai=⊤a_i = \\topai​=⊤ 有： fi(D)≥T−α f_i(D) \\geq T - \\alpha fi​(D)≥T−α 对于所有 ai=⊥a_i = \\botai​=⊥ 有： fi(D)≤T+α f_i(D) \\leq T + \\alpha fi​(D)≤T+α 那么，我们称这个算法对于阈值 TTT 是 (α,β)(\\alpha,\\beta)(α,β) -准确的。 算法1 可能出什么问题？噪声阈值 T^\\hat{T}T^ 可能离 TTT 很远，例如 ∣T^−T∣≥α|\\hat{T}-T|\\geq \\alpha∣T^−T∣≥α。 另外，小的 fi(D)T−αf_i(D)fi​(D)T−α 可能会添加大量噪声，以至于报告为高于阈值（即使阈值接近正确），而大 fi(D)>T+αf_i(D)>T+\\alphafi​(D)>T+α 可能报告为低于阈值。所有这些都以 α\\alphaα 的指数形式发生，概率很小。总而言之，我们在选择噪声阈值时可能会遇到问题，或者在一个或多个单独的噪声值 νiν_iνi​ 中可能会遇到这种问题。当然，我们可能同时存在两种错误。因此在下面的分析中，我们为每种类型分配 α/2\\alpha/2α/2。 （个人理解：AboveThreshold 中需要向阈值 TTT 和扰动 νk\\nu_kνk​ 添加 Laplace 噪声。根据 Laplace 分布的特点（下图）： 可以看出，算法会以小概率对阈值和扰动添加过大的噪声。如图的左右两侧。这就会造成上文提到的 “噪声阈值 T^\\hat{T}T^ 可能离 TTT 很远，例如 ∣T^−T∣≥α|\\hat{T}-T|\\geq \\alpha∣T^−T∣≥α”。同样，对扰动的噪声也可能过大。这样就导致，即使 T^\\hat{T}T^ 与 TTT 接近的情况下，造成小值回答（不允许释放）超过阈值被释放；大值回答（允许释放）小于阈值被拒绝。由于 AboveThreshold 会出现这两种错误，进而不满足 定义3.9 的规定。所以对于这两种错误情况，下面定理为噪声阈值 T^\\hat{T}T^ 和 扰动 νk\\nu_kνk​ 各分配 α/2\\alpha/2α/2 的界。并将概率上界 β\\betaβ 和噪声取之范围 α\\alphaα 关联起来，使得 AboveThreshold 算法不会出现两种错误情况，进而满足 定义3.9 的规定。 ） 定理 3.24 对于 kkk 个查询的任何序列，f1,...,fkf_1,...,f_kf1​,...,fk​ 使得 ∣{ik:fi(D)≥T−α}∣=0|\\{i∣{ik:fi​(D)≥T−α}∣=0（即，唯一接近阈值以上的查询是最后一个），当： α=8(log⁡k+log⁡(2/β))ε \\alpha = \\frac{8(\\log k+\\log(2/\\beta))}{\\varepsilon} α=ε8(logk+log(2/β))​ AboveThreshold 算法 (D,fi,T,ε)(D,{f_i},T,\\varepsilon)(D,fi​,T,ε) 是 (α,β)(\\alpha,\\beta)(α,β)-准确的： 【证明】 如果我们能够证明除概率最大为 β\\betaβ 以外，当: max⁡i∈[k]∣νi∣+∣T−T^∣≤α(∗) \\max_{i \\in [k]}|\\nu_i|+|T-\\hat{T}|\\leq\\alpha \\qquad (*) i∈[k]max​∣νi​∣+∣T−T^∣≤α(∗) 时，由观察易得该定理。 如果是这样的情况，那么对于任意 ai=⊤a_i=\\topai​=⊤，有： fi(D)+νi≥T^≥T−∣T−T^∣(1) f_i(D) + \\nu_i \\geq \\hat{T} \\geq T-|T-\\hat{T}| \\qquad (1) fi​(D)+νi​≥T^≥T−∣T−T^∣(1) 进一步推导： fi(D)≥T−∣T−T^∣−∣νi∣≥T−α(2) f_i(D) \\geq T-|T-\\hat{T}|-|\\nu_i|\\geq T-\\alpha \\qquad (2) fi​(D)≥T−∣T−T^∣−∣νi​∣≥T−α(2) 同样的，对于任意 ai=⊥a_i = \\botai​=⊥，有： fi(D)≤T^≤T+∣T−T^∣+∣νi∣≤T+α f_i(D) \\leq \\hat{T} \\leq T+|T-\\hat{T}|+|\\nu_i|\\leq T+\\alpha fi​(D)≤T^≤T+∣T−T^∣+∣νi​∣≤T+α 我们将会有对于任意 ik:fi(D)T−αT−∣νi∣−∣T−T^∣iik:fi​(D)T−αT−∣νi​∣−∣T−T^∣。所以： fi(D)+νi≤T^f_i(D)+\\nu_i\\leq \\hat{T}fi​(D)+νi​≤T^，即：ai=⊥a_i=\\botai​=⊥。因此，算法在第 k 个查询被回答前不会停止。 我们现在完成证明。回忆一下 事实3.7，当 Y∽Lap(b)Y\\backsim Lap(b)Y∽Lap(b) 时，Pr[∣Y∣≥t⋅b]=exp⁡(−t)\\text{Pr}[|Y|\\geq t\\cdot b]=\\exp(-t)Pr[∣Y∣≥t⋅b]=exp(−t)，算法中 T^\\hat{T}T^ 的尺度参数 b=2/εb=2/\\varepsilonb=2/ε 因此我们有： Pr[∣T−T^∣≥α2]=exp⁡(−εα4) \\text{Pr}[|T-\\hat{T}|\\geq \\frac{\\alpha}{2}]=\\exp\\Big(-\\frac{\\varepsilon \\alpha}{4}\\Big) Pr[∣T−T^∣≥2α​]=exp(−4εα​) 由定理设定最大概率为 β/2\\beta/2β/2，我们可以得知：α≥4log⁡(2/β)ε\\alpha\\geq \\frac{4\\log(2/\\beta)}{\\varepsilon}α≥ε4log(2/β)​ 。 同样，由 布尔不等式，且算法中 νk\\nu_kνk​ 的尺度参数 b=4/εb=4/\\varepsilonb=4/ε可知： Pr[max⁡i∈[k]∣νi∣≥α/2]≤k⋅exp⁡(−εα8) \\text{Pr}[\\max_{i\\in [k]}|\\nu_i|\\geq \\alpha/2]\\leq k\\cdot\\exp\\Big(-\\frac{\\varepsilon \\alpha}{8}\\Big) Pr[i∈[k]max​∣νi​∣≥α/2]≤k⋅exp(−8εα​) 由定理设定最大概率为 β/2\\beta/2β/2，我们可以得知：α≥4log⁡(2/β)+log⁡kε\\alpha\\geq \\frac{4\\log(2/\\beta)+\\log k}{\\varepsilon}α≥ε4log(2/β)+logk​ 。 这两个推导共同证明了该定理。 【定理 3.24 证毕】 【补充(1)式：在 AboveThreshold 算法中，当 ai=⊤,fi(D)+νi≥T^a_i=\\top,f_i(D)+\\nu_i\\geq \\hat{T}ai​=⊤,fi​(D)+νi​≥T^，∣T−T^∣|T-\\hat{T}|∣T−T^∣ 为 Laplace 噪声，故阈值必然大于等于其下界 T−∣T−T^∣T-|T-\\hat{T}|T−∣T−T^∣ 】 【补充(2)式：由 (∗)(*)(∗) 可以推得： max⁡i∈[k]∣νi∣+∣T−T^∣≤α⟹∣νi∣+∣T−T^∣≤max⁡i∈[k]∣νi∣+∣T−T^∣≤α⟹−∣νi∣−∣T−T^∣≥−α⟹T−∣νi∣−∣T−T^∣≥T−α \\begin{aligned} \\max_{i \\in [k]}|\\nu_i|+|T-\\hat{T}|&\\leq\\alpha\\\\ \\implies |\\nu_i|+|T-\\hat{T}| &\\leq \\max_{i \\in [k]}|\\nu_i|+|T-\\hat{T}| \\leq \\alpha\\\\ \\implies -|\\nu_i|-|T-\\hat{T}| &\\geq -\\alpha\\\\ \\implies T-|\\nu_i|-|T-\\hat{T}| &\\geq T-\\alpha \\end{aligned} i∈[k]max​∣νi​∣+∣T−T^∣⟹∣νi​∣+∣T−T^∣⟹−∣νi​∣−∣T−T^∣⟹T−∣νi​∣−∣T−T^∣​≤α≤i∈[k]max​∣νi​∣+∣T−T^∣≤α≥−α≥T−α​ 】 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-16 09:57:10 "},"3-Basic-Techniques-and-Composition-Theorems/The-sparse-vector-technique/Sparse.html":{"url":"3-Basic-Techniques-and-Composition-Theorems/The-sparse-vector-technique/Sparse.html","title":"3.6.2 稀疏算法","keywords":"","body":"3.6.1 稀疏算法 现在，我们展示如何使用合成技术处理多个“高于阈值”的查询。 稀疏算法可以认为是：当查询进入时，它会反复调用 AboveThreshold。 每次报告高于阈值的查询后，该算法仅在 AboveThreshold 的新实例上重新启动剩余的查询流。在重新启动AboveThreshold ccc 次之后停止（即在出现 ccc 个高于阈值的查询之后）。 由于 AboveThreshold 的每个实例都是(ε,0)(\\varepsilon,0)(ε,0)- 差分隐私的，因此适用合成定理。 定理 3.25 稀疏算法是 (ε,δ)(\\varepsilon,\\delta)(ε,δ)-差分隐私的。 【证明】 我们发现到 Sparse 算法完全等同于以下过程：我们对查询流 {fi}\\{f_i\\}{fi​} 运行 AboveThreshold 算法 (D,{fi},T,ε′)(D,\\{f_i\\},T,\\varepsilon')(D,{fi​},T,ε′)，并设置： ε′={εcif δ=0;ε8cln⁡1δOtherwise. \\varepsilon' = \\begin{cases} \\frac{\\varepsilon}{c} &\\text{if } \\delta = 0 ;\\\\ \\frac{\\varepsilon}{\\sqrt{8c\\ln \\frac{1}{\\delta}}} &\\text{Otherwise.} \\end{cases} ε′={cε​8clnδ1​​ε​​if δ=0;Otherwise.​ 使用 AboveThreshold 算法提供答案。当 AboveThreshold 算法停止时（在回答了1个超过阈值的查询之后），我们只需在剩余的查询流上重新启动 Sparse算法(D,{fi},T,ε′)(D,\\{f_i\\},T,\\varepsilon')(D,{fi​},T,ε′) ，并继续这个过程直到我们重新启动 AboveThreshold 算法 ccc 次。第 ccc 次 AboveThreshold 算法停止后，Sparse算法 也停止。我们已经证明了AboveThreshold 算法 (D,{fi},T,ε′)(D,\\{f_i\\},T,\\varepsilon')(D,{fi​},T,ε′) 是(ε′,0)(\\varepsilon',0)(ε′,0)-差分隐私的。最后，根据高级合成定理（定理 3.20 和 推论 3.21），ccc 个 ε′=ε8cln⁡1δ\\varepsilon' = \\frac{\\varepsilon}{\\sqrt{8c\\ln \\frac{1}{\\delta}}}ε′=8clnδ1​​ε​-差分隐私算法的合成是 (ε,δ)(\\varepsilon,\\delta)(ε,δ) -差分隐私，并且 ccc 个 ε′=ε/c\\varepsilon' = \\varepsilon/cε′=ε/c- 差分隐私算法的合成是 (ε,0)(\\varepsilon,0)(ε,0) -差分隐私。 需要证明 包含 ccc 个 AboveThreshold 算法 的 Sparse 算法的准确性。我们注意到，如果对于每个 AboveThreshold 算法 (α,β/c)(\\alpha,\\beta/c)(α,β/c) 精确的，那么 Sparse 算法将是 (α,β)(\\alpha,\\beta)(α,β) 精确的。 【定理 3.25 证毕】 定理 3.26 对于 k 个查询的任何序列，f1,...,fkf_1,...,f_kf1​,...,fk​ 使得 L(T)≡∣{i:fi(D)≥T−α}∣≤cL(T)\\equiv|\\{i:f_i(D)\\geq T - \\alpha\\}|\\leq cL(T)≡∣{i:fi​(D)≥T−α}∣≤c。如果 δ>0\\delta >0δ>0，当： α=(ln⁡k+ln⁡2cβ)512cln⁡1δε \\alpha = \\frac{(\\ln k+\\ln\\frac{2c}{\\beta})\\sqrt{512c\\ln\\frac{1}{\\delta}}}{\\varepsilon} α=ε(lnk+lnβ2c​)512clnδ1​​​ Sparse 算法是 (α,β)(\\alpha,\\beta)(α,β) 精确的。 如果 δ=0\\delta =0δ=0，当： α=8x(ln⁡k+ln⁡(2c/β))ε \\alpha = \\frac{8x(\\ln k + \\ln(2c/\\beta))}{\\varepsilon} α=ε8x(lnk+ln(2c/β))​ Sparse 算法是 (α,β)(\\alpha,\\beta)(α,β) 精确的。 【证明】 运用 定理3.24 的证明方法，将 β\\betaβ 设为β/c\\beta/cβ/c，并分别根据 δ>0\\delta > 0δ>0 或 δ=0\\delta=0δ=0 将 ε\\varepsilonε 设为 ε8cln⁡1δ\\frac{\\varepsilon}{\\sqrt{8c\\ln \\frac{1}{\\delta}}}8clnδ1​​ε​ 和 ε/c\\varepsilon/cε/c 即可。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-16 09:59:31 "},"3-Basic-Techniques-and-Composition-Theorems/The-sparse-vector-technique/NumericSparse.html":{"url":"3-Basic-Techniques-and-Composition-Theorems/The-sparse-vector-technique/NumericSparse.html","title":"3.6.3 数值稀疏算法","keywords":"","body":"3.6.3 数值稀疏算法 最后，我们给出了 Sparse 算法的一个版本，它实际上输出了高于阈值查询的数值，我们只需要在精度上损失一个常数因子就可以做到这一点。我们称这种算法为 NumericSparse，它是一种简单的使用 Laplace 机制组成的 Sparse 算法。它不是输出向量 a∈{⊤,⊥}∗a \\in \\{\\top,\\bot\\}^*a∈{⊤,⊥}∗ ，而是输出向量 a∈(R∪{⊥})∗a \\in (\\mathbb{R} \\cup \\{\\bot\\})^*a∈(R∪{⊥})∗。 我们发现 NumericSparse 算法是具有隐私性的： 定理 3.27 NumericSparse 算法是(ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私的。 【证明】 我们发现，如果δ=0\\delta=0δ=0，则NumericSparse算法 (D,{fi},T,c,ε,0)(D,\\{f_i\\},T,c,\\varepsilon,0)(D,{fi​},T,c,ε,0) 就是 Sparse 算法 (D,{fi},T,c,89ε,0)(D,\\{f_i\\},T,c,\\frac{8}{9}\\varepsilon,0)(D,{fi​},T,c,98​ε,0) 的自适应组合，其中输出具体数值使用了具有隐私参数 (ε′,δ)=(19ε,0)(\\varepsilon',\\delta)=(\\frac{1}{9}\\varepsilon,0)(ε′,δ)=(91​ε,0) 的 Lapalace 机制。如果 δ>0\\delta>0δ>0，则 NumericSparse 算法 (D,{fi},T,c,ε,δ)(D,\\{f_i\\},T,c,\\varepsilon,\\delta)(D,{fi​},T,c,ε,δ) 是 Sparse 算法 (D,{fi},T,c,512512+1ε,δ/2)(D,\\{f_i\\},T,c,\\frac{\\sqrt{512}}{\\sqrt{512}+1}\\varepsilon,\\delta/2)(D,{fi​},T,c,512​+1512​​ε,δ/2) 的自适应组合， 其中输出具体数值使用了具有隐私参数 (ε′,δ)=(1512ε,δ/2)(\\varepsilon',\\delta)=(\\frac{1}{\\sqrt{512}}\\varepsilon,\\delta/2)(ε′,δ)=(512​1​ε,δ/2) 的 Lapalace 机制。 因此，NumericSparse 算法的隐私来自简单的组合。 【定理 3.27 证毕】 要讨论准确性，我们必须定义一种机制的准确性，这是指响应一系列数值查询而输出流 a∈(R∪{⊥})∗a \\in (\\mathbb{R} \\cup \\{\\bot\\})^*a∈(R∪{⊥})∗ 的含义： 定义3.10（数值精度） 一个响应 kkk 个查询流 f1,...,fkf_1,...,f_kf1​,...,fk​ 并输出应答流 a1,...,∈(R∪{⊥})∗a_1,...,\\in(\\mathbb{R} \\cup \\{\\bot\\})^*a1​,...,∈(R∪{⊥})∗ 的算法，如果除概率最大为 β\\betaβ 之外，算法不会在 fkf_kfk​ 之前停止，并且对于所有 ai∈Ra_i \\in \\mathbb{R}ai​∈R 有： ∣fi(D)−ai∣≤α |f_i(D)-a_i|\\leq \\alpha ∣fi​(D)−ai​∣≤α 对于所有 ai=⊥a_i =\\botai​=⊥，有： fi(D)≤T+α f_i(D) \\leq T + \\alpha fi​(D)≤T+α 则这个算法是相对于阈值 TTT 的 (α,β)(\\alpha,\\beta)(α,β) 准确。 定理 3.28。 对于 kkk 个查询的任何序列 f1,...fkf_1,...f_kf1​,...fk​ 使得 L(T)≡∣{i:fi(D)≥T−α}∣≤cL(T)\\equiv|\\{i:f_i(D)\\geq T-\\alpha\\}|\\leq cL(T)≡∣{i:fi​(D)≥T−α}∣≤c ，如果 δ>0\\delta>0δ>0，当： α=(ln⁡k+ln⁡4cβ)cln⁡2δ(512+1)ε \\alpha = \\frac{(\\ln k+\\ln \\frac{4c}{\\beta})\\sqrt{c\\ln \\frac{2}{\\delta}}(\\sqrt{512}+1)}{\\varepsilon} α=ε(lnk+lnβ4c​)clnδ2​​(512​+1)​ NumericSparse 算法是相对于阈值 TTT 的 (α,β)(\\alpha,\\beta)(α,β) 准确的。 如果 δ=0\\delta=0δ=0，当： α=9c(ln⁡k+ln⁡(4c/β))ε \\alpha = \\frac{9c(\\ln k + \\ln(4c/\\beta))}{\\varepsilon} α=ε9c(lnk+ln(4c/β))​ NumericSparse 算法是相对于阈值 TTT 的 (α,β)(\\alpha,\\beta)(α,β) 准确的。 【证明】 精度需要两个条件：首先，对于所有 ai=⊥:fi(D)≤Ta_i =\\bot:f_i(D)\\leq Tai​=⊥:fi​(D)≤T： Sparse 准确定理以 1−β/21-\\beta/21−β/2 概率成立。另外，对于所有 ai∈Ra_i\\in \\mathbb{R}ai​∈R ，它要求 ∣fi(D)−ai∣≤α|f_i(D)-a_i|\\leq \\alpha∣fi​(D)−ai​∣≤α 。 这通过 Laplace 机制的精度以 1−β/21-\\beta/21−β/2 概率成立。 【定理 3.28证毕】 我们到底显示了什么？如果给我们一系列查询，并保证只有最多 ccc 个答案的答案高于 T+αT+\\alphaT+α，我们就可以回答高于给定阈值 TTT 的那些查询，直至误差 α\\alphaα。如果我们事先知道进行这些高于阈值查询的身份，并使用拉普拉斯机制进行回答，那么在给定相同的隐私保证的情况下，此精度等于（等于常数和log⁡k\\log klogk）。也就是说，稀疏向量技术允许我们几乎“免费”地辨别这些大型查询的身份，只为这些不相关的查询进行对数精度的响应。这种算法与另一种形式（通过指数机制找到造成隐私损失大的查询，然后通过拉普拉斯机制响应这些查询）提供相同的保证。然而，这个稀疏向量算法运行起来很简单，而且最关键的是，它允许我们自适应地选择查询。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-16 09:58:47 "},"3-Basic-Techniques-and-Composition-Theorems/Bibliographic-notes.html":{"url":"3-Basic-Techniques-and-Composition-Theorems/Bibliographic-notes.html","title":"参考文献","keywords":"","body":"参考文献 Randomized Response is due to Warner [84] (predating differential privacy by four decades!). The Laplace mechanism is due to Dwork et al. [23]. The exponential mechanism was invented by McSherry and Talwar [60]. Theorem 3.16 (simple composition) was claimed in [21]; the proof appearing in Appendix B is due to Dwork and Lei [22]; McSherry and Mironov obtained a similar proof. The material in Sec-tions 3.5.1 and 3.5.2 is taken almost verbatim from Dwork et al. [32].\\text{Pr}ior to [32] composition was modeled informally, much as we did for the simple composition bounds. For specific mechanisms applied on a single database, there are “evolution of confidence” arguments due to Dinur, Dwork, and Nissim [18, 31], (which pre-date the definition of differential privacy) showing that the privacy parameter in k-fold com- √k position need only deteriorate like k if we are willing to tolerate a (negligible) loss in δ (for k Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-11-23 10:57:42 "},"4-Releasing-Linear-Quries-with-Correlated-Error/Overview.html":{"url":"4-Releasing-Linear-Quries-with-Correlated-Error/Overview.html","title":"四、带相关误差的线性查询","keywords":"","body":"四、带相关误差的线性查询 隐私数据分析中最基本的原语之一就是能够回答对数据集的数值查询。在上一章中，我们开始看到可以通过向查询答案中添加独立的噪声来实现此目的的工具。 在本章中，我们将继续进行这项研究，并发现通过添加相关的噪声的方法代替以前添加独立噪声的方法，我们可以获得更高精确度回答更为大量查询的能力。在这里，我们看到了解决此问题的两种特定机制，我们将在下一节中对其进行概括。 在本节中，我们考虑的算法比简单使用拉普拉斯机制合成的算法能更精确地解决查询发布问题。这种改进是可能的，因为查询集是作为一个整体来处理的（即使在在线设置中也是如此），所以允许将各个查询的噪音关联起来。要立即发现符合这些思路的某些内容，请考虑第1章中描述的差异攻击中的一对查询。“数据库中有多少人具有镰状细胞特征？”和“除了 X 以外，数据库中有多少人具有镰状细胞特征？” 假设有一种机制使用拉普拉斯机制回答了第一个问题，然后在提出第二个问题时回答“您已经知道了近似答案，因为您刚刚问过我几乎相似的问题。”对这两个问题的协调响应不会比单独执行任何一个问题所引起的隐私损失更多，因此节省了（少量）隐私。 查询发布问题是很自然的：给定一类查询通过数据库 Q\\mathcal{Q}Q，我们希望为每个查询 fi∈Qf_i \\in \\mathcal{Q}fi​∈Q 发布一些答案 aia_iai​ ，使得误差 max⁡i∣ai−fi(x)∣\\max_i|a_i-f_i(x)|maxi​∣ai​−fi​(x)∣ 尽可能小，而仍然保留着差分隐私  [1]\\ ^[1] [1]。回想一下，对于任何低敏感性查询，我们可以应用拉普拉斯机制向这些查询的每个答案添加不同的，独立的噪声。不幸的是，在固定的隐私级别上，比如 (ε,0)(\\varepsilon,0)(ε,0)-隐私保证，我们必须向答案添加一定大小的噪声，这些噪声是由尺度参数为 ∣Q∣|\\mathcal{Q}|∣Q∣ 的拉普拉斯机制产生的。添加这样大小的噪声是因为这是组合查询的敏感度可能增长的速率。同样，对于 (ε,δ)(\\varepsilon,\\delta)(ε,δ) 隐私保证来说，噪声尺度参数为 ∣Q∣ln⁡(1/δ)\\sqrt{|\\mathcal{Q}|\\ln(1/\\delta)}∣Q∣ln(1/δ)​。例如，假设我们的查询类别 Q\\mathcal{Q}Q 仅包含同一查询的许多副本：fi=f∗f_i=f^*fi​=f∗。如果我们使用拉普拉斯机制来释放答案，它将添加独立的噪声，因此每个 aia_iai​ 将是均值为 f∗(x)f^*(x)f∗(x) 的独立随机变量。显然，在这种情况下，噪声速率必须以 ∣Q∣|\\mathcal{Q}|∣Q∣ 增长。因为否则 aia_iai​ 的平均值将收敛到真实值 f∗(x)f^*(x)f∗(x)，这将违反隐私要求的。但是，在这种情况下，因为对于所有 iii 来说 fi=f∗f_i = f^*fi​=f∗，所以仅对 f∗f^*f∗ 近似一次（a∗≈f∗(X)a^* \\approx f^*(X)a∗≈f∗(X)），并为所有 iii 释放 ai=a∗a_i=a^*ai​=a∗ 更有意义。在这种情况下，噪声尺度将不必再按 ∣Q∣|\\mathcal{Q}|∣Q∣ 。在本章中，我们旨在通过添加非独立噪声作为查询集的函数来设计比 Laplace 机制更精确的算法（误差范围为 log⁡∣Q∣\\log |\\mathcal{Q}|log∣Q∣）。 （原文注[1] 正是隐私限制使问题变得有趣。如果没有这个约束，查询发布问题只需为每个查询输出精确的答案，就可以轻松而优化地解决。） 回想一下，我们的数据全体是 X={χ1,χ2,...,χ∣X∣}\\mathcal{X}=\\{\\chi_1,\\chi_2,...,\\chi_{|\\mathcal{X}|}\\}X={χ1​,χ2​,...,χ∣X∣​}，数据库用 N∣X∣\\mathbb{N}^{|\\mathcal{X}|}N∣X∣ 直方图表示。线性查询类似于计数查询，但归一化为取区间 [0,1][0,1][0,1] 中的值，而不仅仅是布尔值。具体地说，线性查询 fff 采用 f:X→[0,1]f:\\mathcal{X}\\to[0,1]f:X→[0,1] 的形式，并应用于数据库 xxx，返回数据库上查询的总和或平均值（我们将考虑两者，具体取决于哪个更便于分析）。当我们认为线性查询是返回平均值时，我们将它们称为标准化线性查询，并且其函数值为： f(x)=1∥x∥1∑i=1∣X∣xi⋅f(χi) f(x) = \\frac{1}{\\Vert x\\Vert _1}\\sum_{i=1}^{|\\mathcal{X}|}x_i\\cdot f(\\chi_i) f(x)=∥x∥1​1​i=1∑∣X∣​xi​⋅f(χi​) 当我们认为线性查询返回求和值时，我们将它们称为非标准化的线性查询，并且其函数值为： f(x)=∑i=1∣X∣xi⋅f(χi) f(x)=\\sum_{i=1}^{|\\mathcal{X}|}x_i\\cdot f(\\chi_i) f(x)=i=1∑∣X∣​xi​⋅f(χi​) 【补充： 该部分约定的符号所代表的含义可以参考2.3节中内容。其中 X={χ1,χ2,...,χ∣X∣}\\mathcal{X}=\\{\\chi_1,\\chi_2,...,\\chi_{|\\mathcal{X}|}\\}X={χ1​,χ2​,...,χ∣X∣​} 表示数据全体， χi\\chi_iχi​ 表示不同类别的数据集。xix_ixi​ 表示 χi\\chi_iχi​ 的记录数量。∥x∥1\\Vert x\\Vert _1∥x∥1​ 代表 ℓ1\\ell_1ℓ1​ 距离概念，∥x∥1\\Vert x\\Vert _1∥x∥1​ 表示数据库 xxx 的记录数。∣X∣|\\mathcal{X}|∣X∣ 表示数据全体中所有的类别总数。又有 f(χi)→[0,1]f(\\chi_i)\\to [0,1]f(χi​)→[0,1]】 无论何时我们声明一个界限，都应该从上下文中清楚地知道我们所说的是标准化查询还是非标准化查询，因为它们接受的值在非常不同的范围内。注意，标准化线性查询采用 [0,1][0,1][0,1] 中的值，而非标准化查询采用[0,∥x∥1][0,\\Vert x\\Vert _1][0,∥x∥1​] 中的值。 注意，根据该定义，线性查询的灵敏度 Δf≤1\\Delta f\\leq 1Δf≤1 。后面的章节将讨论任意的低敏感度查询。 我们将介绍两种技术，分别用于离线和在线案例。令人惊讶的是，离线技术是指数机制的一个即时应用，它使用了来自学习理论的采样边界！该算法简单应用指数机制，该指数机制的范围等于所有小型数据库 yyy ，并且质量函数 u(x,y)u(x,y)u(x,y) ： u(x,y)=−max⁡f∈Q∣f(x)−f(y)∣(4.1) u(x,y) = -\\max_{f\\in \\mathcal{Q}} |f(x)-f(y)| \\qquad \\qquad \\quad(4.1) u(x,y)=−f∈Qmax​∣f(x)−f(y)∣(4.1) 采样边界（见下面的引理4.3）告诉我们，xxx 的 ln⁡∣Q/α2∣\\ln|\\mathcal{Q}/\\alpha^2|ln∣Q/α2∣ 个元素的随机子集很可能给我们所有 f(x)f(x)f(x) 的一个很好的近似（具体地，加性误差由 α\\alphaα 限制），因此我们知道将可能的输出集合限制到小的数据库是足够的。我们实际上并不关心潜在输出数据库是小的，只关心它们别太多：它们的数量在效用证明中起作用，这是指数机制效用定理（定理3.11）的直接应用。更具体地说，如果潜在输出的总数不是太多（特别是，低效用输出的总数不是太多），则坏输出与好输出的比率不是太大。 在线机制虽然事先不知道整个查询集，但其精度与离线机制相同，是稀疏向量技术的直接应用。因此，隐私是能保证，但效用性将需要证明。关键在于，即使对于一组非常大的计数查询，也很少有查询是“重要的”；也就是说，重要的查询将是“稀疏的”。与稀疏向量算法一样，我们可以根据有效查询的数量缩放噪声，而不依赖于查询的总数。 在我们继续介绍这些机制之前，我们只给出一个有用的线性查询类的示例。 示例 4.1 假设数据库的元素由 ddd 个布尔特征表示。例如，第一特征可以表示个人是否是男性或女性，第二特征可以表示他们是否是大学毕业生，第三特征可以表示他们是否是公民等等。我们的数据全体为 X={0,1}d\\mathcal{X}=\\{0,1\\}^dX={0,1}d。给定这些属性的子集 S⊆{1,...,d}S \\subseteq \\{1,...,d\\}S⊆{1,...,d} ，我们可能想知道数据集中有多少人具有这些属性。（例如，“肺癌家族史的男性大学毕业生占数据集的多少？”）这自然定义了一个查询，称为 单调关联查询。由属性 SSS 的子集进行参数化，并定义为：fS(z)=∏i∈Szif_S(z)=\\prod_{i\\in S}z_ifS​(z)=∏i∈S​zi​，其中 z∈Xz\\in \\mathcal{X}z∈X。所有此类查询为 Q={fS:S⊆{1,...,d}}\\mathcal{Q}=\\{f_S:S \\subseteq \\{1,...,d\\}\\}Q={fS​:S⊆{1,...,d}} ，并且大小为 ∣Q∣=2d|\\mathcal{Q}|=2^d∣Q∣=2d。回答的集合有时称为列联表或边际表，是发布有关数据集的统计信息的常用方法。通常，我们可能对所有答案的关联都不感兴趣，而只是对那些询问特征集 SSS 中固定大小（∣S∣=k|S|=k∣S∣=k）的子集的答案感兴趣。这一类的查询 Qk={fS:S⊆{1,...,d},∣S∣=k}\\mathcal{Q}_k=\\{f_S:S \\subseteq \\{1,...,d\\},|S|=k\\}Qk​={fS​:S⊆{1,...,d},∣S∣=k} 大小为：(kd)(_k^d)(kd​) （排列组合公式 CdkC_d^kCdk​） 这类查询只是本节中给出的算法可以准确回答的一个示例。（请注意，如果我们也希望允许询问否定属性的（非单调）关联，我们也可以做到这一点——只需将特征空间从 ddd 翻倍到 2d2d2d，并为所有 iii 设置 zd+i=1−ziz_{d+i}=1-z_izd+i​=1−zi​，其中所有 i∈{1,...,d}i \\in \\{1,...,d\\}i∈{1,...,d}。） 【补充个人理解例4.1：数据库的元素由 ddd 个布尔特征表示，可以知道，数据总体 X\\mathcal{X}X 有 ddd 个特征求其笛卡尔乘积表示：X={{0,1}×,...,{0,1}⎵d}={0,1}d\\mathcal{X}=\\{\\underbrace{\\{0,1\\}\\times,...,\\{0,1\\}}_{\\text{d}}\\}=\\{0,1\\}^dX={d{0,1}×,...,{0,1}​​}={0,1}d。存在对这些特征进行查询，并列查询特征个数从 111 个到 ddd 个。同时，我们想知道关于符合这些特征的人数占总人数的多少，会自然而然使用：fS(z)=∏i∈Szif_S(z)=\\prod_{i\\in S}z_ifS​(z)=∏i∈S​zi​ 这个公式。这个公式中 ziz_izi​ 为拥有 iii 特征的人数在总人数的占比，显然总的占比为各个特征占比的乘积。对特征的查询集合（不限特征个数）即为文中的 Q={fS:S⊆{1,...,d}}\\mathcal{Q}=\\{f_S:S \\subseteq \\{1,...,d\\}\\}Q={fS​:S⊆{1,...,d}}，由集合的子集总数公式可得：∣Q∣=2d|\\mathcal{Q}|=2^d∣Q∣=2d。 之后，对于去非条件的查询，需要对特征进行拓展，如最后一段所属。举例来说即： “肺癌家族史的男性非大学毕业生占数据集的多少？” 】 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-10 19:02:04 "},"4-Releasing-Linear-Quries-with-Correlated-Error/An-offline-algorithm-SmallDB/An-offline-algorithm-SmallDB.html":{"url":"4-Releasing-Linear-Quries-with-Correlated-Error/An-offline-algorithm-SmallDB/An-offline-algorithm-SmallDB.html","title":"SmallDB：离线算法","keywords":"","body":"4.1 SmallDB：离线算法 在本节中，我们给出一种基于采样思想得到一个小数据库的算法，并用于指数机制上。我们将表明，对于计数查询，只需考虑较小的数据库即可：它们的大小仅是查询类的函数以及我们所需的近似精度α\\alphaα，而私有数据库大小 ∥x∥1\\Vert x\\Vert _1∥x∥1​ 不是关键。这很重要，因为它将允许我们同时为所有足够大的数据库保证在指数机制范围内至少有一个数据库，这个数据库在查询 Q\\mathcal{Q}Q 中可以很好地近似于 xxx，并且还能保证在数据库中没有太多数据库消除“好”数据库上的概率质量。 我们首先认识到 SmallDB 机制是 ε\\varepsilonε- 差分隐私的。 命题 4.1 SmallDB 机制是 (ε,0)(\\varepsilon,0)(ε,0)- 差分隐私的。 【证明】 SmallDB 机制是指数机制的一个简单实例，因此其隐私性服从 定理3.10 我们可以类似地使用指数机制分析方式，对 SmallDB 机制进行分析，以了解其效用保证。但是首先，我们必须证明我们选择范围 R={y∈N∣X∣:∥y∥1=log⁡∣Q∣α2}\\mathcal{R}=\\{y \\in \\mathbb{N}^{|\\mathcal{X}|}:\\Vert y\\Vert _1 = \\frac{\\log |\\mathcal{Q}|}{\\alpha^2}\\}R={y∈N∣X∣:∥y∥1​=α2log∣Q∣​}，其输出的所有数据库集合的大小为：log⁡∣Q∣/α2\\log |\\mathcal{Q}|/\\alpha^2log∣Q∣/α2。 【命题 4.1 证毕】 定理 4.2 对于线性查询 Q\\mathcal{Q}Q 的任何有限类，如果 R={y∈N∣X∣:∥y∥1=log⁡∣Q∣α2}\\mathcal{R}=\\{y \\in \\mathbb{N}^{|\\mathcal{X}|}:\\Vert y\\Vert _1 = \\frac{\\log |\\mathcal{Q}|}{\\alpha^2}\\}R={y∈N∣X∣:∥y∥1​=α2log∣Q∣​} 则对于所有 x∈N∣X∣x \\in \\mathbb{N}^{|\\mathcal{X}|}x∈N∣X∣，存在一个 y∈Ry \\in \\mathcal{R}y∈R 使得： max⁡f∈Q∣f(x)−f(y)∣≤α \\max_{f\\in \\mathcal{Q}}|f(x)-f(y)|\\leq \\alpha f∈Qmax​∣f(x)−f(y)∣≤α 成立。 换句话说，我们将证明对于线性查询的任何集合 Q\\mathcal{Q}Q，对于任何数据库 xxx，都有一个“小”数据库 yyy，其大小为 ∥y∥1=log⁡∣Q∣α2\\Vert y\\Vert _1 = \\frac{\\log |\\mathcal{Q}|}{\\alpha^2}∥y∥1​=α2log∣Q∣​ ，这个“小”数据库能将 Q\\mathcal{Q}Q 中每个查询的答案编码，编码误差最大为 α\\alphaα。 引理 4.3（采样边界） 对于任何 x∈N∣X∣x \\in \\mathbb{N}^{|\\mathcal{X}|}x∈N∣X∣ 和 任何 线性查询集合 Q\\mathcal{Q}Q ，存在一个数据库 yyy，其大小为： ∥y∥1=log⁡∣Q∣α2 \\Vert y\\Vert _1=\\frac{\\log|\\mathcal{Q}|}{\\alpha^2} ∥y∥1​=α2log∣Q∣​ 使得： max⁡f∈Q∣f(x)−f(y)∣≤α \\max_{f\\in \\mathcal{Q}}|f(x)-f(y)|\\leq \\alpha f∈Qmax​∣f(x)−f(y)∣≤α 【证明】 令 m=log⁡∣Q∣α2m=\\frac{\\log|\\mathcal{Q}|}{\\alpha^2}m=α2log∣Q∣​ 。 我们将从 xxx 的元素中提取 mmm 个均匀随机的样本来构建数据库 yyy。 具体来说，对于 i∈{1,...,m}i \\in \\{1,...,m\\}i∈{1,...,m}，令 XiX_iXi​ 为一个以概率 xj/∥x∥1x_j/\\Vert x \\Vert_1xj​/∥x∥1​ 从 χj∈X\\chi_j \\in \\mathcal{X}χj​∈X 取值的随机变量，令 yyy 为包含元素 X1,...,XmX_1,...,X_mX1​,...,Xm​ 的数据库。现在固定任何 f∈Qf\\in \\mathcal{Q}f∈Q 并考虑概率 f(y)f(y)f(y)。我们有： f(y)=1∥y∥1∑i=1∣X∣yi⋅f(χi)=1m∑i=1mf(Xi) f(y) = \\frac{1}{\\Vert y \\Vert_1}\\sum_{i=1}^{|\\mathcal{X}|}y_i\\cdot f(\\chi_i) = \\frac{1}{m} \\sum_{i=1}^m f(X_i) f(y)=∥y∥1​1​i=1∑∣X∣​yi​⋅f(χi​)=m1​i=1∑m​f(Xi​) 我们注意到和的每个项 f(Xi)f(X_i)f(Xi​) 是有界随机变量，取值为 0≤f(Xi)≤10 \\leq f(X_i) \\leq 10≤f(Xi​)≤1，其期望为： E[f(Xi)]=∑j=1∣X∣xj∥x∥1f(χj)=f(x) \\mathbb{E}[f(X_i)] = \\sum_{j=1}^{|\\mathcal{X}|}\\frac{x_j}{\\Vert x \\Vert_1}f(\\chi_j) = f(x) E[f(Xi​)]=j=1∑∣X∣​∥x∥1​xj​​f(χj​)=f(x) f(y)f(y)f(y) 的期望为： E[f(y)]=1m∑i=1mE[f(Xi)]=f(x) \\mathbb{E}[f(y)] = \\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}[f(X_i)] = f(x) E[f(y)]=m1​i=1∑m​E[f(Xi​)]=f(x) 因此，我们可以应用 定理3.1 中所述的切尔诺夫不等式，这使： Pr[∣f(y)−f(x)∣>α]≤2e−2mα2 \\text{Pr}[|f(y)-f(x)|>\\alpha]\\leq 2e^{-2m\\alpha^2} Pr[∣f(y)−f(x)∣>α]≤2e−2mα2 在所有线性查询 f∈Qf \\in \\mathcal{Q}f∈Q 上使用布尔不等式，我们得到： Pr[max⁡f∈Q∣f(y)−f(x)∣>α]≤2∣Q∣e−2mα2 \\text{Pr}\\big[\\max_{f \\in \\mathcal{Q}}|f(y)-f(x)|>\\alpha\\big]\\leq 2|\\mathcal{Q}|e^{-2m\\alpha^2} Pr[f∈Qmax​∣f(y)−f(x)∣>α]≤2∣Q∣e−2mα2 当 m=log⁡∣Q∣α2m=\\frac{\\log|\\mathcal{Q}|}{\\alpha^2}m=α2log∣Q∣​ 使等式右边小于 111（只要 Q>2\\mathcal{Q}>2Q>2 ），z这证明了存在一个满足上述界限的大小为 mmm 的数据库，从而完成了引理的证明。 【引理 4.3 证毕】。 因为 R\\mathcal{R}R 必包含所有大小为 log⁡∣Q∣α2\\frac{\\log|\\mathcal{Q}|}{\\alpha^2}α2log∣Q∣​ 的数据库，可以简单地得出 定理4.2 的证明。 命题 4.4 令 Q\\mathcal{Q}Q 为线性查询的任何类别。 令 yyy 为 SmallDB(x,Q,ε,α)\\text{SmallDB}(x,\\mathcal{Q},\\varepsilon,\\alpha)SmallDB(x,Q,ε,α)输出的数据库。然后以 1−β1-\\beta1−β 的概率有： max⁡f∈Q∣f(x)−f(y)∣≤α+2(log⁡∣X∣log⁡∣Q∣α2+log⁡(1β))ε∥x∥1 \\max_{f\\in\\mathcal{Q}}|f(x)-f(y)|\\leq \\alpha + \\frac{2\\Big(\\frac{\\log|\\mathcal{X}|\\log|\\mathcal{Q}|}{\\alpha^2}+\\log(\\frac{1}{\\beta})\\Big)}{\\varepsilon\\Vert x \\Vert_1} f∈Qmax​∣f(x)−f(y)∣≤α+ε∥x∥1​2(α2log∣X∣log∣Q∣​+log(β1​))​ 【证明】 应用指数机制的效用边界定理 （定理 3.11），其中 Δu=1∥x∥1,OPTu(D)≤α\\Delta u=\\frac{1}{\\Vert x \\Vert_1}, \\text{OPT}_u(D)\\leq \\alphaΔu=∥x∥1​1​,OPTu​(D)≤α（由 定理 4.2 得来），我们发现： Pr[max⁡f∈Q∣f(x)−f(y)∣≥α+2ε∥x∥1(log⁡(∣R∣)+t)]≤e−t \\text{Pr}[\\max_{f \\in \\mathcal{Q}}|f(x)-f(y)|\\geq \\alpha + \\frac{2}{\\varepsilon \\Vert x \\Vert_1}(\\log(|\\mathcal{R}|)+t)]\\leq e^{-t} Pr[f∈Qmax​∣f(x)−f(y)∣≥α+ε∥x∥1​2​(log(∣R∣)+t)]≤e−t 我们通过两步完成证明：（1）注意到数据库 yyy 的大小 R\\mathcal{R}R 至多为 log⁡∣Q∣/α2\\log|\\mathcal{Q}|/\\alpha^2log∣Q∣/α2，所以 ∣R∣≤∣X∣log⁡∣Q∣/α2|\\mathcal{R}|\\leq |\\mathcal{X}|^{\\log|\\mathcal{Q}|/\\alpha^2}∣R∣≤∣X∣log∣Q∣/α2；（2）由于概率为：1−β1-\\beta1−β，所以令 t=log⁡(1β)t=\\log(\\frac{1}{\\beta})t=log(β1​)。 【命题 4.4 证毕】。 最后，我们要声明 SmallDB 的效用定理。 定理4.5 通过适当选择 α\\alphaα，令 yyy 为 SmallDB(x,Q,ε,α2)\\text{SmallDB}(x,\\mathcal{Q},\\varepsilon,\\frac{\\alpha}{2})SmallDB(x,Q,ε,2α​) 的数据库输出，我们可以确保概率为 1−β1-\\beta1−β 时有： max⁡f∈Q∣f(x)−f(y)∣≤(16log⁡∣X∣log⁡∣Q∣+4log⁡(1β)ε∥x∥1)1/3(4.2) \\max_{f\\in\\mathcal{Q}}|f(x)-f(y)|\\leq \\Bigg( \\frac{16\\log |\\mathcal{X}|\\log|\\mathcal{Q}|+4\\log(\\frac{1}{\\beta})}{\\varepsilon\\Vert x \\Vert_1}\\Bigg)^{1/3} \\qquad \\qquad (4.2) f∈Qmax​∣f(x)−f(y)∣≤(ε∥x∥1​16log∣X∣log∣Q∣+4log(β1​)​)1/3(4.2) 等价地，对于任何数据库 xxx ，其： ∥x∥1≥16log⁡∣X∣log⁡∣Q∣+4log⁡(1β)εα3(4.3) \\Vert x \\Vert_1 \\geq \\frac{16\\log |\\mathcal{X}|\\log|\\mathcal{Q}|+4\\log(\\frac{1}{\\beta})}{\\varepsilon \\alpha^3}\\qquad \\qquad (4.3) ∥x∥1​≥εα316log∣X∣log∣Q∣+4log(β1​)​(4.3) 有概率 1−β1-\\beta1−β 使得 max⁡f∈Q∣f(x)−f(y)∣≤α\\max_{f\\in\\mathcal{Q}}|f(x)-f(y)|\\leq \\alphamaxf∈Q​∣f(x)−f(y)∣≤α。 【证明】 根据定理 4.4 我们有 ： max⁡f∈Q∣f(x)−f(y)∣≤α2+2(4log⁡∣X∣log⁡∣Q∣α2+log⁡(1β))ε∥x∥1 \\max_{f\\in\\mathcal{Q}}|f(x)-f(y)|\\leq \\frac{\\alpha}{2} + \\frac{2\\Big(\\frac{4\\log|\\mathcal{X}|\\log|\\mathcal{Q}|}{\\alpha^2}+\\log(\\frac{1}{\\beta})\\Big)}{\\varepsilon\\Vert x \\Vert_1} f∈Qmax​∣f(x)−f(y)∣≤2α​+ε∥x∥1​2(α24log∣X∣log∣Q∣​+log(β1​))​ 设置该量最大为 α\\alphaα ，则证明式（4.3）成立。即可推出式 （4.2），同时能能出式（4.4） 请注意，该定理指出，对于固定的 α\\alphaα 和 ε\\varepsilonε，即使 δ=0\\delta=0δ=0，也可以回答与数据库大小成指数数量的查询（查询数量 k≤exp⁡(O(α3ε∥x∥1log⁡∣X∣))k\\leq \\exp\\Big(O\\Big(\\frac{\\alpha^3\\varepsilon\\Vert x\\Vert_1}{\\log|\\mathcal{X}|}\\Big)\\Big)k≤exp(O(log∣X∣α3ε∥x∥1​​))）。这与拉普拉斯机制相反，当我们直接拉普拉斯机制用于回答线性查询时，只能线性大小的查询。 还请注意，在此讨论中，考虑标准化查询最为方便。 但是，我们可以简单地乘以 ∥x∥1\\Vert x \\Vert_1∥x∥1​来获得未标准化查询的相应范围： 定理4.6（非标准化查询的精度定理） 通过适当选择 α\\alphaα，令 yyy 为 SmallDB(x,Q,ε,α2)\\text{SmallDB}(x,\\mathcal{Q},\\varepsilon,\\frac{\\alpha}{2})SmallDB(x,Q,ε,2α​) 的数据库输出，我们可以确定当概率为 1−β1-\\beta1−β 时： max⁡f∈Q∣f(x)−f(y)∣≤∥x∥12/3(16log⁡∣X∣log⁡∣Q∣+4log⁡(1β)ε∥x∥1)1/3(4.4) \\max_{f\\in\\mathcal{Q}}|f(x)-f(y)|\\leq \\Vert x\\Vert_1^{2/3}\\Bigg( \\frac{16\\log |\\mathcal{X}|\\log|\\mathcal{Q}|+4\\log(\\frac{1}{\\beta})}{\\varepsilon\\Vert x \\Vert_1}\\Bigg)^{1/3} \\qquad \\qquad (4.4) f∈Qmax​∣f(x)−f(y)∣≤∥x∥12/3​(ε∥x∥1​16log∣X∣log∣Q∣+4log(β1​)​)1/3(4.4) Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-16 10:07:50 "},"4-Releasing-Linear-Quries-with-Correlated-Error/An-offline-algorithm-SmallDB/An-offline-algorithm-SmallDB-Refined-Bounds.html":{"url":"4-Releasing-Linear-Quries-with-Correlated-Error/An-offline-algorithm-SmallDB/An-offline-algorithm-SmallDB-Refined-Bounds.html","title":"SmallDB：精确边界","keywords":"","body":"4.1 SmallDB：离线算法-精确边界 更精确的边界 我们证明，每组线性查询 Q\\mathcal{Q}Q 都有一个最大为 ∣X∣log⁡∣Q∣/α2|\\mathcal{X}|^{\\log|\\mathcal{Q}|/\\alpha^2}∣X∣log∣Q∣/α2 的数据库集合，这些数据库相对于 Q\\mathcal{Q}Q 很好地近似了每个数据库 xxx，且误差最大为 α\\alphaα。但是，这通常被高估了，因为它完全忽略了查询的结构。例如，如果 Q\\mathcal{Q}Q 仅包含相同的查询，并且每次以不同的伪装一次又一次地重复，那么就没有理由使指数机制的范围大小随 ∣Q∣|\\mathcal{Q}|∣Q∣ 增大。类似地，甚至可能存在具有无限基数的查询类 Q\\mathcal{Q}Q，但是尽管如此，小型数据库仍可以很好地近似它们。例如，判断一个点是否在实线上的给定间隔内的查询形成了一个无限大的查询类 Q\\mathcal{Q}Q，因为实线上有无数的间隔。但是，此类查询具有非常简单的结构，可使其被小型数据库很好地近似。通过考虑查询类的更精细的结构，我们将能够为差分隐私机制提供边界，这些差分隐私机将改善简单采样边界（引理4.3），即使对于双倍指数级的大查询类也可以说是不平凡的。此处并未完全阐明这些界限，但会针对较简单的计数查询类声明一些结果。回想一下计数查询 f:X→{0,1}f:\\mathcal{X}\\to \\{0,1\\}f:X→{0,1} 将数据库点映射为布尔值，而不是像线性查询那样映射到 [0,1][0,1][0,1] 中的任何值。 定义4.1（Shattering） 如果对于每个 T⊆ST\\subseteq ST⊆S，存在一个 f∈Qf \\in \\mathcal{Q}f∈Q 使得 {x∈S:f(x)=1}=T\\{x\\in S:f(x)=1\\}=T{x∈S:f(x)=1}=T，则称这一类计数查询 Q\\mathcal{Q}Q 会“打散”点 S⊆XS\\subseteq \\mathcal{X}S⊆X 的集合。也就是说：如果在 2∣S∣2^{|S|}2∣S∣ 中的每一个 SSS 的子集 TTT，Q\\mathcal{Q}Q 中有一些函数将这些元素准确地标记为正，而不标记 S\\TS \\backslash TS\\T 其中的任何元素为正，则称为 Q\\mathcal{Q}Q “打散” SSS。 注意，要使 Q\\mathcal{Q}Q “打散” SSS，必须是 ∣Q≥2∣S∣∣|\\mathcal{Q}\\geq 2^{|S|}|∣Q≥2∣S∣∣ 的情况。 因为 Q\\mathcal{Q}Q 必须为每个子集 T⊆ST\\subseteq ST⊆S 包含一个函数 fff。现在，我们可以定义计数查询的复杂性度量。 定义4.2（Vapnik–Chervonenkis（VC）维度）。 如果存在基数 ∣S∣=d|S|=d∣S∣=d 的某些集合 S⊆XS \\subseteq \\mathcal{X}S⊆X 使得 Q\\mathcal{Q}Q 打散 SSS，并且 Q\\mathcal{Q}Q 不打散任何基数为 d+1d + 1d+1 的 SSS，则计数查询 Q\\mathcal{Q}Q 的集合具有 VC 维 ddd。 我们可以用 VC-DIM(Q)\\text{VC-DIM}(\\mathcal{Q})VC-DIM(Q) 表示这个数量。 再次考虑在域 X=R\\mathcal{X}=\\mathbb{R}X=R 上定义的范围 [0,∞][0,\\infty][0,∞] 上的一维间隔的类别。对应于间隔 [a,b][a,b][a,b] 的函数fa,bf_{a,b}fa,b​ 定义为 fa,b(x)=1f_{a,b}(x)=1fa,b​(x)=1 当且仅当 x∈[a,b]x\\in[a,b]x∈[a,b]。这是一个无限的查询类，但是它的VC维数是 222 。对于任何成对的不连续点 xyx xy，有一个既不包含点 (a,bx)(a,b (a,bx) 的间隔，也有包含两个点 (axyb)(a(axyb)，还有仅包含一个点的间隔 (axby OR xayb)(a(axby OR xayb)。但是，对于 xyzx xyz 的任意3个不同点，没有间隔 [a,b][a,b][a,b] 使得 fa,b[x]=fa,b[z]=1f_{a,b}[x]=f_{a,b}[z]=1fa,b​[x]=fa,b​[z]=1 而 fa,b[y]=0f_{a,b}[y]=0fa,b​[y]=0 我们观察到，有限概念类的 VC维 永远不会太大。 引理 4.7 对于任何有限类 Q\\mathcal{Q}Q，VC-DIM(Q)≤log⁡∣Q∣\\text{VC-DIM}(\\mathcal{Q})\\leq \\log|\\mathcal{Q}|VC-DIM(Q)≤log∣Q∣ 。 证明 如果 VC-DIM(Q)=d\\text{VC-DIM}(\\mathcal{Q})=dVC-DIM(Q)=d，则Q\\mathcal{Q}Q 打散基数为 ∣S∣=d|S|=d∣S∣=d 的一些集合 S⊆XS \\subseteq \\mathcal{X}S⊆X。但是根据打散的定义，由于 SSS 具有 2d2^d2d 个不同的子集， 所以 Q\\mathcal{Q}Q 必须在至少具有 2d2^d2d 不同的函数。 【引理 4.7 证毕】。 事实证明，我们可以替换在 SmallDB 机制的范围内使用术语 log⁡∣Q∣\\log|\\mathcal{Q}|log∣Q∣ 为 VC-DIM(Q)\\text{VC-DIM}(\\mathcal{Q})VC-DIM(Q)。通过前面的引理，这只能是对有限类 Q\\mathcal{Q}Q 的改进。 定理 4.8 对于线性查询 Q\\mathcal{Q}Q 的任何有限类，如果 R={y∈NX:∥y∥1∈O(VC-DIM(Q)α2)}\\mathcal{R}=\\{y\\in \\mathbb{N}^{\\mathcal{X}}:\\Vert y \\Vert_1 \\in O\\big( \\frac{ \\text{VC-DIM}(\\mathcal{Q}) }{\\alpha^2} \\big)\\}R={y∈NX:∥y∥1​∈O(α2VC-DIM(Q)​)}，则对于所有 x∈NXx \\in \\mathbb{N}^{\\mathcal{X}}x∈NX ，存在一个 y∈Ry\\in \\mathcal{R}y∈R 使得： max⁡f∈Q∣f(x)−f(y)∣≤α \\max_{f\\in\\mathcal{Q}}|f(x)-f(y)|\\leq \\alpha f∈Qmax​∣f(x)−f(y)∣≤α 作为该定理的结果，我们使用 VC维 作为查询类复杂度的度量，得了带有的类似 定理 4.5 的类似定理： 定理 4.9 令 yyy 为 SmallDB(x,Q,ε,α2)\\text{SmallDB}(x,\\mathcal{Q},\\varepsilon,\\frac{\\alpha}{2})SmallDB(x,Q,ε,2α​) 的数据库输出，我们可以确保概率为 1−β1-\\beta1−β 时有： max⁡f∈Q∣f(x)−f(y)∣≤O((log⁡∣X∣VC-DIM(Q)+log⁡(1β)ε∥x∥1)1/3) \\max_{f\\in\\mathcal{Q}}|f(x)-f(y)|\\leq O\\Bigg(\\bigg(\\frac{\\log|\\mathcal{X}|\\text{VC-DIM}(\\mathcal{Q})+\\log(\\frac{1}{\\beta})}{\\varepsilon\\Vert x \\Vert_1}\\bigg)^{1/3}\\Bigg) f∈Qmax​∣f(x)−f(y)∣≤O((ε∥x∥1​log∣X∣VC-DIM(Q)+log(β1​)​)1/3) 等价地，对于任何数据库 xxx ，其： ∥x∥1≥log⁡∣X∣VC-DIM(Q)+log⁡(1β)εα3 \\Vert x \\Vert_1 \\geq \\frac{\\log |\\mathcal{X}|\\text{VC-DIM}(\\mathcal{Q})+\\log(\\frac{1}{\\beta})}{\\varepsilon \\alpha^3} ∥x∥1​≥εα3log∣X∣VC-DIM(Q)+log(β1​)​ 有概率 1−β1-\\beta1−β 使得 max⁡f∈Q∣f(x)−f(y)∣≤α\\max_{f\\in\\mathcal{Q}}|f(x)-f(y)|\\leq \\alphamaxf∈Q​∣f(x)−f(y)∣≤α。 一种类似（尽管比较麻烦）的查询复杂度度量方法 “Fat Shattering Dimension” 定义了一类线性查询的复杂性，而不仅限于计数查询。就像 VC维 用于计数查询复杂度度量一样，“Fat Shattering Dimension” 用于控制一类线性查询 Q\\mathcal{Q}Q 的最小 “α-net”（第5节中的定义 5.2）的大小。类似地，此度量可为隐私发布线性查询的机制提供更精细的界限。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-16 09:52:54 "},"4-Releasing-Linear-Quries-with-Correlated-Error/An-online-mechanism-private-multiplicative-weights/An-online-mechanism-private-multiplicative-weights-Overview.html":{"url":"4-Releasing-Linear-Quries-with-Correlated-Error/An-online-mechanism-private-multiplicative-weights/An-online-mechanism-private-multiplicative-weights-Overview.html","title":"可乘权重算法：在线机制","keywords":"","body":"4.2 可乘权重算法：在线机制 我们现在将给出一种机制，用于回答在线的、可以交互选择的查询。该算法将 稀疏向量算法 和指数梯度下降算法相结合，用于在线学习线性预测器。 后一种算法也被称为 对冲（Hedge）或更普遍的可乘权重技术。其思想是：当我们将数据库 D∈N∣X∣D\\in \\mathbb{N}^{|\\mathcal{X}|}D∈N∣X∣ 看作一个直方图，并且只对线性查询（即该直方图的线性函数）感兴趣时，我们就可以将回答线性查询的问题看作是学习线性函数 DDD 的问题，该线性函数 DDD 为给定一个查询 q∈[0,1]∣X∣q\\in [0,1]^{|\\mathcal{X}|}q∈[0,1]∣X∣，返回答案为 ⟨D,q⟩⟨ D,q ⟩⟨D,q⟩。如果学习算法仅需要使用隐私保护的查询方式访问数据，那么隐私损失只会随着学习算法需要的查询数量增加而增加，而不是随我们想回答的查询数增加而增加。我们接下来介绍的 “可乘权重” 算法就是这种学习算法的一个经典例子：该算法仅进行少量查询来学习任何线性预测变量。它始终维持一个当前的 “假设预测器”，并且只通过要求查询其 “假设预测器” 与（真实）隐私数据库有很大差异的查询例子来访问数据。这个算法的保证是，只要给出少量这样的例子，它就可以一直学习目标线性函数，直到误差很小。我们怎样才能找到这些例子？ 我们在上一节中看到的 稀疏向量算法 允许我们动态地执行此操作，同时只为那些在当前乘法权重假设上有高错误的示例损失隐私预算。当查询进入时，我们询问查询的真实答案是否与当前乘法权重假设的查询答案有实质性的不同。注意，这是一个由稀疏向量技术处理的阈值类查询。 如果答案是 “否” ，这意味着当前已知的假设预测器会产生低于阈值，那么我们可以使用公开的假设预测器响应查询，并且不再有隐私损失。 如果答案是 “是”，这意味着当前已知的假设预测器会产生高于阈值的错误，那么我们找到了一个合适的例子来更新我们的学习算法。 因为 “高于阈值” 的答案恰好对应于更新我们的学习算法所需的查询，所以总的隐私损失仅取决于算法的学习率，而不取决于我们答复的查询总数。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-16 09:24:10 "},"4-Releasing-Linear-Quries-with-Correlated-Error/An-online-mechanism-private-multiplicative-weights/The-multiplicative-weight-update-rule.html":{"url":"4-Releasing-Linear-Quries-with-Correlated-Error/An-online-mechanism-private-multiplicative-weights/The-multiplicative-weight-update-rule.html","title":"可乘权重算法更新规则","keywords":"","body":"4.2.1 可乘权重算法更新规则 首先，我们给出可乘权重算法的更新规则，并在回答线性查询的语言中证明其收敛性定理。将数据库 xxx 视为数据全体 X\\mathcal{X}X 上的概率分布将很方便。也就是说，让 Δ([X])\\Delta([\\mathcal{X}])Δ([X]) 表示集合 [∣X∣][|\\mathcal{X}|][∣X∣] 上的概率分布集合，我们有 x∈Δ([X])x \\in\\Delta([\\mathcal{X}])x∈Δ([X])。请注意，我们始终可以缩放数据库以具有此属性，而无需更改任何线性查询的规范化值。 定理 4.10 固定一类线性查询 Q\\mathcal{Q}Q 和一个数据库 x∈Δ([X])x \\in\\Delta([\\mathcal{X}])x∈Δ([X])，并让 x1∈Δ([X])x^1 \\in\\Delta([\\mathcal{X}])x1∈Δ([X]) 描述 X\\mathcal{X}X 上的均匀分布： xi1=1/∣X∣x_i^1=1/|\\mathcal{X}|xi1​=1/∣X∣。 现在考虑数据库的最大长度序列 xtx^txt（t∈{2,...,L}t\\in\\{2,...,L\\}t∈{2,...,L}）。如 算法5 中所述，通过设置 xt+1=MW(xt,ft,vt)x^{t+1}=\\boldsymbol{MW}(x^t,f_t,v_t)xt+1=MW(xt,ft​,vt​) 来生成序列，其中对于每个 ttt，ft∈Qf_t\\in\\mathcal{Q}ft​∈Q 和 vt∈Rv_t \\in\\mathbb{R}vt​∈R 满足以下两个条件： 1. ∣ft(x)−ft(xt)∣>α,and2. ∣ft(x)−vt∣α \\begin{aligned} &1.\\ |f_t(x)-f_t(x^t)|>\\alpha,\\text{and} \\\\ &2.\\ |f_t(x)-v_t| ​1. ∣ft​(x)−ft​(xt)∣>α,and2. ∣ft​(x)−vt​∣α​ 则： L≤1+4log⁡∣X∣α2 L\\leq 1+\\frac{4\\log|\\mathcal{X}|}{\\alpha^2} L≤1+α24log∣X∣​ 注意，如果我们证明这个定理，我们将证明对于序列中的最后一个数据库 xL+1x^{L+1}xL+1 ，对于所有 f∈Q:∣f(x)−f(xL+1)∣≤αf\\in \\mathcal{Q}:|f(x)-f(x^{L+1})|\\leq \\alphaf∈Q:∣f(x)−f(xL+1)∣≤α，否则可能会扩展序列，与定理中的序列最大值 LLL 矛盾。换句话说，给定有区别的查询 ftf_tft​ ，可乘权重更新规则只需要很少的步骤 (L)(L)(L) 就可以对线性查询 Q\\mathcal{Q}Q 的任何类别学习私有数据库 xxx，直到一定的精度 α\\alphaα。我们将在下面的在线可乘权重算法中使用这个定理。在线可乘权重算法将始终在 ttt 时刻具有对数据库 xxx 的公开近似值 xtx^txt。给定输入查询 fff，该算法将计算 ∣f(x)−f(xt)∣|f(x)-f(x^t)|∣f(x)−f(xt)∣ 差别的噪声近似值。如果（噪声）差很大，则该算法将为真实答案 f(x)f(x)f(x) 提供噪声近似 f(x)+λtf(x)+\\lambda_tf(x)+λt​，其中 λt\\lambda_tλt​ 是从一些适当选择的 Laplace 分布得到。并且乘法权重算法更新规则将用参数 (xt,f,f(x)+λt)(x^t,f,f(x)+\\lambda_t)(xt,f,f(x)+λt​) 调用。如果该可乘权重算法更新规则仅当满足 定理 4.10 条件 1 与 定理 4.10 条件 2 时被调用，则我们应用 定理4.10 可以得到这样一个结论：更新不那么多（因为 LLL 不是太大），并且得到的 xL+1x^{L+1}xL+1 可以为所有 Q\\mathcal{Q}Q 中查询提供准确的答案（因为没有剩余其他有区别的查询了。） 通过跟踪在 ttt 时刻测量假设数据库 xtx^txt 和真实数据库 xxx 之间相似性的势函数 Ψ\\PsiΨ 我们证明了 定理4.10 。（注：势函数（potential function）：在平摊分析（amortized analysis）的势能法中，用来描述过去资源的投入可在后来操作中使用程度的函数。在线算法通常使用平摊分析。详见维基百科与相关文章） 我们将会表明： 1.势函数开始时不会太大。 2.在每个更新回合中，势函数都会大量减少。 3.势函数总是非负的。 综合这三个事实，我们将得出这样的结论：更新回合不能太多。 现在让我们开始分析收敛定理的证明。 【证明定理 4.10】 我们必须证明任何属性为 ∣ft(xt)−ft(x)∣>α|f_t(x^t)-f_t(x)|>\\alpha∣ft​(xt)−ft​(x)∣>α，且 ∣vt−ft(x)∣α|v_t - f_t(x)|∣vt​−ft​(x)∣α 的序列 {(xt,ft,vt)}t=1,...,L\\{(x^t,f_t,v_t)\\}_{t=1,...,L}{(xt,ft​,vt​)}t=1,...,L​ 不能有 L>4log⁡∣X∣α2L>\\frac{4\\log|\\mathcal{X}|}{\\alpha^2}L>α24log∣X∣​。 我们将势函数定义如下。回想一下我们将数据库作为概率分布(即，假设 ∥x∥1=1\\Vert x \\Vert_1=1∥x∥1​=1)。当然，这不需要实际修改实际数据库。当将数据库视为概率分布时，势函数是 xxx 和 xtx^txt 之间的相对熵（或 KL散度）： Ψt=defKL(x∥xt)=∑i=1∣X∣x[i]log⁡(x[i]xt[i]) \\Psi_t \\overset{\\text{def}}{=} KL(x\\Vert x^t) = \\sum_{i=1}^{|\\mathcal{X}|}x[i]\\log\\Big(\\frac{x[i]}{x^t[i]}\\Big) Ψt​=defKL(x∥xt)=i=1∑∣X∣​x[i]log(xt[i]x[i]​) 我们从一个简单的事实开始： 命题 4.11 对于所有的 ttt，Ψt≥0\\Psi_t\\geq 0Ψt​≥0 且 Ψ1≤log⁡∣X∣\\Psi_1\\leq \\log|\\mathcal{X}|Ψ1​≤log∣X∣。 【证明 命题 4.11】 证明相对熵（KL-散度）始终是非负的，需要借助对数和不等式，该不等式表示如果 a1,...,an;b1,...,bna_1,...,a_n;b_1,...,b_na1​,...,an​;b1​,...,bn​ 是非负数，则： ∑iailog⁡aibi≥(∑iai)∑iai∑ibi \\sum_i a_i \\log\\frac{a_i}{b_i} \\geq \\Big(\\sum_ia_i\\Big)\\frac{\\sum_ia_i}{\\sum_ib_i} i∑​ai​logbi​ai​​≥(i∑​ai​)∑i​bi​∑i​ai​​ 其次证明 Ψ1≤log⁡∣X∣\\Psi_1\\leq \\log|\\mathcal{X}|Ψ1​≤log∣X∣。首先回想一下 x1[i]=1/∣X∣x^1[i]=1/|\\mathcal{X}|x1[i]=1/∣X∣，则 Ψ1=∑i=1∣X∣x[i]log⁡(∣X∣x[i])\\Psi_1 = \\sum_{i=1}^{|\\mathcal{X}|}x[i]\\log(|\\mathcal{X}|x[i])Ψ1​=∑i=1∣X∣​x[i]log(∣X∣x[i])。注意，xxx 为概率分布，则：当 x[1]=1,x[i]=0,i>1x[1]=1,x[i]=0,i>1x[1]=1,x[i]=0,i>1 时，相对熵 Ψ1\\Psi_1Ψ1​ 有最大值，且为 Ψ1=log⁡∣X∣\\Psi_1 = \\log|\\mathcal{X}|Ψ1​=log∣X∣。 【命题 4.11 证毕】 我们现在要讨论的是，在每一步，势函数至少下降了 α2/4\\alpha^2/4α2/4。因为势开始于 log⁡∣X∣\\log|\\mathcal{X}|log∣X∣，并且必须总是非负的，所以我们知道在数据库更新序列中最多可以有 L≤4log⁡∣X∣/α2L\\leq 4\\log|\\mathcal{X}|/\\alpha^2L≤4log∣X∣/α2 步。首先，让我们看看每一步的势到底下降了多少： 引理 4.12 Ψt−Ψt+1≥η(⟨rt,xt⟩−⟨rt,x⟩)−η2 \\Psi_t - \\Psi_{t+1}\\geq \\eta\\Big(⟨ r_t,x^t ⟩-⟨ r_t,x ⟩\\Big) - \\eta^2 Ψt​−Ψt+1​≥η(⟨rt​,xt⟩−⟨rt​,x⟩)−η2 【证明】 由于 ∑i=1∣X∣x[i]=1\\sum_{i=1}^{|\\mathcal{X}|}x[i]=1∑i=1∣X∣​x[i]=1，则： Ψt−Ψt+1=∑i=1∣X∣x[i]log⁡(x[i]xt[i])−∑i=1∣X∣x[i]log⁡(x[i]xt+1[i])=∑i=1∣X∣x[i]log⁡(xt+1[i]xt[i])=∑i=1∣X∣x[i]log⁡(x^it+1/∑ix^it+1xt[i])=∑i=1∣X∣x[i][log⁡(xitexp⁡(−ηrt[i])xt[i])−log⁡(∑j=1∣X∣xjtexp⁡(−ηrt[j]))]=−(∑i=1∣X∣x[i]ηrt[i])−log⁡(∑i=1∣X∣xjtexp⁡(−ηrt[j]))=−η⟨rt,x⟩−log⁡(∑i=1∣X∣xjtexp⁡(−ηrt[j]))≥−η⟨rt,x⟩−log⁡(∑i=1∣X∣xjt(1+η2−ηrt[j]))=−η⟨rt,x⟩−log⁡(1+η2−η⟨rt,xt⟩)≥η(⟨rt,xt⟩−⟨rt,x⟩)−η2 \\begin{aligned} \\Psi_t - \\Psi_{t+1} &= \\sum_{i=1}^{|\\mathcal{X}|}x[i]\\log\\Big(\\frac{x[i]}{x^t[i]}\\Big) - \\sum_{i=1}^{|\\mathcal{X}|}x[i]\\log\\Big(\\frac{x[i]}{x^{t+1}[i]}\\Big)\\\\ &= \\sum_{i=1}^{|\\mathcal{X}|}x[i]\\log\\Big(\\frac{x^{t+1}[i]}{x^t[i]}\\Big)\\\\ &=\\sum_{i=1}^{|\\mathcal{X}|}x[i]\\log\\Big(\\frac{\\hat{x}_i^{t+1}/\\sum_i\\hat{x}_i^{t+1}}{x^t[i]}\\Big)\\\\ &= \\sum_{i=1}^{|\\mathcal{X}|}x[i]\\bigg[\\log\\Big(\\frac{x_i^t\\exp(-\\eta r_t[i])}{x^t[i]}\\Big)-\\log\\Big(\\sum_{j=1}^{|\\mathcal{X}|}x_j^t\\exp(-\\eta r_t[j])\\Big)\\bigg]\\\\ &= -\\Bigg(\\sum_{i=1}^{|\\mathcal{X}|}x[i]\\eta r_t[i]\\Bigg)-\\log\\Bigg(\\sum_{i=1}^{|\\mathcal{X}|}x_j^t\\exp(-\\eta r_t[j])\\Bigg)\\\\ &= -\\eta ⟨r_t,x⟩ - \\log\\Bigg(\\sum_{i=1}^{|\\mathcal{X}|}x_j^t\\exp(-\\eta r_t[j])\\Bigg)\\\\ &\\geq -\\eta ⟨r_t,x⟩ - \\log\\Bigg(\\sum_{i=1}^{|\\mathcal{X}|}x_j^t(1+\\eta^2-\\eta r_t[j])\\Bigg)\\\\ &= -\\eta ⟨r_t,x⟩ - \\log\\Big(1+\\eta^2-\\eta⟨r_t,x^t⟩\\Big)\\\\ &\\geq \\eta\\Big(⟨ r_t,x^t ⟩-⟨ r_t,x ⟩\\Big) - \\eta^2 \\end{aligned} Ψt​−Ψt+1​​=i=1∑∣X∣​x[i]log(xt[i]x[i]​)−i=1∑∣X∣​x[i]log(xt+1[i]x[i]​)=i=1∑∣X∣​x[i]log(xt[i]xt+1[i]​)=i=1∑∣X∣​x[i]log(xt[i]x^it+1​/∑i​x^it+1​​)=i=1∑∣X∣​x[i][log(xt[i]xit​exp(−ηrt​[i])​)−log(j=1∑∣X∣​xjt​exp(−ηrt​[j]))]=−(i=1∑∣X∣​x[i]ηrt​[i])−log(i=1∑∣X∣​xjt​exp(−ηrt​[j]))=−η⟨rt​,x⟩−log(i=1∑∣X∣​xjt​exp(−ηrt​[j]))≥−η⟨rt​,x⟩−log(i=1∑∣X∣​xjt​(1+η2−ηrt​[j]))=−η⟨rt​,x⟩−log(1+η2−η⟨rt​,xt⟩)≥η(⟨rt​,xt⟩−⟨rt​,x⟩)−η2​ 第一个不等式由下面这个事实得出（注：泰勒公式和 rt[j]∈rt,rt[j]≤1r_t[j] \\in r_t,r_t[j]\\leq 1rt​[j]∈rt​,rt​[j]≤1）： exp⁡(−ηrt[j])≤1−ηrt[j]+η2(rt[j])2≤1−ηrt[j]+η2 \\exp(-\\eta r_t[j])\\leq 1-\\eta r_t[j] + \\eta^2 (r_t[j])^2\\leq 1-\\eta r_t[j] + \\eta^2 exp(−ηrt​[j])≤1−ηrt​[j]+η2(rt​[j])2≤1−ηrt​[j]+η2 第二个不等式由对数不等式：log⁡(1+y)≤y,y>1\\log(1+y)\\leq y,y>1log(1+y)≤y,y>1 得到。 【引理 4.12证毕】 有了前面的命题和引理之后，可以完成剩下的证明。 根据 数据库/查询序列 的条件（在上述 定理4.10 的假设中进行了描述），对于每一个 ttt： 1. ∣ft(x)−ft(xt)∣>α,and2. ∣ft(x)−vt∣α \\begin{aligned} &1.\\ |f_t(x)-f_t(x^t)|>\\alpha,\\text{and} \\\\ &2.\\ |f_t(x)-v_t| ​1. ∣ft​(x)−ft​(xt)∣>α,and2. ∣ft​(x)−vt​∣α​ 因此，当且仅当 vtft(xt)v_t vt​ft​(xt) 时，ft(x)ft(xt)f_t(x)ft​(x)ft​(xt)。 特别地，如果 ft(xt)−ft(x)≥αf_t(x^t)-f_t(x)\\geq \\alphaft​(xt)−ft​(x)≥α，则 rt=ftr_t = f_trt​=ft​ ，如果 ft(x)−ft(xt)≥αf_t(x)-f_t(x^t)\\geq \\alphaft​(x)−ft​(xt)≥α，则 rt=1−ftr_t = 1-f_trt​=1−ft​。 因此，通过 引理4.12 和 可乘权重更新规则中所述的 η=α/2\\eta = \\alpha/2η=α/2 有： Ψt−Ψt+1≥α2(⟨rt,xt⟩−⟨rt,x⟩)−α24≥α2(α)−α24=α24 \\Psi_t - \\Psi_{t+1}\\geq \\frac{\\alpha}{2}\\Big(⟨ r_t,x^t ⟩-⟨ r_t,x ⟩\\Big) - \\frac{\\alpha^2}{4} \\geq \\frac{\\alpha}{2}(\\alpha) - \\frac{\\alpha^2}{4} = \\frac{\\alpha^2}{4} Ψt​−Ψt+1​≥2α​(⟨rt​,xt⟩−⟨rt​,x⟩)−4α2​≥2α​(α)−4α2​=4α2​ 最后可知： 0≤ΨL≤Ψ1−L⋅α24≤log⁡∣X∣−L⋅α24 0 \\leq \\Psi_L\\leq \\Psi_1 - L\\cdot\\frac{\\alpha^2}{4}\\leq \\log|\\mathcal{X}|-L\\cdot\\frac{\\alpha^2}{4} 0≤ΨL​≤Ψ1​−L⋅4α2​≤log∣X∣−L⋅4α2​ 变换得到：L≤4log⁡∣X∣α2L \\leq \\frac{4\\log|\\mathcal{X}|}{\\alpha^2}L≤α24log∣X∣​。 【定理 4.10 证毕】 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-16 12:11:55 "},"4-Releasing-Linear-Quries-with-Correlated-Error/An-online-mechanism-private-multiplicative-weights/The-OnlineMW-via-NumericSparse-algorithm.html":{"url":"4-Releasing-Linear-Quries-with-Correlated-Error/An-online-mechanism-private-multiplicative-weights/The-OnlineMW-via-NumericSparse-algorithm.html","title":"数值稀疏向量可乘权重算法","keywords":"","body":"4.2.2 数值稀疏向量可乘权重算法 现在，我们可以将可乘权重更新规则与 NumericSparse 算法结合起来，以提供交互式查询发布机制。对于 (ε,0)(\\varepsilon,0)(ε,0) 隐私，我们实质上（使用一些较差的常数）恢复了 SmallDB 的边界 对于(ε,δ)(\\varepsilon,\\delta)(ε,δ)- 差分隐私，我们能够使用组合定理来获得更好的边界。 对 NumericSparse 的查询询问的是 fi(x)f_i(x)fi​(x) 所给出的误差大小是否在适当选择的阈值 TTT 之上，即通过将 fif_ifi​ 应用于当前近似值 xtx_txt​ 与 xxx 来估计 fi(x)f_i(x)fi​(x) 的误差。也就是说，这些查询询问 ∣f(x)−f(xt)∣|f(x)-f(x_t)|∣f(x)−f(xt​)∣ 的值是否大于阈值。出于技术原因（无绝对值），这是通过询问 f(x)−f(xt)f(x)-f(x_t)f(x)−f(xt​) 和 f(xt)−f(x)f(x_t)-f(x)f(xt​)−f(x) 来完成的。回想一下，NumericSparse 算法的响应是 ⊥\\bot⊥ 或某些超过 TTT 的正值。我们将助记符 EEE 用于响应，以强调查询正在询问错误。 定理 4.13 在线数值稀疏向量可乘权重算法机制 The Online Multiplicative Weights Mechanism (via NumericSparse) 是 (ε,0)(\\varepsilon,0)(ε,0)- 差分隐私的。 【证明】 由 NumericSparse 算法的隐私分析即可证明该定理，因在线数值稀疏向量可乘权重算法机制仅通过 NumericSparse 算法访问数据库。 【定理 4.13 证毕】 大体上说，数值稀疏向量可乘权重算法的效用证明是使用 NumericSparse 的效用定理 定理 3.28 得出的。其结论为：仅当查询 ftf_tft​ 是真正有区别的查询时，可乘权重更新规则才被调用，这意味着 ∣ft(x)−f(xt)∣|f_t(x)-f(x_t)|∣ft​(x)−f(xt​)∣ 的值是“大”的，并且发布的噪声响应是足够“精确地”近似 ft(x)f_t(x)ft​(x)。有了这些假设之后，我们可以应用收敛定理（定理 4.10）来得到结论：权重更新的总次数是小的，因此算法可以回应所有 Q\\mathcal{Q}Q 的查询。 定理 4.14 对于 δ=0\\delta=0δ=0 且概率至少为 1−β1-\\beta1−β，对于所有查询 fif_ifi​，数值稀疏向量可乘权重算法返回答案 aia_iai​，使得 ∣fi(x)−ai∣≤3α|f_i(x)-a_i| \\leq 3\\alpha∣fi​(x)−ai​∣≤3α 对于任何 α\\alphaα 有： α≥32log⁡∣X∣(log⁡∣Q∣+log⁡(32log⁡∣X∣α2β))εα2∥x∥1 \\alpha \\geq \\frac{32\\log|\\mathcal{X}|\\Big(\\log|\\mathcal{Q}|+\\log(\\frac{32\\log|\\mathcal{X}|}{\\alpha^2\\beta})\\Big)}{\\varepsilon\\alpha^2\\Vert x\\Vert_1} α≥εα2∥x∥1​32log∣X∣(log∣Q∣+log(α2β32log∣X∣​))​ 【证明】 略 通过优化上述表达式的 α\\alphaα 并去除归一化因子，我们发现 数值稀疏向量可乘权重算法 可以将每个线性查询的准确度设为 3α3\\alpha3α，且概率为 β\\betaβ。有： α≤∥x∥12/3(36log⁡∣X∣(log⁡∣Q∣+log⁡(32log⁡∣X∣1/3∥x∥12/3β))ε)1/3 \\alpha \\leq \\Vert x \\Vert_1^{2/3} \\Bigg( \\frac{36\\log|\\mathcal{X}|\\Big(\\log|\\mathcal{Q}|+\\log(\\frac{32\\log|\\mathcal{X}|^{1/3}\\Vert x \\Vert_1^{2/3}}{\\beta})\\Big)}{\\varepsilon}\\Bigg)^{1/3} α≤∥x∥12/3​(ε36log∣X∣(log∣Q∣+log(β32log∣X∣1/3∥x∥12/3​​))​)1/3 与 SmallDB 算法相当。 通过重复同样的论点，但是使用稀疏向量的 (ε,δ)(\\varepsilon,\\delta)(ε,δ)-差分隐私的效用定理（定理 3.28 ），我们得到了以下定理： 定理 4.15 对于 δ>0\\delta>0δ>0 且概率至少为 1−β1-\\beta1−β，对于所有查询 fif_ifi​，数值稀疏向量可乘权重算法返回答案 aia_iai​ 满足 ∣fi(x)−ai∣≤3α|f_i(x)-a_i| \\leq 3\\alpha∣fi​(x)−ai​∣≤3α，对于任何 α\\alphaα 有： α≥(2+322)log⁡∣X∣log⁡2δ(log⁡∣Q∣+log⁡(32log⁡∣X∣α2β))εα2∥x∥1 \\alpha \\geq \\frac{(2+32\\sqrt{2})\\sqrt{\\log|\\mathcal{X}|\\log\\frac{2}{\\delta}}\\Big(\\log|\\mathcal{Q}|+\\log(\\frac{32\\log|\\mathcal{X}|}{\\alpha^2\\beta})\\Big)}{\\varepsilon\\alpha^2\\Vert x\\Vert_1} α≥εα2∥x∥1​(2+322​)log∣X∣logδ2​​(log∣Q∣+log(α2β32log∣X∣​))​ 再次优化上述表达式的 α\\alphaα 并去除归一化因子，我们发现 数值稀疏向量可乘权重算法 可以将每个线性查询的准确度设为 3α3\\alpha3α，且概率为 β\\betaβ。有： α≤∥x∥11/2((2+322)log⁡∣X∣log⁡2δ(log⁡∣Q∣+log⁡(32∥x∥1β))ε)1/2 \\alpha \\leq \\Vert x \\Vert_1^{1/2} \\Bigg( \\frac{(2+32\\sqrt{2})\\sqrt{\\log|\\mathcal{X}|\\log\\frac{2}{\\delta}}\\Big(\\log|\\mathcal{Q}|+\\log(\\frac{32\\Vert x \\Vert_1}{\\beta})\\Big)}{\\varepsilon}\\Bigg)^{1/2} α≤∥x∥11/2​(ε(2+322​)log∣X∣logδ2​​(log∣Q∣+log(β32∥x∥1​​))​)1/2 这比 SmallDB 算法提供了更好的精度（作为 ∥x∥1\\Vert x \\Vert_1∥x∥1​ 的函数）。直观地说，更高的精度来自算法的迭代性质，这允许我们利用 (ε,δ)(\\varepsilon,\\delta)(ε,δ)-隐私合成定理。SmallDB 算法只运行一次，因此没有机会利用合成定理的特点。 数值稀疏向量可乘权重算法 的准确性取决于几个参数，值得进一步讨论。最后，该算法使用稀疏矢量技术与线性函数的学习算法配对来回答查询。正如我们在上一节中所证明的那样，当总共进行了 kkk 个灵敏度w为 1/∥x∥11/\\Vert x\\Vert_11/∥x∥1​ 的查询，且这些查询中最多 ccc 个可以具有 “高于阈值” 的答案。回想一下，这些误差项的出现是因为稀疏向量算法只能为 “高于阈值” 的查询“支付”隐私预算，因此会增加噪声 O(c/(ε∥x∥1))O(c/(\\varepsilon\\Vert x \\Vert_1))O(c/(ε∥x∥1​)) 到每个查询。另一方面，由于我们最终将尺度范围为 Ω(c/(ε∥x∥1))\\Omega(c/(\\varepsilon \\Vert x \\Vert_1))Ω(c/(ε∥x∥1​)) ）的独立拉普拉斯噪声添加到总共 kkk 个查询中，因此我们期望表示所有 kkk 个查询的最大误差都比 log⁡k\\log klogk 因子大。但是 ccc 为多少，我们应该问什么查询呢？可乘权重学习算法为我们提供了一种查询策略，并保证了 c=O(log⁡∣X∣/α2)c=O(\\log |\\mathcal{X}|/\\alpha^2)c=O(log∣X∣/α2) 查询对于任何 ααα 都将高于 T=O(α)T = O(\\alpha)T=O(α) 的阈值（我们要求的查询始终是：“实际答案与当前可乘权重假设所预计答案有多少不同？这些问题的答案既为我们提供了查询的真实答案，又为您提供了当查询高于阈值时如何适当更新学习算法的说明。综上，这使得我们将阈值设置为 O(α)O(\\alpha)O(α)，其中 ααα 是满足的表达式：α=O(log⁡∣X∣log⁡k/(ε∥x∥1α2))\\alpha=O(\\log|\\mathcal{X}|\\log k/(\\varepsilon\\Vert x \\Vert_1\\alpha^2))α=O(log∣X∣logk/(ε∥x∥1​α2)) 。这样可以最大程度地减少两种误差来源：稀疏矢量技术的误差和未能更新乘法权重假设的误差。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-20 08:46:58 "},"4-Releasing-Linear-Quries-with-Correlated-Error/Bibliographic-notes.html":{"url":"4-Releasing-Linear-Quries-with-Correlated-Error/Bibliographic-notes.html","title":"参考文献","keywords":"","body":"参考文献 The offline query release mechanism given in this section is from Blum et al. [8], which gave bounds in terms of the VC-Dimension of the query class (Theorem 4.9). The generalization to fat shattering dimension is given in [72]. The online query release mechanism given in this section is from Hardt and Rothblum [44]. This mechanism uses the classic multiplica- tive weights update method, for which Arora, Hazan and Kale give an excellent survey [1]. Slightly improved bounds for the private multi- plicative weights mechanism were given by Gupta et al. [39], and the analysis here follows the presentation from [39]. Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2019-12-20 08:47:42 "},"5-Generalizations/Overview.html":{"url":"5-Generalizations/Overview.html","title":"五、差分隐私泛化","keywords":"","body":"五、泛化 在本章中，我们泛化了上一节的查询发布算法。结果，我们得到了任意低灵敏度查询（不仅是线性查询）的界限，还有线性查询的新界限。这些泛化也为查询发布和机器学习之间的联系提供了一些启示。 第 4 章中的 SmallDB 离线查询发布机制是我们称之为 Net 机制的一个特例。我们发现，两种机制都产生了合成数据库，这为在隐私数据库上的任何查询 Q\\mathcal{Q}Q 的返回值提供了一种方便的逼近方法：只评估合成数据库上的查询，并将结果作为有噪声的答案。更一般地说，一种机制可以产生任意形式的数据结构，与固定的、公共的算法（独立于数据库）一起提供一种近似查询值的方法。 Net 机制是 SmallDB 机制的一个简单泛化：首先，独立于实际数据库，定义一个数据结构的 α\\alphaα-Net，其独立于真实数据库，这样使得通过发布的数据结构对 Q\\mathcal{Q}Q 中的任何查询进行求值，可以很好地（在加性α误差范围内）估计私有数据库上的查询值。下一步，对Q\\mathcal{Q}Q 中的任何查询应用指数机制对 Net 中的元素进行选择，指数机制中的效用函数将 Net 中元素的最大错误最小化。 我们还对 在线可乘权重算法 进行了泛化，以便我们可以使用任何其他在线学习算法实例化该算法，以通过一组查询来学习数据库。我们注意到，这种机制可以在线运行，也可以离线运行，在这种情况下，这些向 “在线” 机制请求的查询集合是由 “隐私区分器” 来选择的，该隐私区分器识别当前假设与实际数据库大不相同的查询。这些查询将在在线算法中生成更新步骤。事实证明，“隐私区分器”等同于不可知论学习算法，该算法为有效的查询发布机制提供了一个明确的来源。 在下面各节中，我们将讨论查询集 Q\\mathcal{Q}Q 类型的数据结构。 定义 5.1 一类查询 Q\\mathcal{Q}Q 有一些数据结构类 D\\mathcal{D}D，从这些数据结构类 D\\mathcal{D}D 中提取的数据结构 DDD 被隐式地赋予了一个分析函数 Eval:D×Q→R\\text{Eval:}\\mathcal{D}\\times\\mathcal{Q} \\to \\mathbb{R}Eval:D×Q→R，我们可以用它来分析 DDD 上的任何查询。然而，为了避免被符号所困扰，当上下文中的含义清楚时，我们将简单地写 f(D)f(D)f(D) 来表示 Eval(D,f)\\text{Eval}(D,f)Eval(D,f)。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2020-01-02 13:11:41 "},"5-Generalizations/Mechanisms-via-alpha-nets.html":{"url":"5-Generalizations/Mechanisms-via-alpha-nets.html","title":"𝞪-nets机制","keywords":"","body":"5.1 𝞪-nets 机制 给定一个查询集 Q\\mathcal{Q}Q，下面我们开始定义 α\\alphaα-nets。 定义 5.2（α\\alphaα-nets。） 关于查询类 Q\\mathcal{Q}Q 的数据结构 α\\alphaα-nets 为集合 N⊂N∣X∣\\mathcal{N}\\subset \\mathbb{N}^{|\\mathcal{X}|}N⊂N∣X∣。对于所有 x∈N∣X∣x\\in \\mathbb{N}^{\\mathcal{|X|}}x∈N∣X∣，都存在一个 α\\alphaα-nets 的元素 y∈Ny\\in \\mathcal{N}y∈N，使得： max⁡f∈Q∣f(x)−f(y)∣≤α \\max_{f\\in\\mathcal{Q}}|f(x)-f(y)|\\leq \\alpha f∈Qmax​∣f(x)−f(y)∣≤α 我们用 Nα(Q)\\mathcal{N}_\\alpha(\\mathcal{Q})Nα​(Q) 表示在 Q\\mathcal{Q}Q 的所有 α\\alphaα-nets 集合中最小的 α\\alphaα-nets 基数。 也就是说，对于每个可能的数据库 xxx 及 Q\\mathcal{Q}Q 中的所有查询，都存在一个 α\\alphaα-nets 的元素，该元素“看起”来像 xxx，直到容错度为 α\\alphaα。 小 α\\alphaα-nets 对我们很有用，因为当与指数机制配对时，它们将为查询带来高精度。给定一类函数 Q\\mathcal{Q}Q，我们将定义一个指数机制的实例化，称之为 Net 机制。我们首先观察到 Net 机制 保留了 ε\\varepsilonε-差分隐私。 命题 5.1 Net 机制 是 (ε,0)(\\varepsilon,0)(ε,0)-差分隐私的。 【证明】 Net 机制 只是指数机制的实例化。因此，隐私定理遵循 定理 3.10。 【命题 5.1 证毕】 我们可以类似地对指数机制进行分析，以开始理解 Net 机制 的效用保证： 命题 5.2 令 Q\\mathcal{Q}Q 为敏感度 1/∥x∥11/\\Vert x \\Vert_11/∥x∥1​ 查询的任何类别。令 yyy 为 Net 机制 NetMechanism(x,Q,ε,α)\\text{NetMechanism}(x,\\mathcal{Q},\\varepsilon,\\alpha)NetMechanism(x,Q,ε,α) 输出的数据库。然后有 1−β1-\\beta1−β 的概率使得： max⁡f∈Q∣f(x)−f(y)∣≤α+2(log⁡(∣Nα(Q)∣)+log⁡(1β))ε∥x∥1 \\max _{f \\in \\mathcal{Q}}|f(x)-f(y)| \\leq \\alpha+\\frac{2\\left(\\log \\left(\\left|\\mathcal{N}_{\\alpha}(\\mathcal{Q})\\right|\\right)+\\log \\left(\\frac{1}{\\beta}\\right)\\right)}{\\varepsilon\\|x\\|_{1}} f∈Qmax​∣f(x)−f(y)∣≤α+ε∥x∥1​2(log(∣Nα​(Q)∣)+log(β1​))​ 【证明】通过应用定理 3.11并注意到 S(q)=1∥x∥1S(q)=\\frac{1}{\\Vert x \\Vert_1}S(q)=∥x∥1​1​，并且根据 α-net 的定义有 OPT(D)≤α\\text{OPT}(D)\\leq \\alphaOPT(D)≤α，我们发现： Pr⁡[max⁡f∈Q∣f(x)−f(y)∣≥α+2ε∥x∥1(log⁡(∣Nα(Q)∣)+t)]≤e−t \\operatorname{Pr}\\left[\\max _{f \\in \\mathcal{Q}}|f(x)-f(y)| \\geq \\alpha+\\frac{2}{\\varepsilon\\|x\\|_{1}}\\left(\\log \\left(\\left|\\mathcal{N}_{\\alpha}(\\mathcal{Q})\\right|\\right)+t\\right)\\right] \\leq e^{-t} Pr[f∈Qmax​∣f(x)−f(y)∣≥α+ε∥x∥1​2​(log(∣Nα​(Q)∣)+t)]≤e−t 当 t=log⁡(1β)t=\\log(\\frac{1}{\\beta})t=log(β1​) 则完成证明命题 5.2。 【命题 5.2 证毕】 因此，我们可以知道，通过 ∣Nα(Q)∣\\left|\\mathcal{N}_{\\alpha}(\\mathcal{Q})\\right|∣Nα​(Q)∣ 的上界（Q\\mathcal{Q}Q 为函数集合），推得差分隐私机制能同时为 Q\\mathcal{Q}Q 类中的所有函数提供的精度的上界。 这正是我们在 第4.1节 中所做的，我们看到当 Q\\mathcal{Q}Q 是一类线性查询时，关键质量是 Q\\mathcal{Q}Q 的 VC\\text{VC}VC 维。 Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2020-01-19 08:31:08 "},"5-Generalizations/The-iterative-construction-mechanism.html":{"url":"5-Generalizations/The-iterative-construction-mechanism.html","title":"迭代构建机制","keywords":"","body":"5.2 迭代构建机制 在本节中，我们将推导出隐私累加权重算法的离线泛化算法，可以使用任何适当定义的学习算法将这个算法进行实例化。一般来说，数据库更新算法维护一系列数据结构 D1,D2,...D^1,D^2,...D1,D2,...，这些结构为输入数据库 xxx 提供​​越来越好的近似值（在某种意义上取决于数据库更新算法）。此外，这些机制通过仅考虑一个查询 fff 来产生序列中的下一个数据结构，这个查询 fff 在真实数据库 xxx 产生的结果与在数据结构 DtD^tDt 产生的结果有显著的不同。（即：f(Dt)f(D^t)f(Dt) 与 f(x)f(x)f(x) 区别很大。）本节中的算法表明，在很小的程度上，以差分隐私的方式解决 “查询-发布” 问题就等于以差分隐私的方式解决更简单的学习或区分问题：给定了隐私区分算法和非隐私数据库更新算法，我们得到相应的隐私发布算法。对于一般的线性查询设置，我们可以插入指数机制作为规范的专用区分器，而将乘数权重算法作为通用的数据库更新算法，但是在特殊情况下，可以使用更有效的区分器。 从语法上讲，我们将考虑形式为 U:D×Q×R→DU:\\mathcal{D}\\times\\mathcal{Q}\\times\\mathbb{R}\\to \\mathcal{D}U:D×Q×R→D 的函数。其中 D\\mathcal{D}D表示一类数据结构，这类数据结构可以对 Q\\mathcal{Q}Q 中的查询进行评估。函数 UUU 的输入为：1、 D\\mathcal{D}D 中的数据结构，将当前数据结构表示为 DtD^tDt；2、区别查询 fff，并且可以被限制为某个集合 Q\\mathcal{Q}Q；3、并且还实数 xxx，其估计 f(x)f(x)f(x)。以下我们正式定义一个 数据库更新序列，以控制用于生成数据库序列 D1,D2,...D^1,D^2,...D1,D2,... 的 UUU 输入序列。 定义 5.3 数据库更新序列 设 x∈N∣X∣x\\in \\mathbb{N}^{|\\mathcal{X}|}x∈N∣X∣ 为任意数据库，并设 {(Dt,ft,vt)}t=1,...,L∈(D×Q×R)L\\{(D^t,f_t,v_t)\\}_{t=1,...,L}\\in(\\mathcal{D}\\times\\mathcal{Q}\\times\\mathbb{R})^L{(Dt,ft​,vt​)}t=1,...,L​∈(D×Q×R)L 为元组序列。如果满足以下条件： 1、D1=U(⊥,⋅,⋅)D^1=U(\\bot,\\cdot,\\cdot)D1=U(⊥,⋅,⋅) ， 2、任意 t=1,2,...,L,∣ft(x)−ft(Dt)∣≥αt=1,2,...,L,|f_t(x)-f_t(D^t)|\\geq \\alphat=1,2,...,L,∣ft​(x)−ft​(Dt)∣≥α 3、任意 t=1,2,...,L,∣ft(x)−vt∣αt=1,2,...,L,|f_t(x)-v_t| t=1,2,...,L,∣ft​(x)−vt​∣α 4、任意 t=1,2,...,L−1,Dt+1=U(Dt,ft,vt)t=1,2,...,L-1,D^{t+1}=U(D^t,f_t,v_t)t=1,2,...,L−1,Dt+1=U(Dt,ft​,vt​) 则将其这个序列称之为：(U,x,Q,α,T)(U,x,\\mathcal{Q},\\alpha,T)(U,x,Q,α,T)-数据库更新序列（ (U,x,Q,α,T)−database update sequence(U,x,\\mathcal{Q},\\alpha,T)-database\\ update \\ sequence(U,x,Q,α,T)−database update sequence ） Copyright © GuoJohnny 2019 all right reserved，powered by Gitbook修订时间： 2020-02-04 07:41:02 "}}